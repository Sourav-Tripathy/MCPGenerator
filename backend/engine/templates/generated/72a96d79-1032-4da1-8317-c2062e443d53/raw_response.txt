{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Union, Dict, Any, Literal\n\n# --- Base Models from Type Definitions ---\n\nclass ImageUrl(BaseModel):\n    \"\"\"Represents an image URL, potentially a data URI.\"\"\"\n    url: str = Field(..., description=\"URL of the image (http(s):// or data:image/...)\")\n\nclass FileUrl(BaseModel):\n    \"\"\"Represents a file data URI.\"\"\"\n    url: str = Field(..., description=\"Data URI of the file (data:[<mediatype>][;base64],<data>). Max 10MB.\")\n\nclass MessageContentPart(BaseModel):\n    \"\"\"Represents a part of a message content, which can be text or an image/file data URI.\"\"\"\n    type: Literal['text', 'image_url', 'file_url'] = Field(..., description=\"Type of content part ('text', 'image_url', 'file_url').\")\n    text: Optional[str] = Field(None, description=\"Text content.\")\n    image_url: Optional[ImageUrl] = Field(None, description=\"Image URL object.\")\n    file_url: Optional[FileUrl] = Field(None, description=\"File data URI object.\")\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant'] = Field(..., description=\"Role of the message author ('user' or 'assistant').\")\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"Content of the message. Can be simple text or a list of content parts for multimodal input.\")\n\nclass UrlCitationAnnotation(BaseModel):\n    \"\"\"Details of a URL citation within the response content.\"\"\"\n    title: str = Field(..., description=\"Title of the cited web page.\")\n    exactQuote: str = Field(..., description=\"The exact quote from the source used in the answer.\")\n    url: str = Field(..., description=\"URL of the source.\")\n    dateTime: str = Field(..., description=\"Timestamp when the content was accessed or published.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation object within the response delta/message.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitationAnnotation] = Field(None, description=\"Details if the annotation type is 'url_citation'.\")\n\nclass ResponseMessage(BaseModel):\n    \"\"\"Represents the assistant's message in the response (non-streamed).\"\"\"\n    role: str = Field(..., description=\"Typically 'assistant'.\")\n    content: Optional[str] = Field(None, description=\"The main textual content of the response.\")\n    type: Optional[str] = Field(None, description=\"Type indicator, e.g., 'text'.\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"List of annotations, like URL citations.\")\n\nclass ResponseDelta(BaseModel):\n    \"\"\"Represents the delta in a streamed response chunk.\"\"\"\n    role: Optional[str] = Field(None, description=\"Typically 'assistant'.\")\n    content: Optional[str] = Field(None, description=\"The partial content of the response chunk.\")\n    type: Optional[str] = Field(None, description=\"Type indicator, e.g., 'text'.\")\n    # Note: Annotations might appear in the final chunk or specific chunks in streaming\n    annotations: Optional[List[Annotation]] = Field(None, description=\"List of annotations, like URL citations.\")\n\n\n# --- Input Model for chat_completion Tool ---\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the DeepSearch chat completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use. Currently only 'jina-deepsearch-v1' is supported.\")\n    stream: bool = Field(True, description=\"Whether to stream back partial progress and the final answer. Recommended to be true to avoid timeouts.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains reasoning effort. Supported values: 'low', 'medium', 'high'. Overridden by budget_tokens or max_attempts.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides reasoning_effort.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Overrides reasoning_effort.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces further thinking/search steps even for seemingly trivial queries.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema to ensure the final answer matches the structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        # Ensure default values are used correctly\n        use_enum_values = True\n\n# --- Output Models (Based on OpenAI Schema Compatibility) ---\n\nclass UsageInfo(BaseModel):\n    \"\"\"Token usage information for the request.\"\"\"\n    prompt_tokens: int\n    completion_tokens: Optional[int] = None # May not be present in all chunks\n    total_tokens: int\n\n# Non-Streaming Output\nclass DeepSearchChatOutputChoice(BaseModel):\n    index: int\n    message: ResponseMessage\n    finish_reason: Optional[str] = None # e.g., 'stop', 'length'\n\nclass DeepSearchChatOutput(BaseModel):\n    \"\"\"Structure of the non-streaming response from DeepSearch.\"\"\"\n    id: str\n    object: str = \"chat.completion\"\n    created: int # Unix timestamp\n    model: str\n    choices: List[DeepSearchChatOutputChoice]\n    usage: UsageInfo\n    visitedUrls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process.\")\n    searchQueries: Optional[List[str]] = Field(None, description=\"List of search queries executed.\")\n\n# Streaming Output\nclass DeepSearchChatChunkChoice(BaseModel):\n    index: int\n    delta: ResponseDelta\n    finish_reason: Optional[str] = None # Usually in the last chunk\n\nclass DeepSearchChatChunk(BaseModel):\n    \"\"\"Structure of a streaming chunk from DeepSearch.\"\"\"\n    id: str\n    object: str = \"chat.completion.chunk\"\n    created: int # Unix timestamp\n    model: str\n    choices: List[DeepSearchChatChunkChoice]\n    usage: Optional[UsageInfo] = None # Usually in the last chunk\n    visitedUrls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process, often in the final chunk.\")\n    searchQueries: Optional[List[str]] = Field(None, description=\"List of search queries executed, often in the final chunk.\")\n\n"
    },
    {
      "name": "client.py",
      "content": "import httpx\nimport logging\nimport os\nimport json\nfrom typing import Dict, Any, Union, AsyncGenerator, Optional\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatInput, DeepSearchChatOutput, DeepSearchChatChunk\n\nlogger = logging.getLogger(__name__)\n\nclass DeepSearchError(Exception):\n    \"\"\"Custom exception for DeepSearch API errors.\"\"\"\n    def __init__(self, status_code: Optional[int] = None, message: str = \"DeepSearch API error\"):\n        self.status_code = status_code\n        self.message = message\n        super().__init__(self.message)\n\nclass DeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = 180.0):\n        \"\"\"\n        Initializes the DeepSearchClient.\n\n        Args:\n            api_key: The Jina API key. Reads from JINA_API_KEY env var if not provided.\n            base_url: The base URL for the DeepSearch API. Reads from DEEPSEARCH_API_BASE_URL or defaults.\n            timeout: Default timeout for HTTP requests in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API key not provided or found in JINA_API_KEY environment variable.\")\n\n        self.base_url = base_url or os.getenv(\"DEEPSEARCH_API_BASE_URL\", \"https://deepsearch.jina.ai\")\n        self.endpoint = \"/v1/chat/completions\"\n        self.timeout = timeout\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # For non-streaming\n        }\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=self.timeout\n        )\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n\n    async def _request(\n        self,\n        method: str,\n        endpoint: str,\n        payload: Optional[Dict[str, Any]] = None,\n        stream: bool = False\n    ) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:\n        \"\"\"Makes an asynchronous request to the DeepSearch API.\"\"\"\n        request_headers = self.headers.copy()\n        if stream:\n            request_headers[\"Accept\"] = \"text/event-stream\"\n\n        try:\n            if stream:\n                return self._stream_request(method, endpoint, payload, request_headers)\n            else:\n                response = await self.client.request(method, endpoint, json=payload, headers=request_headers)\n                response.raise_for_status() # Raise exception for 4xx/5xx errors\n                return response.json()\n\n        except httpx.HTTPStatusError as e:\n            error_message = f\"HTTP error {e.response.status_code}: {e.response.text}\"\n            logger.error(f\"Request failed: {error_message}\")\n            # Try to parse error details from response if available\n            try:\n                error_details = e.response.json()\n                message = error_details.get('detail', error_message)\n            except json.JSONDecodeError:\n                message = error_message\n            raise DeepSearchError(status_code=e.response.status_code, message=message) from e\n\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out after {self.timeout}s: {e}\")\n            raise DeepSearchError(message=f\"Request timed out: {e}\") from e\n\n        except httpx.RequestError as e:\n            logger.error(f\"Request error: {e}\")\n            raise DeepSearchError(message=f\"Request error: {e}\") from e\n\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON response: {e}\")\n            raise DeepSearchError(message=f\"Invalid JSON response from API: {e}\") from e\n\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred: {e}\")\n            raise DeepSearchError(message=f\"An unexpected error occurred: {e}\") from e\n\n    async def _stream_request(\n        self,\n        method: str,\n        endpoint: str,\n        payload: Optional[Dict[str, Any]],\n        headers: Dict[str, str]\n    ) -> AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Handles streaming requests using Server-Sent Events (SSE).\"\"\"\n        try:\n            async with self.client.stream(method, endpoint, json=payload, headers=headers) as response:\n                # Check for initial errors before starting to stream\n                if response.status_code >= 400:\n                    error_body = await response.aread()\n                    error_message = f\"HTTP error {response.status_code}: {error_body.decode()}\"\n                    logger.error(f\"Streaming request failed: {error_message}\")\n                    # Try to parse error details\n                    try:\n                        error_details = json.loads(error_body)\n                        message = error_details.get('detail', error_message)\n                    except json.JSONDecodeError:\n                        message = error_message\n                    raise DeepSearchError(status_code=response.status_code, message=message)\n\n                # Process SSE stream\n                async for line in response.aiter_lines():\n                    if line.startswith(\"data:\"):\n                        data_str = line[len(\"data:\"):].strip()\n                        if data_str == \"[DONE]\":\n                            logger.info(\"Stream finished with [DONE] marker.\")\n                            break\n                        if data_str:\n                            try:\n                                chunk_data = json.loads(data_str)\n                                # Validate chunk structure (optional but recommended)\n                                try:\n                                    DeepSearchChatChunk.model_validate(chunk_data)\n                                except ValidationError as ve:\n                                    logger.warning(f\"Received stream chunk failed validation: {ve}. Chunk: {chunk_data}\")\n                                yield chunk_data\n                            except json.JSONDecodeError:\n                                logger.warning(f\"Failed to decode stream data line: {data_str}\")\n                    elif line.strip(): # Log other non-empty lines if needed\n                         logger.debug(f\"Received non-data line in stream: {line}\")\n\n        except httpx.HTTPStatusError as e:\n            # This might catch errors if status check happens after stream starts (less common)\n            error_message = f\"HTTP error {e.response.status_code} during stream: {e.response.text}\"\n            logger.error(f\"Streaming request failed: {error_message}\")\n            raise DeepSearchError(status_code=e.response.status_code, message=error_message) from e\n        except httpx.TimeoutException as e:\n            logger.error(f\"Stream timed out after {self.timeout}s: {e}\")\n            raise DeepSearchError(message=f\"Stream timed out: {e}\") from e\n        except httpx.RequestError as e:\n            logger.error(f\"Stream request error: {e}\")\n            raise DeepSearchError(message=f\"Stream request error: {e}\") from e\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during streaming: {e}\")\n            raise DeepSearchError(message=f\"An unexpected error occurred during streaming: {e}\") from e\n\n    async def chat_completion(\n        self, params: DeepSearchChatInput\n    ) -> Union[DeepSearchChatOutput, AsyncGenerator[DeepSearchChatChunk, None]]:\n        \"\"\"\n        Executes a chat completion request using the DeepSearch engine.\n\n        Args:\n            params: The input parameters for the chat completion.\n\n        Returns:\n            If stream=False, returns a DeepSearchChatOutput object.\n            If stream=true, returns an async generator yielding DeepSearchChatChunk objects.\n\n        Raises:\n            DeepSearchError: If the API request fails.\n            ValidationError: If the response data doesn't match the expected Pydantic model.\n        \"\"\"\n        payload = params.model_dump(exclude_none=True) # Exclude optional fields not set\n        logger.info(f\"Sending chat completion request to {self.base_url}{self.endpoint} (stream={params.stream})\")\n        # logger.debug(f\"Request payload: {payload}\") # Be careful logging sensitive data\n\n        response_data = await self._request(\n            method=\"POST\",\n            endpoint=self.endpoint,\n            payload=payload,\n            stream=params.stream\n        )\n\n        if params.stream:\n            # The response_data is already the async generator from _stream_request\n            async def validated_generator() -> AsyncGenerator[DeepSearchChatChunk, None]:\n                try:\n                    async for chunk in response_data:\n                        try:\n                            yield DeepSearchChatChunk.model_validate(chunk)\n                        except ValidationError as e:\n                            logger.error(f\"Stream chunk validation failed: {e}. Chunk: {chunk}\")\n                            # Decide whether to yield anyway, raise, or skip\n                            # For now, we log and skip the invalid chunk\n                            continue\n                except DeepSearchError as e:\n                    logger.error(f\"Error consuming stream generator: {e}\")\n                    # Re-raise or handle as needed; FastMCP might handle exceptions from generators\n                    raise\n            return validated_generator()\n        else:\n            # Validate and return the single response object\n            try:\n                validated_output = DeepSearchChatOutput.model_validate(response_data)\n                logger.info(\"Received successful non-streaming chat completion response.\")\n                return validated_output\n            except ValidationError as e:\n                logger.error(f\"Non-streaming response validation failed: {e}. Response: {response_data}\")\n                raise DeepSearchError(message=f\"Invalid response structure: {e}\") from e\n"
    },
    {
      "name": "main.py",
      "content": "import logging\nimport os\nfrom typing import List, Optional, Dict, Any, Union, AsyncGenerator\n\nfrom dotenv import load_dotenv\nfrom mcp.server.fastmcp import FastMCP\nfrom pydantic import ValidationError\n\nfrom models import (\n    Message,\n    DeepSearchChatInput,\n    DeepSearchChatOutput,\n    DeepSearchChatChunk,\n    # Import other models if needed directly in tool signatures, though using the Input model is cleaner\n)\nfrom client import DeepSearchClient, DeepSearchError\n\n# --- Configuration & Initialization ---\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlog_level = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\nlogging.basicConfig(\n    level=log_level,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP server for Jina AI's DeepSearch API. Performs complex search queries with reasoning and web search.\"\n)\n\n# Initialize API Client\n# It's good practice to handle potential initialization errors\ntry:\n    api_key = os.getenv(\"JINA_API_KEY\")\n    if not api_key:\n        logger.warning(\"JINA_API_KEY environment variable not set. Client initialization might fail or use defaults.\")\n\n    client_timeout_str = os.getenv(\"MCP_TIMEOUT\", \"180\")\n    try:\n        client_timeout = float(client_timeout_str)\n    except ValueError:\n        logger.warning(f\"Invalid MCP_TIMEOUT value '{client_timeout_str}'. Using default 180.0 seconds.\")\n        client_timeout = 180.0\n\n    deepsearch_client = DeepSearchClient(api_key=api_key, timeout=client_timeout)\n    logger.info(f\"DeepSearchClient initialized for base URL: {deepsearch_client.base_url}\")\nexcept ValueError as e:\n    logger.exception(f\"Failed to initialize DeepSearchClient: {e}\")\n    # Depending on desired behavior, you might exit or let it fail later\n    # For now, we'll let it proceed and fail at request time if key is missing\n    deepsearch_client = None # Ensure it's defined, even if None\nexcept Exception as e:\n    logger.exception(f\"An unexpected error occurred during client initialization: {e}\")\n    deepsearch_client = None\n\n# --- MCP Tool Definition ---\n\n@mcp.tool(input_model=DeepSearchChatInput)\nasync def chat_completion(\n    messages: List[Message],\n    model: str = \"jina-deepsearch-v1\",\n    stream: bool = True,\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = \"medium\", # type: ignore\n    budget_tokens: Optional[int] = None,\n    max_attempts: Optional[int] = None,\n    no_direct_answer: Optional[bool] = False,\n    max_returned_urls: Optional[int] = None,\n    structured_output: Optional[Dict[str, Any]] = None,\n    good_domains: Optional[List[str]] = None,\n    bad_domains: Optional[List[str]] = None,\n    only_domains: Optional[List[str]] = None\n) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:\n    \"\"\"\n    Executes a chat completion request using the DeepSearch engine.\n\n    It takes a series of messages, searches the web, reads content, and reasons\n    iteratively to generate a comprehensive answer. Supports streaming for\n    real-time updates and reasoning steps.\n\n    Args:\n        messages: A list of messages comprising the conversation history.\n        model: ID of the model to use (default: 'jina-deepsearch-v1').\n        stream: Whether to stream back partial progress (default: True).\n        reasoning_effort: Constrains reasoning effort ('low', 'medium', 'high').\n        budget_tokens: Maximum number of tokens allowed for the process.\n        max_attempts: Maximum number of retries for solving the problem.\n        no_direct_answer: Forces further thinking/search steps.\n        max_returned_urls: Maximum number of URLs in the final answer/chunk.\n        structured_output: JSON schema to ensure the final answer matches.\n        good_domains: List of domains to prioritize for content retrieval.\n        bad_domains: List of domains to strictly exclude from content retrieval.\n        only_domains: List of domains to exclusively include in content retrieval.\n\n    Returns:\n        If stream=False, a single dictionary representing the chat completion result (DeepSearchChatOutput).\n        If stream=True, an async generator yielding dictionaries (DeepSearchChatChunk).\n\n    Raises:\n        MCP specific errors on failure.\n    \"\"\"\n    if not deepsearch_client:\n        logger.error(\"DeepSearchClient is not initialized. Cannot process request.\")\n        # FastMCP typically handles exceptions by returning an error response\n        raise RuntimeError(\"DeepSearchClient failed to initialize. Check API key and configuration.\")\n\n    try:\n        # Create the input object from arguments\n        # Pydantic automatically validates types based on annotations\n        input_params = DeepSearchChatInput(\n            messages=messages,\n            model=model,\n            stream=stream,\n            reasoning_effort=reasoning_effort,\n            budget_tokens=budget_tokens,\n            max_attempts=max_attempts,\n            no_direct_answer=no_direct_answer,\n            max_returned_urls=max_returned_urls,\n            structured_output=structured_output,\n            good_domains=good_domains,\n            bad_domains=bad_domains,\n            only_domains=only_domains\n        )\n\n        logger.info(f\"Calling DeepSearch chat_completion tool (stream={stream})\")\n        result = await deepsearch_client.chat_completion(params=input_params)\n\n        if stream:\n            # Return the async generator directly\n            async def dict_generator() -> AsyncGenerator[Dict[str, Any], None]:\n                async for chunk in result: # type: ignore\n                    yield chunk.model_dump(exclude_none=True)\n            return dict_generator()\n        else:\n            # Return the single result as a dictionary\n            return result.model_dump(exclude_none=True) # type: ignore\n\n    except DeepSearchError as e:\n        logger.error(f\"DeepSearch API error in chat_completion tool: {e.message} (Status: {e.status_code})\", exc_info=True)\n        # Re-raise for FastMCP to handle and return a proper error response\n        # You might want to customize the error message or type here\n        raise RuntimeError(f\"DeepSearch API Error: {e.message}\") from e\n    except ValidationError as e:\n        logger.error(f\"Data validation error: {e}\", exc_info=True)\n        raise ValueError(f\"Invalid input or output data: {e}\") from e\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred in chat_completion tool\")\n        raise RuntimeError(f\"An unexpected server error occurred: {e}\") from e\n\n# --- Lifecycle Hooks (Optional) ---\n\n@mcp.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Clean up resources on server shutdown.\"\"\"\n    if deepsearch_client:\n        logger.info(\"Closing DeepSearchClient connection...\")\n        await deepsearch_client.close()\n        logger.info(\"DeepSearchClient connection closed.\")\n    else:\n        logger.info(\"No active DeepSearchClient to close.\")\n\n# --- Main Execution ---\n\nif __name__ == \"__main__\":\n    # This block allows running the server directly using Uvicorn\n    # For production, consider using a process manager like Gunicorn with Uvicorn workers\n    import uvicorn\n    port = int(os.getenv(\"PORT\", 8000))\n    host = os.getenv(\"HOST\", \"127.0.0.1\")\n    log_level_uvicorn = os.getenv(\"LOG_LEVEL\", \"info\").lower()\n\n    logger.info(f\"Starting DeepSearch MCP server on {host}:{port}\")\n    uvicorn.run(\"main:mcp\", host=host, port=port, log_level=log_level_uvicorn, reload=True)\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp\nhttpx>=0.25.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.20.0 # For running the server\nliteral_eval; python_version < '3.8' # For older python versions if needed by dependencies, check if necessary\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain from https://jina.ai/cloud/\nJINA_API_KEY=\"your_jina_api_key_here\"\n\n# Optional: Override the default DeepSearch API base URL\n# DEEPSEARCH_API_BASE_URL=\"https://deepsearch.jina.ai\"\n\n# Optional: Set the timeout for API requests (in seconds)\n# Defaults to 180 seconds in the client.py\nMCP_TIMEOUT=180\n\n# Optional: Set the logging level (e.g., DEBUG, INFO, WARNING, ERROR)\nLOG_LEVEL=INFO\n\n# Optional: Server host and port for Uvicorn (if running main.py directly)\n# HOST=127.0.0.1\n# PORT=8000\n"
    },
    {
      "name": "README.md",
      "content": "# Jina DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for the Jina AI DeepSearch API, built using FastMCP.\n\nThe Jina DeepSearch API allows users to perform complex search queries that require iterative reasoning, web searching, and reading to find the best answer. It is designed for complex questions needing world knowledge or up-to-date information and is compatible with the OpenAI Chat API schema.\n\nThis MCP server exposes the DeepSearch functionality as standardized tools that can be easily integrated into agentic workflows or other applications.\n\n## Features\n\n*   **Chat Completion:** Provides access to the core DeepSearch chat completion endpoint.\n*   **Streaming Support:** Handles streaming responses (`stream=True`) for real-time updates and reasoning steps, crucial for avoiding timeouts on complex queries.\n*   **Parameter Control:** Supports various DeepSearch parameters like `reasoning_effort`, `budget_tokens`, `max_attempts`, domain filtering (`good_domains`, `bad_domains`, `only_domains`), and `structured_output`.\n*   **Async Implementation:** Built with `asyncio` and `httpx` for efficient asynchronous operations.\n*   **Pydantic Models:** Uses Pydantic for robust data validation and clear schema definitions.\n*   **Configuration:** Easily configurable via environment variables.\n*   **Error Handling:** Includes error handling for API issues, timeouts, and validation errors.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository_url>\n    cd <repository_directory>\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    Copy the example environment file:\n    ```bash\n    cp .env.example .env\n    ```\n    Edit the `.env` file and add your Jina AI API key:\n    ```env\n    JINA_API_KEY=\"your_jina_api_key_here\"\n\n    # Optional: Adjust other settings if needed\n    # MCP_TIMEOUT=180\n    # LOG_LEVEL=INFO\n    ```\n    You can obtain a Jina API key from [Jina AI Cloud](https://jina.ai/cloud/).\n\n## Running the Server\n\nYou can run the MCP server using Uvicorn:\n\n```bash\n# Basic execution with auto-reload (for development)\nuvicorn main:mcp --reload\n\n# Specify host and port\nuvicorn main:mcp --host 0.0.0.0 --port 8080\n```\n\nThe server will start, and you can interact with it using an MCP client or tools like `curl`.\n\n## Environment Variables\n\n*   `JINA_API_KEY` (Required): Your Jina AI API key.\n*   `DEEPSEARCH_API_BASE_URL` (Optional): Override the default API base URL (`https://deepsearch.jina.ai`).\n*   `MCP_TIMEOUT` (Optional): Timeout in seconds for API requests (default: 180).\n*   `LOG_LEVEL` (Optional): Logging level (e.g., `DEBUG`, `INFO`, `WARNING`, `ERROR`) (default: `INFO`).\n*   `HOST` (Optional): Host address for the server (default: `127.0.0.1`).\n*   `PORT` (Optional): Port for the server (default: `8000`).\n\n## Tools\n\n### `chat_completion`\n\nExecutes a chat completion request using the DeepSearch engine.\n\n**Input Schema:** (`models.DeepSearchChatInput`)\n\n*   `messages` (List[`Message`], required): Conversation history.\n*   `model` (str, optional, default: `jina-deepsearch-v1`): Model ID.\n*   `stream` (bool, optional, default: `True`): Enable streaming.\n*   `reasoning_effort` (str, optional, default: `medium`): Reasoning effort (`low`, `medium`, `high`).\n*   `budget_tokens` (int, optional): Max tokens budget.\n*   `max_attempts` (int, optional): Max solution attempts.\n*   `no_direct_answer` (bool, optional, default: `False`): Force search steps.\n*   `max_returned_urls` (int, optional): Max URLs in response.\n*   `structured_output` (dict, optional): JSON schema for output.\n*   `good_domains` (List[str], optional): Prioritized domains.\n*   `bad_domains` (List[str], optional): Excluded domains.\n*   `only_domains` (List[str], optional): Exclusively included domains.\n\n**Returns:**\n\n*   If `stream=False`: A dictionary representing the full chat completion result (`models.DeepSearchChatOutput`).\n*   If `stream=True`: An asynchronous generator yielding dictionaries for each chunk (`models.DeepSearchChatChunk`).\n\n## Authentication\n\nThe server uses API Key authentication. The Jina API key must be provided via the `JINA_API_KEY` environment variable. The client includes this key in the `Authorization: Bearer <key>` header for requests to the DeepSearch API.\n\n## Error Handling\n\nThe server catches common errors:\n\n*   **HTTP Errors:** 4xx and 5xx errors from the DeepSearch API.\n*   **Connection Errors:** Issues connecting to the API.\n*   **Timeout Errors:** Requests taking longer than the configured `MCP_TIMEOUT`.\n*   **Validation Errors:** Issues with input data or unexpected API response formats.\n\nErrors are logged, and FastMCP will typically return a standardized error response to the client.\n\n## Rate Limits\n\nBe mindful of the Jina DeepSearch API rate limits associated with your API key:\n\n*   **Free Tier:** 2 requests per minute (RPM)\n*   **Standard Tier:** 10 RPM\n*   **Premium Tier:** 100 RPM\n\nExceeding these limits will result in HTTP 429 errors.\n"
    }
  ]
}