{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Union, Literal\n\n# --- Type Definitions from Plan ---\n\nclass MessageContentPart(BaseModel):\n    type: Literal[\"text\", \"image_url\"]\n    text: Optional[str] = None\n    image_url: Optional[Dict[str, str]] = None # e.g., {\"url\": \"data:image/jpeg;base64,...\"}\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal[\"user\", \"assistant\", \"system\"] # Added system for completeness, though API might only use user/assistant\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"The content of the message. Can be a plain string or a list for multimodal inputs.\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details about a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: Optional[str] = Field(None, description=\"URL of the source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation within the message content, like a URL citation.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass Delta(BaseModel):\n    \"\"\"The delta content for a streaming chunk.\"\"\"\n    role: Optional[Literal[\"assistant\"]] = Field(None, description=\"Role of the author ('assistant').\")\n    content: Optional[str] = Field(None, description=\"The content delta.\")\n    type: Optional[str] = Field(None, description=\"Type of content (e.g., 'text').\") # Note: API might not send this 'type' field in delta\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations associated with the content delta.\")\n\nclass Choice(BaseModel):\n    \"\"\"A choice in the chat completion response.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    delta: Optional[Delta] = Field(None, description=\"Content delta for streaming response.\")\n    message: Optional[Message] = Field(None, description=\"The full message object for non-streaming response.\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probability information (typically null).\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the model stopped generating tokens (e.g., 'stop').\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Tokens used by the prompt and reasoning process.\")\n    # Note: DeepSearch API might only return prompt_tokens in usage\n    completion_tokens: Optional[int] = Field(None, description=\"Tokens generated for the completion.\")\n    total_tokens: Optional[int] = Field(None, description=\"Total tokens used.\")\n\n# --- Input Model for the Tool ---\n\nclass DeepSearchChatParams(BaseModel):\n    \"\"\"Input parameters for the DeepSearch chat_completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use. Currently only 'jina-deepsearch-v1' is supported.\")\n    stream: Optional[bool] = Field(True, description=\"Whether to stream back partial progress. Recommended to keep enabled to avoid timeouts.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains effort on reasoning. Supported values: 'low', 'medium', 'high'. Default is 'medium'.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides 'reasoning_effort'.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Overrides 'reasoning_effort'.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces the model to take further thinking/search steps even for seemingly trivial queries. Default is false.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema to ensure the final answer matches the supplied structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        # Ensure default values are used correctly\n        use_enum_values = True\n\n# --- Response Models (Mimicking OpenAI Schema) ---\n\nclass ChatCompletionResponse(BaseModel):\n    \"\"\"Response model for non-streaming chat completion, compatible with OpenAI schema.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(\"chat.completion\", description=\"The object type, which is always 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[Choice] = Field(..., description=\"A list of chat completion choices. Can be more than one if n > 1 was requested.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics for the completion request.\")\n    # DeepSearch specific fields\n    visitedURLs: Optional[List[str]] = Field(None, description=\"URLs visited during the search process.\")\n    readURLs: Optional[List[str]] = Field(None, description=\"URLs read and used for reasoning.\")\n    numURLs: Optional[int] = Field(None, description=\"Number of URLs read.\")\n\nclass ChatCompletionChunk(BaseModel):\n    \"\"\"Response model for streaming chat completion chunks, compatible with OpenAI schema.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion. Each chunk has the same ID.\")\n    object: str = Field(\"chat.completion.chunk\", description=\"The object type, which is always 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[Choice] = Field(..., description=\"A list of chat completion choices. For streaming, this usually contains one choice with a delta.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics for the completion request. Only appears in the final chunk.\")\n    # DeepSearch specific fields (might appear in chunks, especially final one)\n    visitedURLs: Optional[List[str]] = Field(None)\n    readURLs: Optional[List[str]] = Field(None)\n    numURLs: Optional[int] = Field(None)\n"
    },
    {
      "name": "client.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import Dict, Any, AsyncGenerator, Optional\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatParams, ChatCompletionResponse, ChatCompletionChunk, Message\n\nlogger = logging.getLogger(__name__)\n\nclass DeepSearchError(Exception):\n    \"\"\"Custom exception for DeepSearch API errors.\"\"\"\n    def __init__(self, status_code: int, message: str):\n        self.status_code = status_code\n        self.message = message\n        super().__init__(f\"DeepSearch API Error {status_code}: {message}\")\n\nclass DeepSearchClient:\n    \"\"\"Client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://deepsearch.jina.ai/v1\"\n    DEFAULT_TIMEOUT = 120.0 # Increased timeout for potentially long searches\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = DEFAULT_TIMEOUT):\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"JINA_API_KEY environment variable not set.\")\n\n        self.base_url = base_url or self.DEFAULT_BASE_URL\n        self.timeout = timeout\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Explicitly accept JSON for non-streaming\n        }\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=self.timeout\n        )\n\n    async def _request(\n        self,\n        method: str,\n        endpoint: str,\n        payload: Optional[Dict[str, Any]] = None,\n        stream: bool = False\n    ) -> httpx.Response:\n        \"\"\"Makes an HTTP request to the DeepSearch API.\"\"\"\n        logger.info(f\"Making {method} request to {endpoint} (stream={stream})\")\n        if payload:\n            logger.debug(f\"Request payload: {json.dumps(payload, indent=2)}\")\n\n        try:\n            if stream:\n                # For streaming, we need different headers and use stream context\n                stream_headers = self.headers.copy()\n                stream_headers[\"Accept\"] = \"text/event-stream\"\n                req = self.client.build_request(method, endpoint, json=payload, headers=stream_headers)\n                response = await self.client.send(req, stream=True)\n            else:\n                response = await self.client.request(method, endpoint, json=payload)\n\n            response.raise_for_status() # Raise HTTPStatusError for 4xx/5xx\n            logger.info(f\"Request successful (Status: {response.status_code})\")\n            return response\n\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out after {self.timeout}s: {e}\")\n            raise DeepSearchError(status_code=408, message=f\"Request timed out: {e}\")\n        except httpx.HTTPStatusError as e:\n            error_message = f\"HTTP error {e.response.status_code}: {e.response.text}\"\n            try:\n                # Attempt to parse error details from response body\n                error_details = e.response.json()\n                error_message = error_details.get('detail', error_message)\n            except json.JSONDecodeError:\n                pass # Use the raw text if JSON parsing fails\n\n            logger.error(error_message)\n            status_code = e.response.status_code\n            if status_code == 401:\n                raise DeepSearchError(status_code=status_code, message=\"Authentication failed. Check your JINA_API_KEY.\")\n            elif status_code == 429:\n                raise DeepSearchError(status_code=status_code, message=f\"Rate limit exceeded. Details: {error_message}\")\n            elif status_code == 400:\n                 raise DeepSearchError(status_code=status_code, message=f\"Invalid request (400): {error_message}\")\n            else:\n                raise DeepSearchError(status_code=status_code, message=error_message)\n        except httpx.RequestError as e:\n            logger.error(f\"An unexpected network error occurred: {e}\")\n            raise DeepSearchError(status_code=500, message=f\"Network error: {e}\")\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during request: {e}\", exc_info=True)\n            raise DeepSearchError(status_code=500, message=f\"Unexpected error: {e}\")\n\n    async def chat_completion(\n        self,\n        params: DeepSearchChatParams\n    ) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:\n        \"\"\"\n        Performs a deep search chat completion.\n\n        Args:\n            params: The parameters for the chat completion.\n\n        Returns:\n            If stream=False, returns a dictionary representing the ChatCompletionResponse.\n            If stream=True, returns an async generator yielding dictionaries representing ChatCompletionChunks.\n        \"\"\"\n        endpoint = \"/chat/completions\"\n        # Use Pydantic's dict method to handle serialization and exclude None values\n        payload = params.dict(exclude_none=True)\n\n        if params.stream:\n            # Return the async generator directly for streaming\n            return self._process_stream(endpoint, payload)\n        else:\n            # Make a non-streaming request and parse the response\n            response = await self._request(\"POST\", endpoint, payload=payload, stream=False)\n            try:\n                response_data = response.json()\n                # Validate response structure (optional but recommended)\n                # ChatCompletionResponse.parse_obj(response_data)\n                logger.debug(f\"Received non-streaming response: {json.dumps(response_data, indent=2)}\")\n                return response_data\n            except json.JSONDecodeError as e:\n                logger.error(f\"Failed to decode JSON response: {e}. Response text: {response.text}\")\n                raise DeepSearchError(status_code=500, message=\"Invalid JSON response from server\")\n            except ValidationError as e:\n                logger.error(f\"Response validation failed: {e}\")\n                raise DeepSearchError(status_code=500, message=f\"Invalid response structure: {e}\")\n\n    async def _process_stream(\n        self,\n        endpoint: str,\n        payload: Dict[str, Any]\n    ) -> AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Processes the SSE stream from the API.\"\"\"\n        try:\n            response = await self._request(\"POST\", endpoint, payload=payload, stream=True)\n            buffer = \"\"\n            async for line in response.aiter_lines():\n                if not line:\n                    # End of an event\n                    if buffer.startswith(\"data: \"):\n                        data_str = buffer[len(\"data: \"):].strip()\n                        if data_str == \"[DONE]\":\n                            logger.info(\"Stream finished with [DONE] message.\")\n                            break\n                        try:\n                            chunk_data = json.loads(data_str)\n                            # Validate chunk structure (optional)\n                            # ChatCompletionChunk.parse_obj(chunk_data)\n                            logger.debug(f\"Received stream chunk: {json.dumps(chunk_data)}\")\n                            yield chunk_data\n                        except json.JSONDecodeError:\n                            logger.warning(f\"Could not decode JSON from stream data: {data_str}\")\n                        except ValidationError as e:\n                            logger.warning(f\"Stream chunk validation failed: {e}. Chunk: {data_str}\")\n                        except Exception as e:\n                             logger.error(f\"Error processing stream chunk: {e}. Chunk: {data_str}\", exc_info=True)\n                    buffer = \"\"\n                else:\n                    buffer += line + \"\\n\"\n\n        except DeepSearchError as e:\n            # Logged in _request, re-raise or handle if needed\n            logger.error(f\"DeepSearchError during streaming: {e}\")\n            raise # Re-raise the specific error\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during streaming: {e}\", exc_info=True)\n            raise DeepSearchError(status_code=500, message=f\"Unexpected streaming error: {e}\")\n        finally:\n            if 'response' in locals() and response is not None:\n                await response.aclose()\n            logger.info(\"Stream processing finished.\")\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n        logger.info(\"DeepSearchClient closed.\")\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP\nfrom typing import Dict, Any, List, Union, AsyncGenerator\nimport logging\nimport os\nimport asyncio\nimport json\nfrom dotenv import load_dotenv\n\nfrom models import DeepSearchChatParams, ChatCompletionResponse, ChatCompletionChunk, Message\nfrom client import DeepSearchClient, DeepSearchError\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- Initialize MCP Server ---\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP service providing access to the Jina AI DeepSearch API. DeepSearch combines web searching, reading, and reasoning to answer complex questions requiring up-to-date information or iterative investigation.\"\n)\n\n# --- Initialize API Client ---\ntry:\n    api_client = DeepSearchClient()\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize DeepSearchClient: {e}\")\n    # Exit if the client cannot be initialized (e.g., missing API key)\n    import sys\n    sys.exit(f\"Error: {e}\")\n\n# --- Define MCP Tool ---\n@mcp.tool()\nasync def chat_completion(params: DeepSearchChatParams) -> Dict[str, Any]:\n    \"\"\"\n    Performs a deep search and reasoning process using the Jina AI DeepSearch API.\n\n    Accepts chat messages and various parameters to control the search and reasoning.\n    Supports both streaming and non-streaming responses.\n\n    Args:\n        params (DeepSearchChatParams): Input parameters including messages, model, stream flag, etc.\n\n    Returns:\n        Dict[str, Any]:\n            - If stream=False, returns the complete Chat Completion object.\n            - If stream=True, consumes the stream and returns a consolidated final response\n              containing the full message, usage stats, and finish reason.\n              (Note: This MCP tool aggregates the stream, it doesn't stream back to the MCP caller).\n    \"\"\"\n    logger.info(f\"Received chat_completion request (stream={params.stream})\")\n    try:\n        result = await api_client.chat_completion(params)\n\n        if isinstance(result, AsyncGenerator):\n            logger.info(\"Processing stream...\")\n            # Consume the stream and aggregate the result\n            final_response = await _aggregate_stream(result)\n            logger.info(\"Stream processing complete.\")\n            return final_response\n        else:\n            # Non-streaming response\n            logger.info(\"Received non-streaming response.\")\n            # Ensure the response is a dictionary (it should be from the client)\n            if isinstance(result, dict):\n                return result\n            else:\n                 logger.error(f\"Unexpected non-streaming response type: {type(result)}\")\n                 return {\"error\": \"Received unexpected response format from API client.\", \"status_code\": 500}\n\n    except DeepSearchError as e:\n        logger.error(f\"DeepSearch API error in chat_completion tool: {e}\")\n        return {\"error\": e.message, \"status_code\": e.status_code}\n    except Exception as e:\n        logger.error(f\"Unexpected error in chat_completion tool: {e}\", exc_info=True)\n        return {\"error\": f\"An unexpected internal error occurred: {str(e)}\", \"status_code\": 500}\n\nasync def _aggregate_stream(stream: AsyncGenerator[Dict[str, Any], None]) -> Dict[str, Any]:\n    \"\"\"Consumes the SSE stream and aggregates chunks into a final response object.\"\"\"\n    full_content = \"\"\n    final_chunk = None\n    all_chunks = [] # Store all chunks for potential debugging or richer info\n    final_usage = None\n    final_choice = None\n    response_id = None\n    created_time = None\n    model_name = None\n    visited_urls = None\n    read_urls = None\n    num_urls = None\n\n    async for chunk_dict in stream:\n        all_chunks.append(chunk_dict)\n        try:\n            chunk = ChatCompletionChunk.parse_obj(chunk_dict)\n            if not response_id: response_id = chunk.id\n            if not created_time: created_time = chunk.created\n            if not model_name: model_name = chunk.model\n\n            if chunk.choices:\n                delta = chunk.choices[0].delta\n                if delta and delta.content:\n                    full_content += delta.content\n                if chunk.choices[0].finish_reason:\n                    final_choice = chunk.choices[0] # Store the choice with the finish reason\n\n            # Usage info usually comes in the last chunk\n            if chunk.usage:\n                final_usage = chunk.usage\n\n            # DeepSearch specific fields might appear in later chunks\n            if chunk.visitedURLs is not None: visited_urls = chunk.visitedURLs\n            if chunk.readURLs is not None: read_urls = chunk.readURLs\n            if chunk.numURLs is not None: num_urls = chunk.numURLs\n\n            final_chunk = chunk # Keep track of the last chunk\n\n        except Exception as e:\n            logger.warning(f\"Failed to process or validate stream chunk: {e}. Chunk: {chunk_dict}\")\n            # Continue processing other chunks if possible\n\n    # Construct the final response mimicking ChatCompletionResponse structure\n    if not final_choice:\n        # If stream ended unexpectedly without a finish_reason\n        logger.warning(\"Stream ended without a final choice containing finish_reason.\")\n        if final_chunk and final_chunk.choices: # Use last known choice\n             final_choice = final_chunk.choices[0]\n        else: # Create a placeholder choice if none exists\n             final_choice = Choice(index=0, message=Message(role=\"assistant\", content=full_content), finish_reason=\"incomplete\")\n    else:\n         # Ensure the final choice has the full aggregated content\n         final_choice.message = Message(role=\"assistant\", content=full_content)\n         final_choice.delta = None # Remove delta from final aggregated choice\n\n    aggregated_response = {\n        \"id\": response_id or \"unknown_stream_id\",\n        \"object\": \"chat.completion\", # Mimic non-streaming object type\n        \"created\": created_time or 0,\n        \"model\": model_name or \"unknown_model\",\n        \"choices\": [final_choice.dict(exclude_none=True)] if final_choice else [],\n        \"usage\": final_usage.dict(exclude_none=True) if final_usage else None,\n        \"visitedURLs\": visited_urls,\n        \"readURLs\": read_urls,\n        \"numURLs\": num_urls,\n        \"_raw_chunks_count\": len(all_chunks) # Add metadata about the stream aggregation\n    }\n\n    return aggregated_response\n\n# --- Graceful Shutdown --- \n@mcp.app.on_event(\"shutdown\")\nasync def shutdown_event():\n    logger.info(\"Shutting down MCP server...\")\n    await api_client.close()\n    logger.info(\"DeepSearch client closed.\")\n\n# --- Run Server ---\nif __name__ == \"__main__\":\n    # Use uvicorn to run the server\n    # Example: uvicorn main:mcp.app --host 0.0.0.0 --port 8000 --reload\n    logger.info(\"Starting DeepSearch MCP server. Run with Uvicorn, e.g.:\")\n    logger.info(\"uvicorn main:mcp.app --host 0.0.0.0 --port 8000\")\n    # The following line is for simple execution context, but uvicorn is preferred for production\n    # mcp.run() # This might not work as expected for async shutdown, use uvicorn\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp\nhttpx>=0.20,<0.28 # Specify upper bound for stability\npydantic>=1.8,<2.0 # Sticking to V1 as per plan, adjust if needed\npython-dotenv>=0.15\nuvicorn>=0.15\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain your key from https://jina.ai/cloud/\nJINA_API_KEY=\"YOUR_JINA_API_KEY_HERE\"\n\n# Optional: Override the default DeepSearch API base URL\n# DEEPSEARCH_BASE_URL=\"https://deepsearch.jina.ai/v1\"\n"
    },
    {
      "name": "README.md",
      "content": "# DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for the Jina AI DeepSearch API, built using FastMCP.\n\n## Description\n\nThe Jina AI DeepSearch API combines web searching, reading, and reasoning capabilities to answer complex questions that require up-to-date information or iterative investigation. It is designed to be compatible with the OpenAI Chat API schema.\n\nThis MCP server provides a standardized interface to access the DeepSearch API's `chat_completion` functionality.\n\n## Features\n\n*   Provides the `chat_completion` tool via MCP.\n*   Supports all parameters of the DeepSearch API endpoint (`model`, `messages`, `stream`, `reasoning_effort`, domain filtering, etc.).\n*   Handles both streaming and non-streaming responses from the API.\n*   Includes robust error handling for API errors, timeouts, and network issues.\n*   Uses Pydantic for request and response validation.\n*   Requires a Jina AI API key for authentication.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository_url>\n    cd <repository_directory>\n    ```\n\n2.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina AI API key:\n        ```env\n        JINA_API_KEY=\"YOUR_JINA_API_KEY_HERE\"\n        ```\n        You can obtain an API key from the [Jina AI Cloud Dashboard](https://jina.ai/cloud/).\n\n## Running the Server\n\nUse `uvicorn` to run the FastMCP application:\n\n```bash\nuvicorn main:mcp.app --host 0.0.0.0 --port 8000 --reload\n```\n\n*   `--host 0.0.0.0`: Makes the server accessible on your network.\n*   `--port 8000`: Specifies the port to run on (default for FastMCP is often 8000).\n*   `--reload`: Enables auto-reloading when code changes (useful for development).\n\nThe MCP server will be available at `http://localhost:8000` (or the specified host/port).\n\n## Available Tools\n\n### `chat_completion`\n\nPerforms a deep search and reasoning process to answer a user query based on web search results and iterative thinking.\n\n**Input Parameters (as a JSON object matching `DeepSearchChatParams`):**\n\n*   `messages` (List[Message], **required**): A list of message objects representing the conversation history. Each message has `role` ('user' or 'assistant') and `content` (string or list for multimodal).\n*   `model` (str, **required**): Model ID. Defaults to `jina-deepsearch-v1`.\n*   `stream` (bool, optional): Whether to stream results. Defaults to `true`. Note: While the API supports streaming, this MCP tool consumes the stream and returns a single aggregated response when `stream=true`.\n*   `reasoning_effort` (str, optional): 'low', 'medium', or 'high'. Defaults to `medium`.\n*   `budget_tokens` (int, optional): Max tokens for the process. Overrides `reasoning_effort`.\n*   `max_attempts` (int, optional): Max retries for solving. Overrides `reasoning_effort`.\n*   `no_direct_answer` (bool, optional): Force search/thinking even for simple queries. Defaults to `false`.\n*   `max_returned_urls` (int, optional): Max URLs in the final answer.\n*   `structured_output` (Dict, optional): JSON schema for structured output.\n*   `good_domains` (List[str], optional): Prioritized domains.\n*   `bad_domains` (List[str], optional): Excluded domains.\n*   `only_domains` (List[str], optional): Exclusively included domains.\n\n**Returns (JSON object):**\n\n*   If `stream=false`, returns the standard Chat Completion JSON response from the API.\n*   If `stream=true`, returns an aggregated JSON response mimicking the non-streaming structure, containing the complete message content, final usage statistics, finish reason, and potentially DeepSearch-specific metadata like URLs visited/read.\n\n## Authentication\n\nThe server uses Bearer Token authentication. The Jina AI API key provided in the `.env` file (`JINA_API_KEY`) is automatically included in the `Authorization` header for requests to the DeepSearch API.\n\n## Error Handling\n\nThe server attempts to catch common errors:\n\n*   **Authentication Errors (401):** If the API key is invalid or missing.\n*   **Rate Limit Errors (429):** If the API rate limit is exceeded.\n*   **Invalid Request Errors (400):** If the input parameters are invalid.\n*   **Server Errors (5xx):** If the DeepSearch API encounters an internal error.\n*   **Timeout Errors:** If the request takes longer than the configured timeout (default 120s).\n*   **Network Errors:** If there's trouble connecting to the API.\n\nErrors are returned as a JSON object with `error` and `status_code` fields.\n\n## Rate Limits\n\nBe aware of the Jina AI DeepSearch API rate limits associated with your API key:\n*   **Free Key:** ~2 requests per minute (RPM)\n*   **Standard Key:** ~10 RPM\n*   **Premium Key:** ~100 RPM\n\nThe client currently does not implement automatic retries or backoff for rate limits, but will return a 429 error.\n"
    }
  ]
}