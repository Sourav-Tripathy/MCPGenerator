{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Literal\n\n# --- Type Definitions from Plan ---\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant'] = Field(..., description=\"The role of the message author ('user' or 'assistant').\")\n    content: str = Field(..., description=\"The content of the message. Can be plain text or a data URI for images/documents.\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Represents a citation linked to a URL found during the search.\"\"\"\n    title: Optional[str] = Field(None, description=\"The title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, alias=\"exactQuote\", description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"The URL of the source.\")\n    dateTime: Optional[str] = Field(None, alias=\"dateTime\", description=\"The timestamp when the URL was accessed or content was published (ISO 8601 format).\")\n\n# --- Input Model for chat_completion Tool ---\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the Jina DeepSearch chat completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation so far. The last message should be from the user. Content can include text or data URIs for images/documents (up to 10MB).\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(True, description=\"Whether to stream the response. Strongly recommended to keep true to avoid timeouts. The MCP tool will handle aggregation if true.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains reasoning effort. Supported values: 'low', 'medium', 'high'.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum tokens allowed for the DeepSearch process. Overrides reasoning_effort.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Overrides reasoning_effort.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces search/thinking steps even for seemingly trivial queries.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema to ensure the final answer matches the structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        # Ensure Pydantic uses the alias for serialization\n        populate_by_name = True\n\n# --- Output/Response Models ---\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics.\"\"\"\n    prompt_tokens: int = Field(..., description=\"Number of tokens in the prompt.\")\n    completion_tokens: Optional[int] = Field(None, description=\"Number of tokens in the generated completion.\") # Optional as it might not be present in all stream chunks\n    total_tokens: int = Field(..., description=\"Total number of tokens used in the request (prompt + completion).\")\n\nclass FinishDetails(BaseModel):\n    \"\"\"Details about why the generation finished, including citations.\"\"\"\n    type: str = Field(..., description=\"Reason the generation finished (e.g., 'stop', 'length').\")\n    stop: Optional[str] = Field(None, description=\"The stop sequence that triggered the finish, if any.\")\n    url_citations: Optional[List[UrlCitation]] = Field(None, alias=\"url_citations\", description=\"List of URLs cited in the response.\")\n\n    class Config:\n        populate_by_name = True\n\nclass ResponseMessage(BaseModel):\n    \"\"\"Message object within the response choice.\"\"\"\n    role: Literal['assistant'] = Field(..., description=\"Role of the message author.\")\n    content: Optional[str] = Field(None, description=\"The content of the message.\") # Content can be None initially and built up\n\nclass Choice(BaseModel):\n    \"\"\"A single response choice.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    message: ResponseMessage = Field(..., description=\"The message generated by the model.\")\n    finish_details: Optional[FinishDetails] = Field(None, alias=\"finish_details\", description=\"Details about why the generation finished.\")\n    # finish_reason is common in OpenAI spec, but DeepSearch uses finish_details\n    # finish_reason: Optional[str] = Field(None, description=\"Reason the generation finished.\")\n\n    class Config:\n        populate_by_name = True\n\nclass DeepSearchChatResponse(BaseModel):\n    \"\"\"The aggregated final response from the DeepSearch process.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(..., description=\"The object type, which is always 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[Choice] = Field(..., description=\"A list of chat completion choices. Usually only one.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics for the completion request.\")\n\n# --- Models for Streaming Chunks (Internal use in API client) ---\n\nclass StreamChoiceDelta(BaseModel):\n    \"\"\"Delta content within a streaming choice.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None)\n    content: Optional[str] = Field(None)\n\nclass StreamChoice(BaseModel):\n    \"\"\"A single choice within a streaming chunk.\"\"\"\n    index: int\n    delta: StreamChoiceDelta\n    finish_details: Optional[FinishDetails] = Field(None, alias=\"finish_details\")\n    # finish_reason: Optional[str] = None # Not typically in delta, but in final chunk's choice\n\n    class Config:\n        populate_by_name = True\n\nclass DeepSearchStreamChunk(BaseModel):\n    \"\"\"Represents a single chunk received during streaming.\"\"\"\n    id: str\n    object: str # e.g., 'chat.completion.chunk'\n    created: int\n    model: str\n    choices: List[StreamChoice]\n    usage: Optional[Usage] = None # Usually present only in the last chunk\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import AsyncGenerator, Optional, Dict, Any\n\nfrom models import DeepSearchChatInput, DeepSearchChatResponse, DeepSearchStreamChunk, ResponseMessage, Choice, Usage, FinishDetails\n\nlogger = logging.getLogger(__name__) \n\n# Custom Exceptions\nclass DeepSearchError(Exception):\n    \"\"\"Base exception for DeepSearch API errors.\"\"\"\n    pass\n\nclass AuthenticationError(DeepSearchError):\n    \"\"\"Exception raised for authentication errors (401).\"\"\"\n    pass\n\nclass RateLimitError(DeepSearchError):\n    \"\"\"Exception raised for rate limit errors (429).\"\"\"\n    pass\n\nclass InvalidRequestError(DeepSearchError):\n    \"\"\"Exception raised for invalid request errors (400).\"\"\"\n    pass\n\nclass ServerError(DeepSearchError):\n    \"\"\"Exception raised for server-side errors (5xx).\"\"\"\n    pass\n\nclass JinaDeepSearchClient:\n    \"\"\"Client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, timeout: float = 180.0): # Increased timeout for potentially long searches\n        \"\"\"\n        Initializes the JinaDeepSearchClient.\n\n        Args:\n            api_key: The Jina AI API key. Defaults to JINA_API_KEY environment variable.\n            timeout: Request timeout in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise AuthenticationError(\"JINA_API_KEY not found in environment variables or provided directly.\")\n\n        self.base_url = \"https://deepsearch.jina.ai\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Ensure we accept JSON\n        }\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=timeout\n        )\n\n    async def _handle_error(self, response: httpx.Response):\n        \"\"\"Raises appropriate exceptions based on HTTP status code.\"\"\"\n        if response.status_code == 401:\n            raise AuthenticationError(f\"Authentication failed: {response.text}\")\n        elif response.status_code == 429:\n            raise RateLimitError(f\"Rate limit exceeded: {response.text}\")\n        elif response.status_code == 400:\n            raise InvalidRequestError(f\"Invalid request: {response.text}\")\n        elif response.status_code >= 500:\n            raise ServerError(f\"Server error ({response.status_code}): {response.text}\")\n        else:\n            response.raise_for_status() # Raise for other 4xx errors\n\n    async def _process_stream(self, response: httpx.Response) -> DeepSearchChatResponse:\n        \"\"\"\n        Processes the SSE stream and aggregates the response.\n\n        Args:\n            response: The streaming HTTPX response.\n\n        Returns:\n            The aggregated DeepSearchChatResponse.\n        \n        Raises:\n            DeepSearchError: If the stream ends unexpectedly or contains errors.\n        \"\"\"\n        aggregated_content = \"\"\n        final_chunk_data: Dict[str, Any] = {}\n        final_usage: Optional[Usage] = None\n        final_finish_details: Optional[FinishDetails] = None\n        first_chunk_processed = False\n\n        try:\n            async for line in response.aiter_lines():\n                if line.startswith(\"data:\"):\n                    data_str = line[len(\"data:\"):].strip()\n                    if data_str == \"[DONE]\":\n                        break\n                    try:\n                        chunk_json = json.loads(data_str)\n                        chunk = DeepSearchStreamChunk.model_validate(chunk_json)\n\n                        if not first_chunk_processed:\n                            # Store metadata from the first chunk\n                            final_chunk_data = chunk.model_dump(exclude={'choices', 'usage'})\n                            first_chunk_processed = True\n\n                        if chunk.choices:\n                            delta = chunk.choices[0].delta\n                            if delta.content:\n                                aggregated_content += delta.content\n                            \n                            # Capture finish_details when it appears\n                            if chunk.choices[0].finish_details:\n                                final_finish_details = chunk.choices[0].finish_details\n\n                        # Usage info usually comes in the last chunk\n                        if chunk.usage:\n                            final_usage = chunk.usage\n\n                    except json.JSONDecodeError:\n                        logger.error(f\"Failed to decode JSON from stream: {data_str}\")\n                        raise DeepSearchError(f\"Invalid JSON received in stream: {data_str}\")\n                    except Exception as e:\n                        logger.error(f\"Error processing stream chunk: {e}\")\n                        raise DeepSearchError(f\"Error processing stream chunk: {e}\")\n        \n        except httpx.ReadTimeout:\n            logger.error(\"Read timeout during streaming\")\n            raise DeepSearchError(\"Timeout while reading stream response.\")\n        except httpx.RemoteProtocolError as e:\n             logger.error(f\"Remote protocol error during streaming: {e}\")\n             raise DeepSearchError(f\"Connection error during streaming: {e}\")\n        finally:\n            await response.aclose()\n\n        if not first_chunk_processed:\n             raise DeepSearchError(\"Stream ended unexpectedly without receiving any data chunks.\")\n\n        # Construct the final response object\n        final_message = ResponseMessage(role='assistant', content=aggregated_content)\n        final_choice = Choice(\n            index=0, \n            message=final_message, \n            finish_details=final_finish_details\n        )\n        \n        # Ensure essential fields are present before creating the final response\n        if 'id' not in final_chunk_data or 'created' not in final_chunk_data or 'model' not in final_chunk_data:\n             raise DeepSearchError(\"Essential metadata (id, created, model) missing from stream chunks.\")\n\n        final_response = DeepSearchChatResponse(\n            id=final_chunk_data['id'],\n            object='chat.completion', # Hardcode as per OpenAI spec for aggregated response\n            created=final_chunk_data['created'],\n            model=final_chunk_data['model'],\n            choices=[final_choice],\n            usage=final_usage\n        )\n\n        return final_response\n\n    async def chat_completion(self, params: DeepSearchChatInput) -> DeepSearchChatResponse:\n        \"\"\"\n        Performs a deep search chat completion.\n\n        Args:\n            params: Input parameters for the chat completion.\n\n        Returns:\n            The chat completion response.\n        \n        Raises:\n            AuthenticationError: If API key is invalid.\n            RateLimitError: If rate limit is exceeded.\n            InvalidRequestError: If the request payload is invalid.\n            ServerError: If the Jina API encounters an error.\n            DeepSearchError: For other API or processing errors.\n            httpx.TimeoutException: If the request times out.\n        \"\"\"\n        endpoint = \"/v1/chat/completions\"\n        \n        # Ensure stream is True for internal processing, even if user set it to False\n        # The MCP tool expects aggregation.\n        payload = params.model_dump(exclude_none=True, by_alias=True)\n        payload['stream'] = True # Force streaming for aggregation\n\n        logger.info(f\"Sending request to {self.base_url}{endpoint} with model {params.model}\")\n        # logger.debug(f\"Payload: {payload}\") # Be careful logging payloads with potentially sensitive data\n\n        try:\n            async with self.client.stream(\"POST\", endpoint, json=payload) as response:\n                await self._handle_error(response) # Check status before starting stream processing\n                logger.info(\"Stream started successfully.\")\n                result = await self._process_stream(response)\n                logger.info(\"Stream processed successfully.\")\n                return result\n                \n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out: {e}\")\n            raise\n        except httpx.HTTPError as e:\n            # Errors during connection or non-2xx that weren't handled by _handle_error (shouldn't happen often)\n            logger.error(f\"HTTP error during request: {e}\")\n            # Attempt to parse error response if available\n            try:\n                error_details = e.response.text\n            except Exception:\n                error_details = str(e)\n            raise DeepSearchError(f\"HTTP error: {error_details}\") from e\n        except DeepSearchError as e:\n             logger.error(f\"DeepSearch API or processing error: {e}\")\n             raise # Re-raise known API errors\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred: {e}\")\n            raise DeepSearchError(f\"An unexpected error occurred: {str(e)}\") from e\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP\nfrom typing import List, Optional, Dict, Any\nimport logging\nimport os\nfrom dotenv import load_dotenv\n\n# Import models and API client\nfrom models import DeepSearchChatInput, DeepSearchChatResponse, Message\nfrom api import JinaDeepSearchClient, DeepSearchError, AuthenticationError, RateLimitError, InvalidRequestError, ServerError\n\n# --- Configuration ---\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# --- MCP Server Initialization ---\n\nmcp = FastMCP(\n    service_name=\"jina_deepsearch\",\n    description=\"MCP service for Jina AI DeepSearch, providing iterative web search, reading, and reasoning capabilities to answer complex questions. It utilizes the Jina AI DeepSearch API, which is compatible with the OpenAI Chat API schema.\"\n)\n\n# --- API Client Initialization ---\n\ntry:\n    api_client = JinaDeepSearchClient()\nexcept AuthenticationError as e:\n    logger.error(f\"Failed to initialize JinaDeepSearchClient: {e}\")\n    # Allow server to start but tools will fail if client wasn't initialized\n    api_client = None \nexcept Exception as e:\n    logger.error(f\"Unexpected error initializing JinaDeepSearchClient: {e}\")\n    api_client = None\n\n# --- MCP Tool Definition ---\n\n@mcp.tool(\n    name=\"chat_completion\",\n    description=\"Performs a deep search and reasoning process based on a conversation history. It iteratively searches the web, reads content, and reasons to find the best answer to the user's query. Supports text, image (webp, png, jpeg as data URI), and document (txt, pdf as data URI) inputs in messages. Returns the final aggregated answer along with metadata.\",\n    input_model=DeepSearchChatInput,\n    returns=DeepSearchChatResponse\n)\nasync def chat_completion(params: DeepSearchChatInput) -> Dict[str, Any]:\n    \"\"\"\n    MCP tool to perform Jina AI DeepSearch chat completion.\n\n    Handles streaming internally and returns the aggregated response.\n\n    Args:\n        params: Input parameters matching the DeepSearchChatInput model.\n\n    Returns:\n        A dictionary representing the DeepSearchChatResponse or an error dictionary.\n    \"\"\"\n    if api_client is None:\n        logger.error(\"JinaDeepSearchClient is not initialized. Cannot process request.\")\n        return {\"error\": \"Service configuration error: API client not initialized.\"}\n\n    logger.info(f\"Received chat_completion request with model: {params.model}\")\n    \n    # The API client internally forces stream=True and handles aggregation.\n    # We pass the user's preference for stream just in case, but it's overridden.\n    # We don't need to change params.stream here.\n\n    try:\n        result: DeepSearchChatResponse = await api_client.chat_completion(params)\n        logger.info(f\"Successfully completed chat_completion request {result.id}\")\n        # Return the Pydantic model's dictionary representation\n        return result.model_dump(by_alias=True, exclude_none=True)\n\n    except AuthenticationError as e:\n        logger.error(f\"Authentication error: {e}\")\n        return {\"error\": f\"Authentication failed: {e}\"}\n    except RateLimitError as e:\n        logger.warning(f\"Rate limit error: {e}\")\n        return {\"error\": f\"Rate limit exceeded: {e}\"}\n    except InvalidRequestError as e:\n        logger.error(f\"Invalid request error: {e}\")\n        return {\"error\": f\"Invalid request: {e}\"}\n    except ServerError as e:\n        logger.error(f\"Server error: {e}\")\n        return {\"error\": f\"Jina API server error: {e}\"}\n    except DeepSearchError as e:\n        logger.error(f\"DeepSearch API error: {e}\")\n        return {\"error\": f\"DeepSearch API error: {e}\"}\n    except httpx.TimeoutException:\n        logger.error(\"Request to Jina API timed out.\")\n        return {\"error\": \"Request timed out while contacting the Jina DeepSearch API.\"}\n    except Exception as e:\n        logger.exception(f\"Unexpected error in chat_completion tool: {e}\") # Log full traceback\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\n# --- Server Shutdown Hook ---\n\n@mcp.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Cleanly closes the API client connection on server shutdown.\"\"\"\n    if api_client:\n        logger.info(\"Closing Jina DeepSearch API client...\")\n        await api_client.close()\n        logger.info(\"Jina DeepSearch API client closed.\")\n    else:\n        logger.info(\"No API client to close.\")\n\n# --- Run Server ---\n\nif __name__ == \"__main__\":\n    logger.info(\"Starting Jina DeepSearch MCP server...\")\n    # Note: FastMCP's run() method handles the ASGI server setup (like uvicorn)\n    mcp.run()\n"
    },
    {
      "name": "requirements.txt",
      "content": "mcp>=0.1.0\nfastmcp>=0.1.0\nhttpx>=0.23.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\n# uvicorn is usually included via fastmcp or needed to run directly\nuvicorn>=0.15.0 \n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain your key from Jina AI (https://jina.ai/)\nJINA_API_KEY=your_jina_api_key_here\n\n# Optional: Set log level for the MCP server\n# LOG_LEVEL=DEBUG\n"
    },
    {
      "name": "README.md",
      "content": "# Jina AI DeepSearch MCP Server\n\nThis repository contains an MCP (Model Context Protocol) server implementation for interacting with the Jina AI DeepSearch API.\n\n## Description\n\nThe `jina_deepsearch` MCP service provides access to Jina AI's DeepSearch capabilities. DeepSearch is designed to answer complex questions by iteratively searching the web, reading relevant content, and reasoning to synthesize an accurate answer. It supports multimodal inputs (text, images, documents) and provides citations for its generated answers.\n\nThis server uses the `FastMCP` framework and interacts with the Jina AI DeepSearch API endpoint (`https://deepsearch.jina.ai/v1/chat/completions`), which is compatible with the OpenAI Chat Completions API schema.\n\n## Features\n\n*   Provides a single tool: `chat_completion`.\n*   Handles authentication using Jina AI API keys (Bearer Token).\n*   Supports streaming responses (recommended) and aggregates them into a final result.\n*   Includes Pydantic models for robust input validation and output parsing.\n*   Implements error handling for common API issues (authentication, rate limits, invalid requests, server errors).\n*   Configurable via environment variables.\n\n## Tools\n\n### 1. `chat_completion`\n\n*   **Description**: Performs a deep search and reasoning process based on a conversation history. It iteratively searches the web, reads content, and reasons to find the best answer to the user's query. Supports text, image (webp, png, jpeg as data URI), and document (txt, pdf as data URI) inputs in messages. Returns the final aggregated answer along with metadata.\n*   **Input**: `DeepSearchChatInput` model\n    *   `messages` (List[Message], **required**): Conversation history. Last message must be from 'user'.\n        *   `Message`: `{ \"role\": \"user\" | \"assistant\", \"content\": \"text or data URI\" }`\n    *   `model` (str, optional, default: `\"jina-deepsearch-v1\"`): Model ID.\n    *   `stream` (bool, optional, default: `True`): *Note: The server internally forces streaming for aggregation, but you can include this parameter.* \n    *   `reasoning_effort` (str, optional, default: `\"medium\"`): Constraint on reasoning ('low', 'medium', 'high').\n    *   `budget_tokens` (int, optional): Max tokens for the process.\n    *   `max_attempts` (int, optional): Max retries for solving.\n    *   `no_direct_answer` (bool, optional, default: `False`): Force search/thinking steps.\n    *   `max_returned_urls` (int, optional): Max URLs in the final answer.\n    *   `structured_output` (dict, optional): JSON schema for the output structure.\n    *   `good_domains` (List[str], optional): Prioritized domains.\n    *   `bad_domains` (List[str], optional): Excluded domains.\n    *   `only_domains` (List[str], optional): Exclusively included domains.\n*   **Output**: `DeepSearchChatResponse` model (dictionary representation)\n    *   `id` (str): Unique completion ID.\n    *   `object` (str): `\"chat.completion\"`.\n    *   `created` (int): Timestamp.\n    *   `model` (str): Model used.\n    *   `choices` (List[Choice]): List containing the response.\n        *   `Choice`: `{ \"index\": int, \"message\": ResponseMessage, \"finish_details\": FinishDetails }`\n        *   `ResponseMessage`: `{ \"role\": \"assistant\", \"content\": str }`\n        *   `FinishDetails`: `{ \"type\": str, \"stop\": Optional[str], \"url_citations\": Optional[List[UrlCitation]] }`\n        *   `UrlCitation`: `{ \"title\": Optional[str], \"exactQuote\": Optional[str], \"url\": str, \"dateTime\": Optional[str] }`\n    *   `usage` (Optional[Usage]): Token usage statistics.\n        *   `Usage`: `{ \"prompt_tokens\": int, \"completion_tokens\": Optional[int], \"total_tokens\": int }`\n\n## Setup\n\n1.  **Clone the repository (or save the generated files):**\n    ```bash\n    # If you have a git repo\n    # git clone <repository_url>\n    # cd <repository_directory>\n    \n    # Or just save main.py, api.py, models.py, requirements.txt, .env.example\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina AI API key:\n        ```env\n        JINA_API_KEY=your_jina_api_key_here\n        ```\n    *   You can obtain an API key from [Jina AI](https://jina.ai/).\n\n## Running the Server\n\nStart the MCP server using the `main.py` script:\n\n```bash\npython main.py\n```\n\nThe server will typically start on `http://127.0.0.1:8000` (or the default FastMCP port).\n\n## Usage Example (Conceptual MCP Client)\n\n```python\nfrom mcp import MCPClient\n\nasync def main():\n    client = MCPClient(\"http://127.0.0.1:8000\") # URL of your running MCP server\n\n    try:\n        response = await client.tools.jina_deepsearch.chat_completion(\n            messages=[\n                {\"role\": \"user\", \"content\": \"What were the key advancements in AI in 2023?\"}\n            ],\n            reasoning_effort=\"high\"\n        )\n        \n        if \"error\" in response:\n            print(f\"Error: {response['error']}\")\n        else:\n            print(\"DeepSearch Response:\")\n            print(f\"ID: {response.get('id')}\")\n            if response.get('choices'):\n                print(f\"Answer: {response['choices'][0]['message']['content']}\")\n                citations = response['choices'][0].get('finish_details', {}).get('url_citations')\n                if citations:\n                    print(\"\\nCitations:\")\n                    for cit in citations:\n                        print(f\"- [{cit.get('title', 'N/A')}]({cit['url']})\")\n            if response.get('usage'):\n                 print(f\"\\nUsage: {response['usage']}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        await client.close()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n## Error Handling\n\nThe server attempts to catch common errors from the Jina API and returns them in a structured JSON format: `{\"error\": \"Error message\"}`.\n\n*   **AuthenticationError**: Invalid or missing API key (HTTP 401).\n*   **RateLimitError**: API rate limit exceeded (HTTP 429).\n*   **InvalidRequestError**: Malformed request payload (HTTP 400).\n*   **ServerError**: Jina API server-side error (HTTP 5xx).\n*   **TimeoutError**: Request to Jina API timed out.\n*   **DeepSearchError**: Other API-specific or stream processing errors.\n"
    }
  ]
}