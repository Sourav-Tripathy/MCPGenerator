{
  "files": [
    {
      "name": "main.py",
      "content": "import logging\nimport os\nimport asyncio\nfrom typing import Dict, Any\n\nfrom dotenv import load_dotenv\nfrom mcp.server.fastmcp import FastMCP\n\nfrom api import DeepSearchAPIClient, DeepSearchAPIError\nfrom models import ChatCompletionParams, ChatCompletionResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP service for Jina AI's DeepSearch API, combining web searching, reading, and reasoning.\"\n)\n\n# Initialize API Client\ntry:\n    api_client = DeepSearchAPIClient()\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize API client: {e}\")\n    # Optionally exit or prevent server start if API key is essential\n    # exit(1)\n    api_client = None # Allow server to start but tools will fail\n\n@mcp.tool()\nasync def chat_completion(params: ChatCompletionParams) -> Dict[str, Any]:\n    \"\"\"\n    Perform a deep search and reasoning process to answer a query using Jina AI's DeepSearch API.\n\n    This tool mimics OpenAI's Chat Completion but enhances it with iterative web search,\n    reading, and reasoning capabilities.\n\n    Args:\n        params: Parameters for the chat completion request, including messages, model, and optional settings.\n\n    Returns:\n        A dictionary containing the aggregated response from the DeepSearch process,\n        including the answer, usage statistics, and potentially cited URLs within the content.\n        Returns an error dictionary if the API call fails.\n    \"\"\"\n    if not api_client:\n        logger.error(\"DeepSearch API client is not initialized. Check JINA_API_KEY.\")\n        return {\"error\": \"API client not initialized. Missing JINA_API_KEY?\"}\n\n    logger.info(f\"Received chat_completion request with model: {params.model}\")\n\n    try:\n        # Ensure streaming is enabled for aggregation, as recommended\n        # The API client handles the aggregation internally.\n        if not params.stream:\n             logger.warning(\"Streaming is disabled. This might lead to timeouts for complex queries. Enabling stream=True is recommended.\")\n             # Force stream=True internally for aggregation, or handle non-streaming if necessary\n             # For simplicity, we rely on the client handling the specified stream param.\n\n        response_data = await api_client.chat_completion(params)\n\n        # Validate and structure the response using Pydantic model\n        # The client already returns a dict, potentially matching ChatCompletionResponse\n        # We can parse it here for validation, though the client might do it too.\n        try:\n            # Assuming response_data is a dict matching ChatCompletionResponse structure\n            structured_response = ChatCompletionResponse.model_validate(response_data)\n            logger.info(f\"Successfully completed chat_completion request ID: {structured_response.id}\")\n            return structured_response.model_dump(exclude_none=True)\n        except Exception as pydantic_error:\n            logger.error(f\"Failed to parse API response: {pydantic_error}. Raw data: {response_data}\")\n            # Return raw data if parsing fails but request was successful\n            return response_data\n\n    except DeepSearchAPIError as e:\n        logger.error(f\"API Error during chat_completion: {e.status_code} - {e.message}\")\n        return {\"error\": f\"API Error: {e.status_code} - {e.message}\"}\n    except asyncio.TimeoutError:\n        logger.error(\"Request timed out during chat_completion.\")\n        return {\"error\": \"Request timed out.\"}\n    except Exception as e:\n        logger.exception(f\"Unexpected error during chat_completion: {e}\")\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\nif __name__ == \"__main__\":\n    if not api_client:\n        print(\"ERROR: DeepSearch API client failed to initialize.\")\n        print(\"Please ensure the JINA_API_KEY environment variable is set correctly in your .env file or environment.\")\n    else:\n        print(\"Starting DeepSearch MCP server...\")\n        mcp.run()\n"
    },
    {
      "name": "models.py",
      "content": "from typing import List, Optional, Dict, Any, Union\nfrom pydantic import BaseModel, Field\n\n# --- Type Definitions from Implementation Plan ---\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: str = Field(..., description=\"The role of the message author ('user' or 'assistant').\")\n    content: Union[str, List[Dict[str, Any]]] = Field(..., description=\"The content of the message. Can be simple text, or a list for complex content like images/files (e.g., [{'type': 'text', 'text': '...'}, {'type': 'image_url', 'image_url': {'url': 'data:image/jpeg;base64,...'}}]).\")\n\nclass URLCitation(BaseModel):\n    \"\"\"Details of a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited page.\")\n    exactQuote: Optional[str] = Field(None, alias=\"exactQuote\", description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"URL of the citation.\")\n    dateTime: Optional[str] = Field(None, alias=\"dateTime\", description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation within the response content, e.g., a URL citation.\"\"\"\n    type: str = Field(..., description=\"Type of annotation, e.g., 'url_citation'.\")\n    url_citation: Optional[URLCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n    # Add other potential annotation types if known\n\n# --- Input Model for chat_completion Tool ---\n\nclass ChatCompletionParams(BaseModel):\n    \"\"\"Input parameters for the chat_completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation so far. Includes user queries and assistant responses. Can include text, images (webp, png, jpeg encoded as data URI), or files (txt, pdf encoded as data URI, up to 10MB).\")\n    model: str = Field(..., description=\"ID of the model to use. e.g., 'jina-deepsearch-v1'\")\n    stream: bool = Field(True, description=\"Whether to deliver events as they occur through server-sent events. Strongly recommended to keep enabled to avoid timeouts. The MCP tool will handle stream aggregation internally and return the final result.\")\n    reasoning_effort: Optional[str] = Field(\"medium\", description=\"Constrains effort on reasoning. Supported values: 'low', 'medium', 'high'. Reducing effort can speed up responses and reduce token usage.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Larger budgets may improve quality for complex queries. Overrides reasoning_effort.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem using different approaches. Overrides reasoning_effort.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces thinking/search steps even for trivial queries.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer, sorted by relevance.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"Enables Structured Outputs, ensuring the final answer matches the supplied JSON schema.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        extra = 'ignore' # Ignore extra fields if provided\n\n# --- Output Models for chat_completion Tool (Based on OpenAI structure + DeepSearch specifics) ---\n\nclass ResponseMessage(Message):\n    \"\"\"Message object within the response, potentially including annotations.\"\"\"\n    # DeepSearch might include annotations directly or indirectly.\n    # Assuming content might contain markers or the API provides structured annotations separately.\n    # For now, keeping it similar to input Message, but content might be structured differently in reality.\n    pass\n\nclass ChatCompletionChoice(BaseModel):\n    \"\"\"A single choice generated by the model.\"\"\"\n    index: int\n    message: ResponseMessage\n    finish_reason: Optional[str] = Field(None, description=\"Reason the model stopped generating tokens.\")\n    # DeepSearch might add other fields here, like specific search metadata per choice.\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: Optional[int] = None\n    completion_tokens: Optional[int] = None\n    total_tokens: Optional[int] = None\n\nclass ChatCompletionResponse(BaseModel):\n    \"\"\"The final aggregated response from the DeepSearch process.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(..., description=\"The object type, typically 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[ChatCompletionChoice] = Field(..., description=\"A list of chat completion choices. Usually one.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics for the completion request.\")\n    # Add specific DeepSearch fields if known, e.g., visited_urls\n    # visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process.\")\n    # Annotations might be embedded within message content or provided separately.\n\n    class Config:\n        extra = 'allow' # Allow extra fields from the API response\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nimport asyncio\nfrom typing import Dict, Any, AsyncGenerator\n\nfrom models import ChatCompletionParams, ChatCompletionResponse, Message\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_TIMEOUT = 180.0  # seconds, adjust as needed for potentially long searches\nDEFAULT_API_BASE_URL = \"https://deepsearch.jina.ai/v1\"\n\nclass DeepSearchAPIError(Exception):\n    \"\"\"Custom exception for DeepSearch API errors.\"\"\"\n    def __init__(self, status_code: int, message: str):\n        self.status_code = status_code\n        self.message = message\n        super().__init__(f\"[{status_code}] {message}\")\n\nclass DeepSearchAPIClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: str = DEFAULT_API_BASE_URL, timeout: float = DEFAULT_TIMEOUT):\n        \"\"\"\n        Initializes the DeepSearchAPIClient.\n\n        Args:\n            api_key: The Jina API Key. Reads from JINA_API_KEY env var if not provided.\n            base_url: The base URL for the DeepSearch API.\n            timeout: Default request timeout in seconds.\n\n        Raises:
            ValueError: If the API key is not provided and not found in environment variables.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API Key not provided or found in JINA_API_KEY environment variable.\")\n\n        self.base_url = base_url\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Expect JSON response for non-streaming\n        }\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=timeout\n        )\n        logger.info(f\"DeepSearchAPIClient initialized for base URL: {self.base_url}\")\n\n    async def _request(self, method: str, endpoint: str, **kwargs) -> httpx.Response:\n        \"\"\"Makes an HTTP request to the API.\"\"\"\n        try:\n            response = await self.client.request(method, endpoint, **kwargs)\n            response.raise_for_status() # Raise HTTPStatusError for 4xx/5xx\n            return response\n        except httpx.HTTPStatusError as e:\n            # Attempt to parse error details from response body\n            error_message = f\"HTTP error: {e.response.status_code} {e.response.reason_phrase}\"\n            try:\n                error_details = e.response.json()\n                if isinstance(error_details, dict) and 'error' in error_details:\n                     # Standard OpenAI-like error format\n                     err_data = error_details['error']\n                     msg = err_data.get('message', 'No details provided.')\n                     typ = err_data.get('type', 'API Error')\n                     error_message = f\"{typ}: {msg}\"\n                elif isinstance(error_details, dict) and 'detail' in error_details:\n                     # FastAPI-like error format\n                     error_message = f\"Detail: {error_details['detail']}\"\n                else:\n                     error_message += f\" | Response: {e.response.text[:500]}\" # Limit response size\n            except Exception:\n                 error_message += f\" | Response: {e.response.text[:500]}\"\n\n            logger.error(f\"API request failed: {error_message} for {method} {endpoint}\")\n            raise DeepSearchAPIError(status_code=e.response.status_code, message=error_message) from e\n        except httpx.TimeoutException as e:\n            logger.error(f\"API request timed out: {e} for {method} {endpoint}\")\n            raise asyncio.TimeoutError(f\"Request timed out after {self.client.timeout.read} seconds\") from e\n        except httpx.RequestError as e:\n            logger.error(f\"API request error: {e} for {method} {endpoint}\")\n            raise DeepSearchAPIError(status_code=500, message=f\"Request failed: {str(e)}\") from e\n\n    async def _aggregate_stream(self, response: httpx.Response) -> Dict[str, Any]:\n        \"\"\"Aggregates chunks from a streaming Server-Sent Events (SSE) response.\"\"\"\n        aggregated_content = \"\"\n        final_response_chunk = None\n        choices_data = {} # Store content deltas per choice index\n\n        try:\n            async for line in response.aiter_lines():\n                if line.startswith(\"data:\"):\n                    data_str = line[len(\"data:\"):].strip()\n                    if data_str == \"[DONE]\":\n                        logger.info(\"Stream finished with [DONE] marker.\")\n                        break\n                    try:\n                        chunk = json.loads(data_str)\n                        final_response_chunk = chunk # Keep track of the last chunk for metadata\n\n                        if chunk.get('choices'):\n                            delta = chunk['choices'][0].get('delta', {})\n                            content_part = delta.get('content')\n                            if content_part:\n                                choice_index = chunk['choices'][0].get('index', 0)\n                                if choice_index not in choices_data:\n                                    choices_data[choice_index] = \"\"\n                                choices_data[choice_index] += content_part\n                                # logger.debug(f\"Received content chunk: {content_part}\")\n\n                    except json.JSONDecodeError:\n                        logger.warning(f\"Failed to decode JSON from SSE data: {data_str}\")\n                    except Exception as e:\n                        logger.error(f\"Error processing SSE chunk: {e} | Data: {data_str}\")\n                        # Decide whether to continue or raise\n                        # raise # Re-raise might be too strict for minor chunk errors\n\n        except httpx.RemoteProtocolError as e:\n             logger.error(f\"Remote protocol error during streaming: {e}\")\n             raise DeepSearchAPIError(status_code=502, message=f\"Streaming connection error: {e}\")\n        except Exception as e:\n            logger.exception(f\"Unexpected error during stream aggregation: {e}\")\n            raise DeepSearchAPIError(status_code=500, message=f\"Failed during stream processing: {str(e)}\")\n        finally:\n            await response.aclose()\n\n        if not final_response_chunk:\n            logger.error(\"Stream ended unexpectedly or no valid data received.\")\n            raise DeepSearchAPIError(status_code=500, message=\"Stream ended without valid data or [DONE] marker.\")\n\n        # Construct the final response structure based on aggregated data and the last chunk\n        final_response = final_response_chunk.copy()\n        if 'choices' in final_response and final_response['choices']:\n            # Update choices with aggregated content\n            for i, choice in enumerate(final_response['choices']):\n                if i in choices_data:\n                    # Assuming the structure has message.content\n                    if 'message' not in choice:\n                        choice['message'] = {'role': 'assistant', 'content': choices_data[i]}\n                    else:\n                        choice['message']['content'] = choices_data[i]\n                    # Clear delta if it exists in the final structure\n                    choice.pop('delta', None)\n        else:\n             # Handle cases where no choices were received or structure is different\n             logger.warning(\"No choices found in the final stream chunk. Constructing basic response.\")\n             # Create a minimal choice structure if possible\n             if choices_data:\n                 final_response['choices'] = [\n                     {\n                         'index': idx,\n                         'message': {'role': 'assistant', 'content': content},\n                         'finish_reason': final_response.get('choices', [{}])[0].get('finish_reason', 'unknown') # Best guess\n                     }\n                     for idx, content in choices_data.items()\n                 ]\n             else:\n                  # If no content was aggregated at all\n                  final_response['choices'] = [\n                     {\n                         'index': 0,\n                         'message': {'role': 'assistant', 'content': ''},\n                         'finish_reason': 'error'\n                     }\n                 ]\n\n        # Ensure essential fields are present from the last chunk\n        final_response.setdefault('id', 'streamed-' + final_response.get('id', 'unknown'))\n        final_response.setdefault('object', 'chat.completion')\n        final_response.setdefault('created', final_response.get('created', 0))\n        final_response.setdefault('model', final_response.get('model', 'unknown'))\n\n        logger.info(f\"Stream aggregated successfully for ID: {final_response.get('id')}\")\n        return final_response\n\n    async def chat_completion(self, params: ChatCompletionParams) -> Dict[str, Any]:\n        \"\"\"\n        Performs a chat completion request, handling streaming and aggregation.\n\n        Args:\n            params: The parameters for the chat completion.\n\n        Returns:\n            A dictionary representing the aggregated ChatCompletionResponse.\n\n        Raises:\n            DeepSearchAPIError: If the API returns an error.\n            asyncio.TimeoutError: If the request times out.\n        \"\"\"\n        endpoint = \"/chat/completions\"\n        # Use model_dump to serialize Pydantic model, excluding None values\n        payload = params.model_dump(exclude_none=True, by_alias=True)\n\n        logger.debug(f\"Sending chat completion request to {endpoint} with payload: {payload}\")\n\n        if params.stream:\n            # Make a streaming request\n            headers = self.headers.copy()\n            headers['Accept'] = 'text/event-stream'\n            try:\n                response = await self.client.stream(\n                    \"POST\",\n                    endpoint,\n                    json=payload,\n                    headers=headers\n                )\n                # Check status *before* starting iteration for immediate errors\n                if response.status_code >= 400:\n                     # Try to read body for error details, then raise\n                     error_body = await response.aread()\n                     response.raise_for_status() # This will likely raise based on status\n\n                # Aggregate the stream\n                aggregated_data = await self._aggregate_stream(response)\n                return aggregated_data\n            except httpx.HTTPStatusError as e:\n                # Handle errors that occur before or during stream setup\n                error_message = f\"HTTP error during stream setup: {e.response.status_code} {e.response.reason_phrase}\"\n                try:\n                    error_details = json.loads(e.response.text)\n                    if isinstance(error_details, dict) and 'error' in error_details:\n                        err_data = error_details['error']\n                        msg = err_data.get('message', 'No details provided.')\n                        typ = err_data.get('type', 'API Error')\n                        error_message = f\"{typ}: {msg}\"\n                    elif isinstance(error_details, dict) and 'detail' in error_details:\n                        error_message = f\"Detail: {error_details['detail']}\"\n                    else:\n                        error_message += f\" | Response: {e.response.text[:500]}\"\n                except Exception:\n                    error_message += f\" | Response: {e.response.text[:500]}\"\n                logger.error(f\"API stream request failed: {error_message}\")\n                raise DeepSearchAPIError(status_code=e.response.status_code, message=error_message) from e\n            except (httpx.TimeoutException, httpx.RequestError, DeepSearchAPIError, asyncio.TimeoutError) as e:\n                # Re-raise known errors from lower levels or stream aggregation\n                raise e\n            except Exception as e:\n                logger.exception(f\"Unexpected error during streaming request: {e}\")\n                raise DeepSearchAPIError(status_code=500, message=f\"Unexpected streaming error: {str(e)}\") from e\n        else:\n            # Make a regular, non-streaming request\n            response = await self._request(\"POST\", endpoint, json=payload)\n            logger.info(f\"Received non-streaming response for chat completion.\")\n            return response.json()\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n        logger.info(\"DeepSearchAPIClient closed.\")\n"
    },
    {
      "name": "requirements.txt",
      "content": "mcp>=0.1.0\nfastapi>=0.100.0 # MCP dependency\nuvicorn>=0.20.0 # MCP dependency\nhttpx>=0.25.0\npydantic>=2.0\npython-dotenv>=1.0.0\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain your key from https://cloud.jina.ai/\nJINA_API_KEY=your_jina_api_key_here\n\n# Optional: Override the default DeepSearch API base URL\n# DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai/v1\n"
    },
    {
      "name": "README.md",
      "content": "# Jina AI DeepSearch MCP Server\n\nThis repository provides a Model Context Protocol (MCP) server for interacting with the [Jina AI DeepSearch API](https://jina.ai/deepsearch/).\n\nDeepSearch combines web searching, reading, and reasoning to provide comprehensive answers to complex questions. It functions as an autonomous agent that iteratively searches, reads, and reasons, dynamically deciding the next steps based on its findings.\n\nThis MCP server exposes the core functionality of DeepSearch through a standardized tool interface.\n\n## Features\n\n*   **`chat_completion` Tool:** Performs a deep search and reasoning process based on a conversation history. It mimics the interface of OpenAI's Chat Completion API but leverages DeepSearch's advanced web search and analysis capabilities.\n    *   Supports text, image, and file inputs within messages.\n    *   Handles streaming responses (recommended) and aggregates them into a final result.\n    *   Allows configuration of reasoning effort, token budget, domain preferences, and more.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n## Configuration\n\n1.  **Create a `.env` file:**\n    Copy the example file:\n    ```bash\n    cp .env.example .env\n    ```\n\n2.  **Edit `.env`:**\n    Replace `your_jina_api_key_here` with your actual Jina AI API key. You can obtain a key from the [Jina AI Cloud Platform](https://cloud.jina.ai/).\n    ```dotenv\n    JINA_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx\n    ```\n    *   You can optionally override the `DEEPSEARCH_BASE_URL` if needed, but the default (`https://deepsearch.jina.ai/v1`) is usually correct.\n\n## Running the Server\n\nStart the MCP server using:\n\n```bash\npython main.py\n```\n\nThe server will start, typically on `http://127.0.0.1:8000` (or the configured MCP host/port).\n\n## Usage\n\nYou can interact with the server using an MCP client, such as the `mcp` CLI.\n\n**Example using `mcp` CLI:**\n\n```bash\n# List available tools\n# mcp list --url http://127.0.0.1:8000\n\n# Call the chat_completion tool\nmcp call --url http://127.0.0.1:8000 deepsearch.chat_completion \\\n    --param 'params={ \"model\": \"jina-deepsearch-v1\", \"messages\": [{\"role\": \"user\", \"content\": \"What were the key advancements in AI in 2023?\"}] }'\n\n# Example with more options (ensure proper JSON escaping in your shell)\nmcp call --url http://127.0.0.1:8000 deepsearch.chat_completion \\\n    --param 'params={ \"model\": \"jina-deepsearch-v1\", \"messages\": [{\"role\": \"user\", \"content\": \"Compare the performance of Llama 2 and GPT-4 on coding tasks, citing sources.\"}], \"reasoning_effort\": \"high\", \"max_returned_urls\": 5 }'\n```\n\n**Note:** When using complex JSON structures like `messages` or `structured_output` via the CLI, ensure correct JSON formatting and shell escaping.\n\n## Error Handling\n\nThe server includes error handling for:\n\n*   API authentication issues (invalid/missing API key).\n*   API rate limits.\n*   Invalid input parameters.\n*   Timeouts (especially if streaming is disabled).\n*   Server-side errors from the DeepSearch API.\n*   Network issues.\n\nErrors from the API or the MCP tool itself will be returned in a JSON response containing an `\"error\"` key.\n"
    }
  ]
}