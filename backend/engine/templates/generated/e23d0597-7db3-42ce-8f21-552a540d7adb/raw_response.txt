{
  "files": [
    {
      "name": "main.py",
      "content": "import os\nimport logging\nimport json\nfrom typing import AsyncGenerator, Union\n\nfrom dotenv import load_dotenv\nfrom mcp.server.fastmcp import FastMCP\nimport httpx\n\nfrom models import DeepSearchChatParams, DeepSearchChatResponse, DeepSearchChatChunk, MCPErrorResponse\nfrom client import DeepSearchClient, DeepSearchApiError\n\n# --- Configuration ---\nload_dotenv()  # Load environment variables from .env file\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- Initialization ---\nSERVICE_NAME = \"DeepSearch\"\nmcp = FastMCP(SERVICE_NAME)\n\n# Initialize the DeepSearch API client\napi_key = os.getenv(\"JINA_API_KEY\")\ndeepsearch_client = DeepSearchClient(api_key=api_key)\n\n# --- MCP Tool Definition ---\n@mcp.tool()\nasync def chat_completion(params: DeepSearchChatParams) -> Union[DeepSearchChatResponse, AsyncGenerator[DeepSearchChatChunk, None], MCPErrorResponse]:\n    \"\"\"Performs a deep search and reasoning process based on a conversation history.\n\n    This tool sends messages to the DeepSearch API, which then iteratively searches\n    the web, reads content, and reasons to find the best possible answer within its\n    operational constraints (like token budget or reasoning effort).\n\n    Args:\n        params: Parameters for the DeepSearch chat completion request, including messages,\n                model, streaming preference, and other options.\n\n    Returns:\n        If stream=False, returns a DeepSearchChatResponse object with the complete answer.\n        If stream=True, returns an AsyncGenerator yielding DeepSearchChatChunk objects.\n        Returns an MCPErrorResponse if an API error occurs.\n    \"\"\"\n    logger.info(f\"Received chat_completion request with stream={params.stream}\")\n    try:\n        if params.stream:\n            logger.info(\"Initiating streaming chat completion.\")\n            # The client returns an async generator, which FastMCP handles correctly\n            return deepsearch_client.chat_completion_stream(params)\n        else:\n            logger.info(\"Initiating non-streaming chat completion.\")\n            response = await deepsearch_client.chat_completion_no_stream(params)\n            logger.info(\"Non-streaming chat completion successful.\")\n            return response\n\n    except DeepSearchApiError as e:\n        logger.error(f\"DeepSearch API Error: {e.status_code} - {e.detail}\", exc_info=True)\n        return MCPErrorResponse(error=f\"DeepSearch API Error: {e.status_code} - {e.detail}\")\n    except httpx.TimeoutException as e:\n        logger.error(f\"Request timed out: {e}\", exc_info=True)\n        # This is more likely if stream=False for long requests\n        error_msg = \"Request to DeepSearch API timed out.\"\n        if not params.stream:\n            error_msg += \" Consider setting stream=True for long-running requests.\"\n        return MCPErrorResponse(error=error_msg)\n    except httpx.RequestError as e:\n        logger.error(f\"HTTP Request Error: {e}\", exc_info=True)\n        return MCPErrorResponse(error=f\"HTTP Request Error: {e}\")\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n        return MCPErrorResponse(error=f\"An unexpected error occurred: {str(e)}\")\n\n# --- Server Execution ---\nif __name__ == \"__main__\":\n    # Recommended: Run with Uvicorn for production\n    # uvicorn main:mcp --host 0.0.0.0 --port 8000\n    logger.info(f\"Starting {SERVICE_NAME} MCP Server\")\n    mcp.run() # Uses default Uvicorn settings for development\n"
    },
    {
      "name": "models.py",
      "content": "from typing import List, Optional, Dict, Any, Literal\nfrom pydantic import BaseModel, Field\n\n# --- Basic Types defined in Implementation Plan ---\n\nclass ChatMessage(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant', 'system'] = Field(..., description=\"The role of the message author ('user', 'assistant', or 'system').\")\n    content: Any = Field(..., description=\"The content of the message. Typically string, but can be complex for multimodal inputs (e.g., data URI for images/files).\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details about a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"The URL of the citation.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation within the response content, like a URL citation.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass Delta(BaseModel):\n    \"\"\"The content delta in a streaming chunk.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None, description=\"The role of the author of this message.\")\n    content: Optional[str] = Field(None, description=\"The contents of the chunk message.\")\n    type: Optional[str] = Field(None, description=\"Type of content (e.g., 'text').\") # Note: Added based on typical OpenAI schema\n\n# --- Input Model for the Tool ---\n\nclass DeepSearchChatParams(BaseModel):\n    \"\"\"Input parameters for the DeepSearch chat_completion tool.\"\"\"\n    messages: List[ChatMessage] = Field(..., description=\"A list of messages comprising the conversation history.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(True, description=\"Whether to stream the response using Server-Sent Events. Strongly recommended.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains effort on reasoning.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces further thinking/search steps.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of relevant URLs to include.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"A JSON schema to ensure the final answer matches the specified structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include.\")\n\n    class Config:\n        # Ensure that default values are used even if None is passed explicitly for optional fields\n        # Pydantic v2 behavior might handle this better, but being explicit can help\n        validate_assignment = True\n\n# --- Output Models (Mirroring OpenAI Schema + DeepSearch specifics) ---\n\nclass DeepSearchUsage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: int = Field(..., description=\"Number of tokens in the prompt.\")\n    completion_tokens: int = Field(..., description=\"Number of tokens in the generated completion.\")\n    total_tokens: int = Field(..., description=\"Total number of tokens used in the request.\")\n\nclass DeepSearchChoice(BaseModel):\n    \"\"\"A single choice generated by the model (non-streaming).\"\"\"\n    index: int = Field(..., description=\"The index of the choice in the list of choices.\")\n    message: ChatMessage = Field(..., description=\"A chat completion message generated by the model.\")\n    finish_reason: Optional[str] = Field(None, description=\"The reason the model stopped generating tokens.\")\n    # DeepSearch specific additions might appear here, potentially in message.content or metadata\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations related to the message content, like citations.\")\n\nclass DeepSearchChatResponse(BaseModel):\n    \"\"\"The full response object for a non-streaming chat completion request.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(..., description=\"The object type, typically 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[DeepSearchChoice] = Field(..., description=\"A list of chat completion choices.\")\n    usage: Optional[DeepSearchUsage] = Field(None, description=\"Usage statistics for the completion request.\")\n    # DeepSearch specific additions\n    visited_urls: Optional[List[UrlCitation]] = Field(None, description=\"List of URLs visited during the search process.\")\n\nclass DeepSearchStreamChoice(BaseModel):\n    \"\"\"A single choice delta in a streaming response.\"\"\"\n    index: int = Field(..., description=\"The index of the choice in the list of choices.\")\n    delta: Delta = Field(..., description=\"A chat completion delta generated by the model.\")\n    finish_reason: Optional[str] = Field(None, description=\"The reason the model stopped generating tokens. Sent in the final chunk.\")\n    # DeepSearch specific additions might appear here\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations related to the message content, like citations. May appear in chunks.\")\n\nclass DeepSearchChatChunk(BaseModel):\n    \"\"\"A chunk of data received during a streaming chat completion request.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion. Each chunk has the same ID.\")\n    object: str = Field(..., description=\"The object type, typically 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chunk was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[DeepSearchStreamChoice] = Field(..., description=\"A list of chat completion choices. Can contain more than one choice if n > 1.\")\n    usage: Optional[DeepSearchUsage] = Field(None, description=\"Usage statistics. Often sent in the final chunk.\")\n    # DeepSearch specific additions\n    visited_urls: Optional[List[UrlCitation]] = Field(None, description=\"List of URLs visited. May appear in the final chunk.\")\n\n# --- Error Model ---\n\nclass MCPErrorResponse(BaseModel):\n    \"\"\"Standard error response format for MCP tools.\"\"\"\n    error: str = Field(..., description=\"Description of the error that occurred.\")\n"
    },
    {
      "name": "client.py",
      "content": "import os\nimport json\nimport logging\nfrom typing import Optional, AsyncGenerator, Dict, Any\n\nimport httpx\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatParams, DeepSearchChatResponse, DeepSearchChatChunk\n\nlogger = logging.getLogger(__name__)\n\n# Define a custom exception for API errors\nclass DeepSearchApiError(Exception):\n    def __init__(self, status_code: int, detail: str):\n        self.status_code = status_code\n        self.detail = detail\n        super().__init__(f\"DeepSearch API Error {status_code}: {detail}\")\n\nclass DeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://deepsearch.jina.ai/v1\"\n    DEFAULT_TIMEOUT = 120.0 # Increased timeout, especially for non-streaming\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = DEFAULT_TIMEOUT):\n        \"\"\"Initializes the DeepSearchClient.\n\n        Args:\n            api_key: Your Jina AI API key (optional, increases rate limits).\n            base_url: The base URL for the DeepSearch API.\n            timeout: Default request timeout in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        self.base_url = base_url or self.DEFAULT_BASE_URL\n        self.timeout = timeout\n\n        headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n            logger.info(\"Jina API Key found, using Authorization header.\")\n        else:\n            logger.warning(\"Jina API Key not provided. Using default rate limits.\")\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=headers,\n            timeout=self.timeout,\n            follow_redirects=True,\n        )\n\n    async def _request(\n        self,\n        method: str,\n        endpoint: str,\n        params: Optional[Dict[str, Any]] = None,\n        json_data: Optional[Dict[str, Any]] = None,\n        stream: bool = False\n    ) -> Union[httpx.Response, AsyncGenerator[str, None]]:\n        \"\"\"Makes an HTTP request to the DeepSearch API.\"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        logger.debug(f\"Making {method} request to {url}\")\n        try:\n            if stream:\n                # For streaming, we return the response object to be iterated over\n                req = self.client.build_request(method, endpoint, params=params, json=json_data)\n                return await self.client.send(req, stream=True)\n            else:\n                response = await self.client.request(method, endpoint, params=params, json=json_data)\n                response.raise_for_status() # Raise HTTPStatusError for 4xx/5xx\n                return response\n        except httpx.HTTPStatusError as e:\n            # Attempt to parse error details from response body\n            error_detail = f\"HTTP Error: {e.response.status_code}\"\n            try:\n                error_data = e.response.json()\n                if isinstance(error_data, dict) and 'detail' in error_data:\n                    error_detail = error_data['detail']\n                    if isinstance(error_detail, list) and error_detail:\n                         error_detail = error_detail[0].get('msg', str(error_detail[0]))\n                    elif isinstance(error_detail, dict):\n                         error_detail = error_detail.get('message', str(error_detail))\n                else:\n                    error_detail = e.response.text or error_detail\n            except (json.JSONDecodeError, ValueError):\n                error_detail = e.response.text or error_detail\n\n            logger.error(f\"HTTP Error contacting DeepSearch API: {e.response.status_code} - {error_detail}\")\n            raise DeepSearchApiError(status_code=e.response.status_code, detail=error_detail) from e\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request to DeepSearch API timed out: {e}\")\n            raise # Re-raise TimeoutException to be handled by the caller\n        except httpx.RequestError as e:\n            logger.error(f\"Network error contacting DeepSearch API: {e}\")\n            raise # Re-raise RequestError\n        except Exception as e:\n            logger.error(f\"Unexpected error during API request: {e}\", exc_info=True)\n            raise DeepSearchApiError(status_code=500, detail=f\"Unexpected client error: {str(e)}\") from e\n\n    async def chat_completion_no_stream(self, params: DeepSearchChatParams) -> DeepSearchChatResponse:\n        \"\"\"Performs a non-streaming chat completion request.\"\"\"\n        if params.stream:\n            logger.warning(\"stream=True passed to non-streaming method, overriding to False.\")\n            params.stream = False\n\n        payload = params.model_dump(exclude_none=True)\n        logger.info(f\"Sending non-streaming request to /chat/completions with model {params.model}\")\n        logger.debug(f\"Payload: {payload}\")\n\n        response = await self._request(\"POST\", \"/chat/completions\", json_data=payload, stream=False)\n\n        try:\n            response_data = response.json()\n            logger.debug(f\"Received non-streaming response: {response_data}\")\n            return DeepSearchChatResponse(**response_data)\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON response: {response.text}\", exc_info=True)\n            raise DeepSearchApiError(status_code=500, detail=f\"Invalid JSON response from server: {e}\") from e\n        except ValidationError as e:\n            logger.error(f\"Failed to validate response data: {response_data}\", exc_info=True)\n            raise DeepSearchApiError(status_code=500, detail=f\"Invalid response structure from server: {e}\") from e\n\n    async def chat_completion_stream(self, params: DeepSearchChatParams) -> AsyncGenerator[DeepSearchChatChunk, None]:\n        \"\"\"Performs a streaming chat completion request.\n\n        Yields:\n            DeepSearchChatChunk: Chunks of the response as they arrive.\n        \"\"\"\n        if not params.stream:\n            logger.warning(\"stream=False passed to streaming method, overriding to True.\")\n            params.stream = True\n\n        payload = params.model_dump(exclude_none=True)\n        logger.info(f\"Sending streaming request to /chat/completions with model {params.model}\")\n        logger.debug(f\"Payload: {payload}\")\n\n        response_stream = await self._request(\"POST\", \"/chat/completions\", json_data=payload, stream=True)\n\n        try:\n            async for line in response_stream.aiter_lines():\n                if line.startswith(\"data:\"):\n                    data_str = line[len(\"data:\"):].strip()\n                    if data_str == \"[DONE]\":\n                        logger.info(\"Stream finished with [DONE] message.\")\n                        break\n                    if not data_str:\n                        logger.debug(\"Received empty data line, skipping.\")\n                        continue\n\n                    try:\n                        chunk_data = json.loads(data_str)\n                        logger.debug(f\"Received stream chunk: {chunk_data}\")\n                        yield DeepSearchChatChunk(**chunk_data)\n                    except json.JSONDecodeError as e:\n                        logger.warning(f\"Failed to decode stream chunk JSON: '{data_str}'. Error: {e}\")\n                        continue # Skip malformed chunk\n                    except ValidationError as e:\n                        logger.warning(f\"Failed to validate stream chunk: {chunk_data}. Error: {e}\")\n                        continue # Skip invalid chunk structure\n                elif line:\n                    logger.warning(f\"Received unexpected line in stream: '{line}'\")\n\n        except DeepSearchApiError: # Re-raise API errors caught during initial request\n             raise\n        except httpx.HTTPError as e:\n            # Errors during streaming might manifest here\n            logger.error(f\"HTTP error during streaming: {e}\", exc_info=True)\n            raise DeepSearchApiError(status_code=getattr(e, 'response', None) and e.response.status_code or 500, detail=f\"HTTP error during stream: {str(e)}\") from e\n        except Exception as e:\n            logger.error(f\"Unexpected error during streaming: {e}\", exc_info=True)\n            raise DeepSearchApiError(status_code=500, detail=f\"Unexpected client error during stream: {str(e)}\") from e\n        finally:\n            if 'response_stream' in locals() and response_stream is not None:\n                await response_stream.aclose()\n            logger.debug(\"Stream closed.\")\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n        logger.info(\"DeepSearchClient closed.\")\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.2.0\nhttpx[http2]>=0.25.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.20.0\nasync-timeout>=4.0.0\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key (Optional, but recommended for higher rate limits)\n# Get your key from https://cloud.jina.ai/\nJINA_API_KEY=\n\n# Optional: Override the default DeepSearch API base URL\n# DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai/v1\n"
    },
    {
      "name": "README.md",
      "content": "# DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for interacting with the Jina AI DeepSearch API using the FastMCP framework.\n\n## Description\n\nJina AI's DeepSearch combines web searching, reading, and reasoning through iterative steps to provide comprehensive answers to complex questions, especially those requiring up-to-date information or multi-hop reasoning. It is fully compatible with the OpenAI Chat API schema.\n\nThis MCP server exposes the core functionality of DeepSearch through a single tool, allowing agents or applications to leverage its capabilities easily.\n\n## Features\n\n*   **`chat_completion` Tool:** Performs a deep search and reasoning process based on a conversation history.\n    *   Supports standard chat messages (user, assistant, system).\n    *   Handles optional parameters like `model`, `reasoning_effort`, `budget_tokens`, domain filtering, etc.\n    *   Supports **streaming responses** (recommended for potentially long-running searches) via Server-Sent Events (SSE).\n    *   Returns results compatible with the OpenAI Chat Completions API schema, including usage statistics and visited URLs.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository_url>\n    cd <repository_directory>\n    ```\n\n2.  **Create and activate a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file:\n        *   `JINA_API_KEY`: Add your Jina AI API key here. While optional (DeepSearch has a free tier with lower rate limits), it's highly recommended for production use or higher request volumes. You can obtain a key from [Jina AI Cloud](https://cloud.jina.ai/).\n\n## Running the Server\n\nYou can run the MCP server using Uvicorn:\n\n```bash\nuvicorn main:mcp --host 0.0.0.0 --port 8000 --reload\n```\n\n*   `--host 0.0.0.0`: Makes the server accessible on your network.\n*   `--port 8000`: Specifies the port to run on.\n*   `--reload`: Automatically restarts the server when code changes (useful for development).\n\nThe server will be available at `http://localhost:8000` (or the specified host/port).\n\n## Usage\n\nYou can interact with this MCP server using any MCP-compatible client, such as `mcp_client`.\n\n**Example using `mcp_client` (Python):**\n\n```python\nfrom mcp_client import Client\nimport asyncio\n\nasync def main():\n    client = Client(url=\"http://localhost:8000\") # Adjust URL if needed\n\n    # --- Non-Streaming Example ---\n    print(\"--- Non-Streaming Request ---\")\n    params_no_stream = {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What were the key announcements at the latest Apple event?\"}\n        ],\n        \"stream\": False,\n        \"reasoning_effort\": \"low\" # Faster response for simple queries\n    }\n    try:\n        response = await client.arun_tool(\"chat_completion\", params=params_no_stream)\n        print(\"Response:\", response)\n        if isinstance(response, dict) and 'choices' in response:\n            print(\"Answer:\", response['choices'][0]['message']['content'])\n            print(\"Visited URLs:\", response.get('visited_urls'))\n            print(\"Usage:\", response.get('usage'))\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    print(\"\\n---\")\n\n    # --- Streaming Example ---\n    print(\"--- Streaming Request ---\")\n    params_stream = {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Compare the pros and cons of Next.js vs Remix for building web applications in 2024.\"}\n        ],\n        \"stream\": True\n    }\n    try:\n        full_response_content = \"\"\n        async for chunk in client.astream_tool(\"chat_completion\", params=params_stream):\n            print(\"Chunk:\", chunk)\n            if isinstance(chunk, dict) and 'choices' in chunk and chunk['choices']:\n                delta = chunk['choices'][0].get('delta', {})\n                content_piece = delta.get('content')\n                if content_piece:\n                    print(content_piece, end=\"\", flush=True)\n                    full_response_content += content_piece\n        print(\"\\n--- End of Stream ---\")\n        # Note: Usage and visited URLs often come in the *last* chunk in streaming\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n## Authentication\n\nThe server uses Bearer Token authentication if a `JINA_API_KEY` is provided in the environment variables. This key is passed in the `Authorization` header to the DeepSearch API.\n\n## Error Handling\n\nThe server attempts to catch common errors:\n*   **API Errors:** Errors returned by the DeepSearch API (e.g., 400 Bad Request, 401 Unauthorized, 429 Rate Limit Exceeded, 5xx Server Errors) are caught and returned as an MCP error response.\n*   **Timeout Errors:** Requests taking too long (especially non-streaming ones) might time out.\n*   **Network Errors:** Issues connecting to the DeepSearch API.\n*   **Validation Errors:** Problems with the structure of the request or response.\n\nError responses are returned in the standard MCP format: `{\"error\": \"Description of the error.\"}`.\n"
    }
  ]
}