{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\n\n# --- Type Definitions from Implementation Plan ---\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: str = Field(..., description=\"The role of the message author ('user' or 'assistant').\")\n    content: str = Field(..., description=\"The content of the message. Can be plain text or a data URI for images/documents.\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation within the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"The URL of the citation.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation associated with the response content.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass Delta(BaseModel):\n    \"\"\"Represents the change in content for a streamed chunk (or the full content in the final chunk/non-streamed response).\"\"\"\n    role: Optional[str] = Field(None, description=\"The role of the message author, typically 'assistant'. Appears in the first chunk.\")\n    content: Optional[str] = Field(None, description=\"The text content chunk.\")\n    type: Optional[str] = Field(None, description=\"Type of content (e.g., 'text').\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"List of annotations related to the content.\")\n\nclass Choice(BaseModel):\n    \"\"\"Represents a single response choice.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    delta: Optional[Delta] = Field(None, description=\"The message content and annotations (used in streaming).\")\n    message: Optional[Message] = Field(None, description=\"The full message (used in non-streaming responses, similar structure to Delta but within 'message').\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probabilities (currently null in example).\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the generation finished (e.g., 'stop').\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Tokens used by the prompt.\") # Optional because it might not be in every chunk\n    completion_tokens: Optional[int] = Field(None, description=\"Tokens generated for the completion.\") # Optional\n    total_tokens: Optional[int] = Field(None, description=\"Total tokens used in the entire process.\") # Optional\n\nclass DeepSearchResponse(BaseModel):\n    \"\"\"The overall structure of the response from the DeepSearch API (can represent a chunk or a full response).\"\"\"\n    id: str = Field(..., description=\"Unique identifier for the response/chunk.\")\n    object: str = Field(..., description=\"Type of object (e.g., 'chat.completion' or 'chat.completion.chunk').\")\n    created: int = Field(..., description=\"Timestamp of creation (Unix epoch).\")\n    model: str = Field(..., description=\"Model used for the response.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint.\")\n    choices: List[Choice] = Field(..., description=\"List of response choices (usually one).\")\n    usage: Optional[Usage] = Field(None, description=\"Token usage information (present in the final chunk/response).\")\n    visitedURLs: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process (present in final chunk/response).\", alias=\"visitedURLs\") # Alias for camelCase\n    readURLs: Optional[List[str]] = Field(None, description=\"List of URLs read during the search process (present in final chunk/response).\", alias=\"readURLs\") # Alias for camelCase\n    numURLs: Optional[int] = Field(None, description=\"Total number of unique URLs encountered (present in final chunk/response).\", alias=\"numURLs\") # Alias for camelCase\n\n    class Config:\n        allow_population_by_field_name = True # Allow using visitedURLs etc directly\n\n# --- Input Model Definition ---\n\nclass DeepSearchChatParams(BaseModel):\n    \"\"\"Input parameters for the Jina DeepSearch chat completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history. The last message should be the user's query. Supports text, image (webp, png, jpeg), and document (txt, pdf) content encoded as data URIs (up to 10MB).\")\n    model: str = Field(default=\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(default=True, description=\"Whether to stream back partial progress. If disabled, the request might time out for long-running queries. Strongly recommended to keep enabled (default). The MCP tool will handle aggregation if streaming is used.\")\n    reasoning_effort: Optional[str] = Field(None, description=\"Constrains effort on reasoning. Supported values: 'low', 'medium', 'high'. Default: 'medium'.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides 'reasoning_effort'.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Overrides 'reasoning_effort'.\")\n    no_direct_answer: Optional[bool] = Field(None, description=\"Forces the model to take further thinking/search steps even for trivial queries. Default: false.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"A JSON schema object to ensure the final answer matches the supplied schema.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        extra = 'ignore' # Ignore any extra fields sent in the request\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import AsyncGenerator, Dict, Any\nfrom models import DeepSearchChatParams, DeepSearchResponse\n\nlogger = logging.getLogger(__name__)\n\nclass JinaDeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://deepsearch.jina.ai\"\n    API_ENDPOINT = \"/v1/chat/completions\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = 180.0):\n        \"\"\"\n        Initializes the Jina DeepSearch API client.\n\n        Args:\n            api_key: The Jina API key. Reads from JINA_API_KEY env var if not provided.\n            base_url: The base URL for the Jina DeepSearch API. Reads from JINA_DEEPSEARCH_BASE_URL env var or uses default if not provided.\n            timeout: Default timeout for API requests in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        self.base_url = base_url or os.getenv(\"JINA_DEEPSEARCH_BASE_URL\") or self.DEFAULT_BASE_URL\n\n        if not self.api_key:\n            logger.warning(\"JINA_API_KEY not found. API calls may be rate-limited (2 RPM).\")\n            self.headers = {\"Content-Type\": \"application/json\"}\n        else:\n            self.headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {self.api_key}\"\n            }\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=timeout\n        )\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n\n    async def chat_completion(self, params: DeepSearchChatParams) -> AsyncGenerator[DeepSearchResponse, None] | DeepSearchResponse:\n        \"\"\"\n        Performs a chat completion request to the Jina DeepSearch API.\n\n        Handles both streaming and non-streaming responses.\n\n        Args:\n            params: The parameters for the chat completion request.\n\n        Returns:\n            If stream=True, an async generator yielding DeepSearchResponse chunks.\n            If stream=False, a single DeepSearchResponse object.\n\n        Raises:\n            httpx.HTTPStatusError: For API errors (4xx, 5xx).\n            httpx.RequestError: For network-related errors.\n            Exception: For unexpected errors during processing.\n        \"\"\"\n        # Exclude None values from the payload, Pydantic v2 handles alias generation\n        payload = params.model_dump(exclude_none=True, by_alias=False)\n        # Ensure stream is explicitly in payload if True, as it's often expected by APIs\n        payload['stream'] = params.stream\n\n        request_url = self.API_ENDPOINT\n        logger.info(f\"Sending request to {self.base_url}{request_url} with stream={params.stream}\")\n        # logger.debug(f\"Payload: {payload}\") # Be cautious logging payload with potentially sensitive data\n\n        try:\n            if params.stream:\n                return self._stream_request(request_url, payload)\n            else:\n                response = await self.client.post(request_url, json=payload)\n                response.raise_for_status() # Raise HTTPStatusError for 4xx/5xx\n                response_data = response.json()\n                # logger.debug(f\"Received non-streamed response: {response_data}\")\n                return DeepSearchResponse.model_validate(response_data)\n\n        except httpx.HTTPStatusError as e:\n            # Log specific API errors\n            error_details = \"No details available\"\n            try:\n                error_details = e.response.json()\n            except json.JSONDecodeError:\n                error_details = e.response.text\n            logger.error(f\"API Error {e.response.status_code}: {error_details} for request to {e.request.url}\")\n            # Re-raise the original exception to be handled by the caller\n            raise e\n        except httpx.RequestError as e:\n            # Log network errors (timeout, connection issues)\n            logger.error(f\"Network Error: {e.__class__.__name__} while requesting {e.request.url}: {str(e)}\")\n            raise e\n        except Exception as e:\n            logger.error(f\"Unexpected error during chat completion: {e.__class__.__name__}: {str(e)}\")\n            raise e\n\n    async def _stream_request(self, url: str, payload: Dict[str, Any]) -> AsyncGenerator[DeepSearchResponse, None]:\n        \"\"\"Handles the streaming request and yields parsed chunks.\"\"\"\n        buffer = \"\"\n        async with self.client.stream(\"POST\", url, json=payload) as response:\n            # Check for immediate errors before starting to stream\n            if response.status_code >= 400:\n                 error_content = await response.aread()\n                 error_details = \"Unknown error\"\n                 try:\n                     error_details = json.loads(error_content.decode())\n                 except json.JSONDecodeError:\n                     error_details = error_content.decode()\n                 logger.error(f\"API Error {response.status_code} on stream initiation: {error_details}\")\n                 response.raise_for_status() # Raise HTTPStatusError\n\n            async for line in response.aiter_lines():\n                if not line.strip(): # Skip empty keep-alive lines\n                    continue\n                # logger.debug(f\"Raw stream line: {line}\")\n                buffer += line + '\\n'\n                # Process buffer for complete SSE messages\n                while '\\n\\n' in buffer:\n                    message, buffer = buffer.split('\\n\\n', 1)\n                    if message.startswith(\"data: \"):\n                        data_str = message[len(\"data: \"):].strip()\n                        if data_str == \"[DONE]\":\n                            logger.info(\"Stream finished with [DONE] message.\")\n                            return # End of stream\n                        try:\n                            chunk_data = json.loads(data_str)\n                            # logger.debug(f\"Received stream chunk: {chunk_data}\")\n                            yield DeepSearchResponse.model_validate(chunk_data)\n                        except json.JSONDecodeError:\n                            logger.error(f\"Failed to decode JSON from stream chunk: {data_str}\")\n                        except Exception as e:\n                            logger.error(f\"Error processing stream chunk: {e.__class__.__name__}: {e}\")\n                    else:\n                        logger.warning(f\"Received unexpected stream message format: {message}\")\n\n        # Process any remaining buffer content after stream ends (should ideally be empty)\n        if buffer.strip():\n             logger.warning(f\"Unexpected remaining buffer content after stream: {buffer}\")\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP\nfrom typing import Dict, Any, List, Optional\nimport logging\nimport asyncio\nimport os\nimport json\nfrom dotenv import load_dotenv\nimport httpx\n\n# Import models and API client\nfrom models import ( \n    DeepSearchChatParams, \n    DeepSearchResponse, \n    Message, \n    Usage, \n    Choice, \n    Delta, \n    Annotation\n)\nfrom api import JinaDeepSearchClient\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- Initialize MCP Server ---\nmcp = FastMCP(\n    service_name=\"jina_deepsearch\",\n    description=\"Provides access to the Jina DeepSearch API for advanced web search and reasoning.\"\n)\n\n# --- Initialize API Client ---\n# The client will automatically pick up API key and base URL from environment variables\napi_client = JinaDeepSearchClient()\n\n# --- Tool Definition ---\n@mcp.tool()\nasync def chat_completion(params: DeepSearchChatParams) -> Dict[str, Any]:\n    \"\"\"\n    Performs a deep search and reasoning process using the Jina DeepSearch API.\n\n    This tool calls the Jina DeepSearch /v1/chat/completions endpoint, which is \n    compatible with the OpenAI Chat API schema. It supports streaming and \n    aggregates the results if streaming is enabled (default).\n\n    Args:\n        params: An object containing the parameters for the chat completion, \n                including messages, model, stream preference, and other options.\n\n    Returns:\n        A dictionary representing the final aggregated DeepSearchResponse, \n        including the generated content, citations, usage stats, and visited URLs.\n        Returns an error dictionary if the API call fails.\n    \"\"\"\n    logger.info(f\"Received chat_completion request with stream={params.stream}\")\n    try:\n        api_response = await api_client.chat_completion(params)\n\n        if params.stream:\n            logger.info(\"Processing streamed response...\")\n            final_response = None\n            aggregated_content = \"\"\n            aggregated_annotations: List[Annotation] = []\n            final_usage: Optional[Usage] = None\n            final_visited_urls: Optional[List[str]] = None\n            final_read_urls: Optional[List[str]] = None\n            final_num_urls: Optional[int] = None\n            response_id = None\n            created = None\n            model = None\n            system_fingerprint = None\n            finish_reason = None\n\n            async for chunk in api_response: # type: ignore\n                # logger.debug(f\"Processing chunk: {chunk.model_dump_json(exclude_none=True)}\")\n                if not final_response: # Initialize with first chunk's metadata\n                    final_response = chunk.model_copy(deep=True)\n                    response_id = chunk.id\n                    created = chunk.created\n                    model = chunk.model\n                    system_fingerprint = chunk.system_fingerprint\n                else: # Update potentially changing metadata like ID for tracking\n                    response_id = chunk.id\n\n                if chunk.choices:\n                    delta = chunk.choices[0].delta\n                    if delta:\n                        if delta.content:\n                            aggregated_content += delta.content\n                        if delta.annotations:\n                            aggregated_annotations.extend(delta.annotations)\n                    \n                    # Capture finish reason from the last relevant chunk\n                    if chunk.choices[0].finish_reason:\n                        finish_reason = chunk.choices[0].finish_reason\n                \n                # Capture usage and URL info, usually in the last chunk(s)\n                if chunk.usage:\n                    final_usage = chunk.usage\n                if chunk.visitedURLs:\n                    final_visited_urls = chunk.visitedURLs\n                if chunk.readURLs:\n                    final_read_urls = chunk.readURLs\n                if chunk.numURLs:\n                    final_num_urls = chunk.numURLs\n\n            if final_response is None:\n                 logger.error(\"Stream ended without receiving any valid chunks.\")\n                 return {\"error\": \"Stream ended unexpectedly without data.\"}\n\n            # Construct the final aggregated response object\n            aggregated_choice = Choice(\n                index=0,\n                message=Message(role='assistant', content=aggregated_content),\n                delta=None, # Delta is for streaming chunks, message is for final\n                logprobs=None,\n                finish_reason=finish_reason\n            )\n            # Add annotations to the final message if needed, or keep them separate?\n            # The OpenAI schema puts content in message.content. Let's stick to that.\n            # Annotations might need a custom field in the final aggregated response if not part of message.\n            # For now, let's create a basic aggregated response structure.\n\n            aggregated_response_data = {\n                \"id\": response_id or \"aggregated_stream\",\n                \"object\": \"chat.completion\", # Final object type\n                \"created\": created or 0,\n                \"model\": model or params.model,\n                \"system_fingerprint\": system_fingerprint,\n                \"choices\": [aggregated_choice.model_dump(exclude_none=True)],\n                \"usage\": final_usage.model_dump(exclude_none=True) if final_usage else None,\n                \"visitedURLs\": final_visited_urls,\n                \"readURLs\": final_read_urls,\n                \"numURLs\": final_num_urls,\n                # Add aggregated annotations if needed, maybe as a custom field\n                \"aggregated_annotations\": [anno.model_dump(exclude_none=True) for anno in aggregated_annotations] if aggregated_annotations else None\n            }\n            \n            # Validate the constructed response data before returning\n            try:\n                final_aggregated_response = DeepSearchResponse.model_validate(aggregated_response_data)\n                logger.info(\"Successfully aggregated streamed response.\")\n                return final_aggregated_response.model_dump(exclude_none=True, by_alias=True)\n            except Exception as validation_error:\n                logger.error(f\"Failed to validate aggregated response: {validation_error}\")\n                logger.debug(f\"Aggregated data causing validation error: {aggregated_response_data}\")\n                # Return raw aggregated data if validation fails, with an error note\n                aggregated_response_data['error'] = f\"Failed to validate final aggregated response: {validation_error}\"\n                return aggregated_response_data\n\n        else: # Non-streaming case\n            logger.info(\"Received non-streamed response.\")\n            # api_response is already a DeepSearchResponse object\n            return api_response.model_dump(exclude_none=True, by_alias=True) # type: ignore\n\n    except httpx.HTTPStatusError as e:\n        error_message = f\"API Error: {e.response.status_code}\"\n        try:\n            error_details = e.response.json()\n            error_message += f\" - {error_details}\"\n        except json.JSONDecodeError:\n            error_message += f\" - {e.response.text}\"\n        logger.error(error_message)\n        return {\"error\": error_message, \"status_code\": e.response.status_code}\n    except httpx.RequestError as e:\n        error_message = f\"Network Error: {e.__class__.__name__} - {str(e)}\"\n        logger.error(error_message)\n        return {\"error\": error_message}\n    except Exception as e:\n        error_message = f\"An unexpected error occurred: {e.__class__.__name__} - {str(e)}\"\n        logger.exception(error_message) # Log full traceback for unexpected errors\n        return {\"error\": error_message}\n\n# --- Application Lifecycle Hooks ---\n@mcp.app.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Cleanly close the API client connection on server shutdown.\"\"\"\n    logger.info(\"Shutting down API client...\")\n    await api_client.close()\n    logger.info(\"API client closed.\")\n\n# --- Run Server ---\nif __name__ == \"__main__\":\n    # You can run this script directly using `python main.py`\n    # or using uvicorn for more production-like features: `uvicorn main:mcp.app --reload`\n    logger.info(\"Starting Jina DeepSearch MCP server...\")\n    # Note: FastMCP's run() method is simple; for production, use uvicorn directly.\n    # mcp.run() # This might block in some environments, use uvicorn recommended\n    import uvicorn\n    uvicorn.run(mcp.app, host=\"0.0.0.0\", port=8000)\n"
    },
    {
      "name": "requirements.txt",
      "content": "mcp>=0.1.0\nfastmcp>=0.1.0\nhttpx>=0.25.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.20.0\nsse-starlette>=1.0.0\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina DeepSearch API Configuration\n\n# Get your API key from Jina AI (https://jina.ai/)\n# This is optional for low usage (2 RPM) but recommended for standard (10 RPM) or premium tiers.\nJINA_API_KEY=your_jina_api_key_here\n\n# Base URL for the Jina DeepSearch API (usually does not need changing)\n# JINA_DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai\n"
    },
    {
      "name": "README.md",
      "content": "# Jina DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for interacting with the Jina DeepSearch API using FastMCP.\n\nJina DeepSearch combines web searching, reading, and reasoning to answer complex questions that require iterative investigation, world-knowledge, or up-to-date information. This MCP server exposes its capabilities through a standardized tool interface.\n\n## Features\n\n*   **`chat_completion` Tool:** Provides access to the Jina DeepSearch `/v1/chat/completions` endpoint.\n    *   Accepts conversation history (`messages`), including text and data URIs for images/documents.\n    *   Supports streaming responses (default and recommended).\n    *   Aggregates streamed responses into a single final result.\n    *   Handles various parameters like `model`, `reasoning_effort`, domain filtering, structured output, etc.\n    *   Compatible with the OpenAI Chat API schema.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file:\n        *   Add your Jina API key to `JINA_API_KEY`. You can obtain a key from [Jina AI](https://jina.ai/). While the API works without a key, it's heavily rate-limited (2 RPM). A standard key allows 10 RPM.\n        *   The `JINA_DEEPSEARCH_BASE_URL` is usually not needed unless you are using a custom deployment.\n\n## Running the Server\n\nYou can run the server using Uvicorn (recommended for development and production):\n\n```bash\nuvicorn main:mcp.app --host 0.0.0.0 --port 8000 --reload\n```\n\n*   `--host 0.0.0.0`: Makes the server accessible on your network.\n*   `--port 8000`: Specifies the port to run on.\n*   `--reload`: Automatically restarts the server when code changes (useful for development).\n\nAlternatively, you can run `python main.py`, which also uses Uvicorn internally but might offer less control.\n\nThe server will be available at `http://localhost:8000` (or the specified host/port).\n\n## Authentication\n\nThe server uses Bearer token authentication for the Jina DeepSearch API. It reads the API key from the `JINA_API_KEY` environment variable specified in the `.env` file.\n\nIf no API key is provided, the API calls will be made without authentication, likely resulting in stricter rate limits (2 RPM).\n\n## Tool Usage\n\nAn MCP client can interact with this server by calling the `jina_deepsearch.chat_completion` tool.\n\n**Example MCP Client Request:**\n\n```json\n{\n  \"tool_name\": \"jina_deepsearch.chat_completion\",\n  \"parameters\": {\n    \"params\": {\n      \"model\": \"jina-deepsearch-v1\",\n      \"messages\": [\n        {\"role\": \"user\", \"content\": \"What were the main announcements from Apple's latest WWDC event?\"}\n      ],\n      \"stream\": true,\n      \"reasoning_effort\": \"medium\"\n    }\n  }\n}\n```\n\n**Expected MCP Server Response (Aggregated from Stream):**\n\n```json\n{\n  \"result\": {\n    \"id\": \"chatcmpl-xxxx\",\n    \"object\": \"chat.completion\",\n    \"created\": 1700000000,\n    \"model\": \"jina-deepsearch-v1\",\n    \"system_fingerprint\": \"fp_xxxx\",\n    \"choices\": [\n      {\n        \"index\": 0,\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"Apple's latest WWDC event featured several key announcements including... [Full Answer] ... For more details, you can refer to the official Apple newsroom [1].\"\n        },\n        \"finish_reason\": \"stop\"\n      }\n    ],\n    \"usage\": {\n      \"prompt_tokens\": 25,\n      \"completion_tokens\": 350,\n      \"total_tokens\": 375\n    },\n    \"visitedURLs\": [\"https://www.apple.com/newsroom/...\", \"https://www.techsite.com/...\"],\n    \"readURLs\": [\"https://www.apple.com/newsroom/...\"],\n    \"numURLs\": 2,\n    \"aggregated_annotations\": [\n        {\n            \"type\": \"url_citation\",\n            \"url_citation\": {\n                \"url\": \"https://www.apple.com/newsroom/...\",\n                \"title\": \"Apple Newsroom Article Title\",\n                \"exactQuote\": \"Specific quote from the article.\"\n            }\n        }\n    ]\n  }\n}\n```\n\n## Models\n\nPydantic models are used for type validation and serialization:\n\n*   `DeepSearchChatParams`: Defines the input parameters for the `chat_completion` tool.\n*   `DeepSearchResponse`: Defines the structure of the response from the Jina API (used for both chunks and final responses).\n*   Other supporting models (`Message`, `Choice`, `Delta`, `Usage`, `Annotation`, `UrlCitation`) are defined in `models.py` based on the Jina API schema.\n\n## Error Handling\n\nThe server includes error handling for:\n\n*   **API Errors:** Catches `httpx.HTTPStatusError` (4xx, 5xx responses) from the Jina API and returns a JSON error message including the status code and API-provided details.\n*   **Network Errors:** Catches `httpx.RequestError` (e.g., timeouts, connection issues) and returns a JSON error message.\n*   **Validation Errors:** Pydantic models validate incoming parameters and outgoing responses.\n*   **Unexpected Errors:** Generic exceptions are caught, logged, and returned as a JSON error message.\n\n## Rate Limits\n\nBe aware of the Jina DeepSearch API rate limits:\n\n*   **No API Key:** 2 requests per minute (RPM)\n*   **Standard API Key:** 10 RPM\n*   **Premium API Key:** 100 RPM\n\nExceeding these limits will result in `429 Too Many Requests` errors.\n"
    }
  ]
}