{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Union, Dict, Any, Literal\n\n# Type Definitions based on Implementation Plan\n\nclass ChatMessageContentPart(BaseModel):\n    \"\"\"Represents a part of the content in a message, can be text, image URL, or document URL.\"\"\"\n    type: Literal['text', 'image_url', 'document_url'] = Field(..., description=\"The type of the content part.\")\n    text: Optional[str] = Field(None, description=\"The text content.\")\n    image_url: Optional[Dict[str, str]] = Field(None, description=\"The image URL object, expecting {'url': 'data:image/...'}.\")\n    document_url: Optional[Dict[str, str]] = Field(None, description=\"The document URL object, expecting {'url': 'data:application/pdf...' or 'data:text/plain...'}.\")\n\nclass ChatMessage(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant'] = Field(..., description=\"The role of the message author.\")\n    content: Union[str, List[ChatMessageContentPart]] = Field(..., description=\"The content of the message. Can be simple text or a list of content parts for multimodal input.\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: Optional[str] = Field(None, description=\"URL of the source.\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Number of tokens in the prompt.\")\n    completion_tokens: Optional[int] = Field(None, description=\"Number of tokens in the completion.\")\n    total_tokens: Optional[int] = Field(None, description=\"Total number of tokens used.\")\n\n# Input Model for the chat_completion tool\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the DeepSearch chat completion tool.\"\"\"\n    messages: List[ChatMessage] = Field(..., description=\"A list of messages comprising the conversation history. Can include user, assistant roles. Content can include text, or data URIs for images (webp, png, jpeg) or documents (txt, pdf) up to 10MB.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the DeepSearch model to use.\")\n    stream: bool = Field(True, description=\"Whether to stream back partial progress using server-sent events. Recommended to keep true to avoid timeouts on long requests.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains effort on reasoning. 'low', 'medium', or 'high'. Lower effort may be faster but less thorough. Overridden by 'budget_tokens' or 'max_attempts'.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Larger budgets can improve quality for complex queries. Overrides 'reasoning_effort'.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem using different approaches. Overrides 'reasoning_effort'.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces thinking/search steps even if the query seems trivial.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk, sorted by relevance.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"A JSON schema to ensure the final answer matches the specified structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        use_enum_values = True # Ensure Literal values are handled correctly\n\n# Response Models (Streaming and Non-Streaming)\n\nclass ChoiceDelta(BaseModel):\n    \"\"\"Content delta within a streaming chunk.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None)\n    content: Optional[str] = Field(None)\n\nclass Choice(BaseModel):\n    \"\"\"A single choice in a non-streaming response.\"\"\"\n    index: int\n    message: ChatMessage\n    finish_reason: Optional[str] = None\n    citations: Optional[List[UrlCitation]] = Field(None, description=\"List of citations used for the response.\")\n\nclass DeepSearchChatChunk(BaseModel):\n    \"\"\"Represents a chunk of data in a streaming response.\"\"\"\n    id: str\n    object: str = \"chat.completion.chunk\"\n    created: int\n    model: str\n    choices: List[Dict[str, Any]] # Raw choices data, might contain delta or full message parts\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics, typically provided in the final chunk.\")\n    citations: Optional[List[UrlCitation]] = Field(None, description=\"List of citations, potentially updated across chunks.\")\n    # The actual structure of 'choices' in chunks can vary. Often it's like:\n    # choices: List[{\"index\": int, \"delta\": ChoiceDelta, \"finish_reason\": Optional[str]}]\n    # We keep it flexible with Dict[str, Any] for broader compatibility.\n\nclass DeepSearchChatResponse(BaseModel):\n    \"\"\"Represents the full response for a non-streaming request.\"\"\"\n    id: str\n    object: str = \"chat.completion\"\n    created: int\n    model: str\n    choices: List[Choice]\n    usage: Usage\n    citations: Optional[List[UrlCitation]] = Field(None, description=\"List of citations used for the response.\")\n"
    },
    {
      "name": "client.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nimport asyncio\nfrom typing import AsyncGenerator, Union, Optional, Dict, Any\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatInput, DeepSearchChatResponse, DeepSearchChatChunk\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Default base URL for the DeepSearch API\nDEFAULT_DEEPSEARCH_API_BASE_URL = \"https://deepsearch.jina.ai\"\n\nclass DeepSearchError(Exception):\n    \"\"\"Base exception for DeepSearch client errors.\"\"\"\n    def __init__(self, message: str, status_code: Optional[int] = None, response_data: Optional[Dict[str, Any]] = None):\n        super().__init__(message)\n        self.status_code = status_code\n        self.response_data = response_data\n\nclass AuthenticationError(DeepSearchError):\n    \"\"\"Exception for authentication errors (401).\"\"\"\n    pass\n\nclass InvalidRequestError(DeepSearchError):\n    \"\"\"Exception for invalid request errors (400).\"\"\"\n    pass\n\nclass RateLimitError(DeepSearchError):\n    \"\"\"Exception for rate limit errors (429).\"\"\"\n    pass\n\nclass ServerError(DeepSearchError):\n    \"\"\"Exception for server-side errors (5xx).\"\"\"\n    pass\n\nclass DeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = 120.0):\n        \"\"\"\n        Initializes the DeepSearchClient.\n\n        Args:\n            api_key: Jina AI API key. Defaults to JINA_API_KEY environment variable.\n            base_url: Base URL for the DeepSearch API. Defaults to https://deepsearch.jina.ai.\n            timeout: Request timeout in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise AuthenticationError(\"JINA_API_KEY not provided or found in environment variables.\")\n\n        self.base_url = base_url or os.getenv(\"DEEPSEARCH_API_BASE_URL\", DEFAULT_DEEPSEARCH_API_BASE_URL)\n        self.endpoint = \"/v1/chat/completions\"\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\"\n        }\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=timeout\n        )\n\n    async def _request(\n        self,\n        method: str,\n        url: str,\n        json_data: Optional[Dict[str, Any]] = None,\n        stream: bool = False\n    ) -> Union[httpx.Response, AsyncGenerator[str, None]]:\n        \"\"\"Makes an HTTP request to the DeepSearch API.\"\"\"\n        try:\n            if stream:\n                # For streaming, we need to handle the response differently\n                req = self.client.build_request(method, url, json=json_data)\n                response_stream = await self.client.send(req, stream=True)\n                # Raise status errors early for the initial connection\n                response_stream.raise_for_status()\n                return self._process_stream(response_stream)\n            else:\n                response = await self.client.request(method, url, json=json_data)\n                response.raise_for_status() # Raise HTTPStatusError for 4xx/5xx\n                return response\n\n        except httpx.HTTPStatusError as e:\n            status_code = e.response.status_code\n            try:\n                response_data = e.response.json()\n            except json.JSONDecodeError:\n                response_data = {\"error\": e.response.text or \"Unknown error\"}\n            \n            error_message = f\"HTTP error {status_code}: {response_data.get('error', {}).get('message', str(e))}\"\n            logger.error(f\"{error_message} - URL: {e.request.url}\")\n            \n            if status_code == 401:\n                raise AuthenticationError(error_message, status_code, response_data) from e\n            elif status_code == 400:\n                 raise InvalidRequestError(error_message, status_code, response_data) from e\n            elif status_code == 429:\n                raise RateLimitError(error_message, status_code, response_data) from e\n            elif 500 <= status_code < 600:\n                 raise ServerError(error_message, status_code, response_data) from e\n            else:\n                raise DeepSearchError(error_message, status_code, response_data) from e\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out: {e}\")\n            raise DeepSearchError(f\"Request timed out after {self.client.timeout.read}s\", status_code=408) from e\n        except httpx.RequestError as e:\n            logger.error(f\"An error occurred while requesting {e.request.url!r}: {e}\")\n            raise DeepSearchError(f\"Request error: {e}\") from e\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred: {e}\")\n            raise DeepSearchError(f\"An unexpected error occurred: {str(e)}\") from e\n\n    async def _process_stream(self, response: httpx.Response) -> AsyncGenerator[str, None]:\n        \"\"\"Processes the streaming response (Server-Sent Events).\"\"\"\n        buffer = \"\"\n        async for line in response.aiter_lines():\n            if not line:\n                # Empty line indicates end of an event\n                if buffer.startswith(\"data:\"):\n                    data_str = buffer[len(\"data:\"):].strip()\n                    if data_str == \"[DONE]\":\n                        logger.info(\"Stream finished with [DONE] message.\")\n                        break\n                    try:\n                        # Yield the raw JSON string data part\n                        yield data_str\n                    except json.JSONDecodeError:\n                        logger.warning(f\"Failed to decode JSON from stream chunk: {data_str}\")\n                buffer = \"\"\n            else:\n                buffer += line + \"\\n\" # Rebuild multi-line events if any\n        # Ensure the stream is closed\n        await response.aclose()\n\n    async def chat_completion(\n        self,\n        input_data: DeepSearchChatInput\n    ) -> Union[DeepSearchChatResponse, AsyncGenerator[DeepSearchChatChunk, None]]:\n        \"\"\"\n        Performs a deep search chat completion.\n\n        Args:\n            input_data: The input parameters for the chat completion.\n\n        Returns:\n            If stream=False, returns a DeepSearchChatResponse object.\n            If stream=True, returns an async generator yielding DeepSearchChatChunk objects.\n\n        Raises:\n            AuthenticationError: If the API key is invalid.\n            InvalidRequestError: If the request payload is invalid.\n            RateLimitError: If the rate limit is exceeded.\n            ServerError: If the server encounters an error.\n            DeepSearchError: For other client or connection errors.\n            ValidationError: If the response data doesn't match the Pydantic model.\n        \"\"\"\n        payload = input_data.dict(exclude_none=True)\n        stream = input_data.stream\n\n        logger.info(f\"Sending request to DeepSearch API: stream={stream}\")\n        # logger.debug(f\"Payload: {payload}\") # Be cautious logging sensitive data\n\n        response_or_stream = await self._request(\n            method=\"POST\",\n            url=self.endpoint,\n            json_data=payload,\n            stream=stream\n        )\n\n        if stream:\n            # We expect an async generator of raw JSON strings here\n            if not isinstance(response_or_stream, AsyncGenerator):\n                 raise DeepSearchError(\"Expected an async generator for streaming response, but got something else.\")\n            return self._parse_stream_chunks(response_or_stream)\n        else:\n            # We expect an httpx.Response object here\n            if not isinstance(response_or_stream, httpx.Response):\n                 raise DeepSearchError(\"Expected an httpx.Response for non-streaming response, but got something else.\")\n            try:\n                response_json = response_or_stream.json()\n                logger.info(\"Received non-streaming response from DeepSearch API.\")\n                # logger.debug(f\"Response JSON: {response_json}\")\n                return DeepSearchChatResponse.parse_obj(response_json)\n            except json.JSONDecodeError as e:\n                logger.error(f\"Failed to decode JSON response: {e}\")\n                raise DeepSearchError(f\"Failed to decode JSON response: {response_or_stream.text}\") from e\n            except ValidationError as e:\n                logger.error(f\"Failed to validate response against DeepSearchChatResponse model: {e}\")\n                raise DeepSearchError(f\"Invalid response format received: {e}\") from e\n\n    async def _parse_stream_chunks(self, stream: AsyncGenerator[str, None]) -> AsyncGenerator[DeepSearchChatChunk, None]:\n        \"\"\"Parses raw JSON strings from the stream into DeepSearchChatChunk objects.\"\"\"\n        async for chunk_str in stream:\n            try:\n                chunk_json = json.loads(chunk_str)\n                # logger.debug(f\"Received stream chunk: {chunk_json}\")\n                yield DeepSearchChatChunk.parse_obj(chunk_json)\n            except json.JSONDecodeError as e:\n                logger.warning(f\"Failed to decode JSON stream chunk: {chunk_str}, error: {e}\")\n                # Decide whether to skip or raise. Skipping might be more robust for minor stream issues.\n                continue\n            except ValidationError as e:\n                logger.warning(f\"Failed to validate stream chunk against DeepSearchChatChunk model: {chunk_str}, error: {e}\")\n                # Skipping invalid chunks\n                continue\n            except Exception as e:\n                logger.exception(f\"Unexpected error processing stream chunk: {chunk_str}, error: {e}\")\n                # Depending on severity, you might want to raise here\n                continue\n\n# Example usage (for testing client directly)\nasync def main():\n    load_dotenv()\n    client = DeepSearchClient()\n    test_input = DeepSearchChatInput(\n        messages=[ChatMessage(role=\"user\", content=\"What is the weather in San Francisco?\")],\n        stream=False # Test non-streaming first\n    )\n    try:\n        print(\"--- Testing Non-Streaming --- \")\n        response = await client.chat_completion(test_input)\n        print(response.json(indent=2))\n\n        print(\"\\n--- Testing Streaming --- \")\n        test_input.stream = True\n        async for chunk in await client.chat_completion(test_input):\n             print(chunk.json())\n\n    except DeepSearchError as e:\n        print(f\"An error occurred: {e}\")\n        if e.response_data:\n            print(f\"Response data: {e.response_data}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    from dotenv import load_dotenv\n    asyncio.run(main())\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP\nfrom typing import Union, AsyncGenerator\nimport logging\nimport os\nfrom dotenv import load_dotenv\n\nfrom models import DeepSearchChatInput, DeepSearchChatResponse, DeepSearchChatChunk\nfrom client import DeepSearchClient, DeepSearchError, AuthenticationError, InvalidRequestError, RateLimitError, ServerError\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP service for interacting with the Jina AI DeepSearch API. DeepSearch combines web searching, reading, and reasoning for comprehensive investigation.\"\n)\n\n# Initialize DeepSearch API Client\ntry:\n    deepsearch_client = DeepSearchClient()\n    logger.info(\"DeepSearchClient initialized successfully.\")\nexcept AuthenticationError as e:\n    logger.error(f\"Authentication Error: {e}. Please check your JINA_API_KEY.\")\n    # Exit or prevent server start if auth fails critically\n    # For now, we'll let it proceed but tools will fail.\n    deepsearch_client = None # Indicate client is not usable\nexcept Exception as e:\n    logger.exception(f\"Failed to initialize DeepSearchClient: {e}\")\n    deepsearch_client = None\n\n@mcp.tool()\nasync def chat_completion(input_data: DeepSearchChatInput) -> Union[DeepSearchChatResponse, AsyncGenerator[DeepSearchChatChunk, None]]:\n    \"\"\"\n    Performs a deep search and reasoning process based on a conversation history using the Jina AI DeepSearch API.\n\n    Takes user queries, searches the web, reads relevant content, and iteratively reasons to find the best answer.\n    Supports streaming responses, domain filtering, and control over reasoning effort.\n\n    Args:\n        input_data: An object containing the parameters for the chat completion request, including messages, model, stream flag, and other options.\n\n    Returns:\n        If stream=False in input_data, returns a single DeepSearchChatResponse object containing the final answer, usage statistics, and visited URLs.\n        If stream=True in input_data, returns an async generator yielding DeepSearchChatChunk objects, where the final chunk contains the full answer and usage details.\n\n    Raises:\n        MCP specific errors based on client exceptions (e.g., AuthenticationError, RateLimitError, etc.)\n    \"\"\"\n    if not deepsearch_client:\n        logger.error(\"DeepSearchClient is not initialized. Cannot process request.\")\n        # FastMCP typically handles raising exceptions, converting them to MCP errors\n        raise ConnectionError(\"DeepSearch client is not available due to initialization failure.\")\n\n    logger.info(f\"Received chat_completion request: stream={input_data.stream}\")\n    try:\n        result = await deepsearch_client.chat_completion(input_data)\n        logger.info(f\"Successfully initiated/completed chat_completion request for stream={input_data.stream}\")\n        return result\n    except AuthenticationError as e:\n        logger.error(f\"Authentication failed: {e}\")\n        raise # Re-raise for FastMCP to handle\n    except InvalidRequestError as e:\n        logger.error(f\"Invalid request: {e}\")\n        raise # Re-raise\n    except RateLimitError as e:\n        logger.error(f\"Rate limit exceeded: {e}\")\n        raise # Re-raise\n    except ServerError as e:\n        logger.error(f\"DeepSearch server error: {e}\")\n        raise # Re-raise\n    except DeepSearchError as e:\n        logger.error(f\"DeepSearch client error: {e}\")\n        raise # Re-raise\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred in chat_completion tool: {e}\")\n        raise # Re-raise\n\nif __name__ == \"__main__\":\n    # This allows running the server directly using Uvicorn\n    # Example: uvicorn main:mcp --host 0.0.0.0 --port 8000 --reload\n    # The FastMCP's run() method might start its own server or provide instructions.\n    # Check FastMCP documentation for the preferred way to run.\n    # For development, using uvicorn is common:\n    import uvicorn\n    logger.info(\"Starting MCP server with Uvicorn...\")\n    # Make sure the app object is accessible if FastMCP doesn't provide a run command\n    # Assuming FastMCP exposes an ASGI app instance, e.g., mcp.app\n    # If not, adjust according to FastMCP's usage guide.\n    # uvicorn.run(\"main:mcp\", host=\"0.0.0.0\", port=8000, reload=True) # Adjust if 'mcp' is not the ASGI app\n\n    # If FastMCP has its own run method:\n    mcp.run() # This might block and start the server\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0 # Replace with actual version if known, or keep flexible\nhttpx>=0.25.0,<0.28.0\npydantic>=2.0.0,<3.0.0\ntyping_extensions>=4.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.20.0 # For running the server\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain your key from the Jina AI platform\nJINA_API_KEY=\"your_jina_api_key_here\"\n\n# Optional: Override the default DeepSearch API base URL\n# DEEPSEARCH_API_BASE_URL=\"https://deepsearch.jina.ai\"\n"
    },
    {
      "name": "README.md",
      "content": "# Jina AI DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for interacting with the Jina AI DeepSearch API using the FastMCP framework.\n\n## Description\n\nJina AI DeepSearch combines web searching, reading, and reasoning capabilities to perform comprehensive investigations into complex questions. It functions as an agent that iteratively researches topics to provide accurate and well-supported answers. The DeepSearch API is designed to be compatible with the OpenAI Chat API schema.\n\nThis MCP server exposes the core functionality of the DeepSearch API as a tool, allowing agents or applications to leverage its advanced search and reasoning capabilities through the MCP standard.\n\n## Features\n\n*   Provides an MCP interface to the Jina AI DeepSearch `/v1/chat/completions` endpoint.\n*   Supports both standard (non-streaming) and streaming responses.\n*   Handles authentication using Jina AI API keys.\n*   Includes robust error handling for API and network issues.\n*   Uses Pydantic models for clear data validation and structure.\n*   Configurable via environment variables.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2.  **Create a virtual environment:**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure environment variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina AI API key:\n        ```dotenv\n        JINA_API_KEY=\"your_jina_api_key_here\"\n        ```\n        You can obtain an API key from the [Jina AI Platform](https://jina.ai/).\n\n## Running the Server\n\nYou can run the MCP server using Uvicorn:\n\n```bash\nuvicorn main:mcp --host 0.0.0.0 --port 8000 --reload\n```\n\n*   `--host 0.0.0.0`: Makes the server accessible on your network.\n*   `--port 8000`: Specifies the port to run on (adjust if needed).\n*   `--reload`: Automatically restarts the server when code changes (useful for development).\n\nAlternatively, if `FastMCP` provides a built-in run command, you might be able to run it directly:\n\n```bash\npython main.py\n```\n\nCheck the console output for the address where the server is running (e.g., `http://127.0.0.1:8000`).\n\n## Available Tools\n\nThis MCP server provides the following tool:\n\n### `chat_completion`\n\n*   **Description:** Performs a deep search and reasoning process based on a conversation history. It takes user queries, searches the web, reads relevant content, and iteratively reasons to find the best answer. Supports streaming responses, domain filtering, and control over reasoning effort.\n*   **Input:** `DeepSearchChatInput` model\n    *   `messages` (List[ChatMessage], required): Conversation history (user/assistant roles, text/image/document content).\n    *   `model` (str, optional, default: \"jina-deepsearch-v1\"): ID of the DeepSearch model.\n    *   `stream` (bool, optional, default: True): Enable streaming response (recommended).\n    *   `reasoning_effort` (Literal['low', 'medium', 'high'], optional, default: 'medium'): Control reasoning depth.\n    *   `budget_tokens` (int, optional): Max token budget.\n    *   `max_attempts` (int, optional): Max retry attempts.\n    *   `no_direct_answer` (bool, optional, default: False): Force search/thinking steps.\n    *   `max_returned_urls` (int, optional): Max URLs in the final answer.\n    *   `structured_output` (Dict[str, Any], optional): JSON schema for structured output.\n    *   `good_domains` (List[str], optional): Prioritized domains.\n    *   `bad_domains` (List[str], optional): Excluded domains.\n    *   `only_domains` (List[str], optional): Exclusively included domains.\n*   **Output:**\n    *   If `stream=False`: A single `DeepSearchChatResponse` object.\n    *   If `stream=True`: An `AsyncGenerator` yielding `DeepSearchChatChunk` objects.\n\n## Example Usage (Conceptual)\n\nUsing an MCP client (like `mcp-client` CLI or a Python library):\n\n**Non-Streaming:**\n\n```python\nfrom mcp import MCPClient\n\nclient = MCPClient(host='localhost', port=8000)\n\ninput_data = {\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Explain the concept of quantum entanglement in simple terms.\"}\n    ],\n    \"stream\": False,\n    \"reasoning_effort\": \"medium\"\n}\n\nresponse = await client.call('deepsearch', 'chat_completion', input_data=input_data)\n\nprint(response)\n# Output: DeepSearchChatResponse object\n```\n\n**Streaming:**\n\n```python\nfrom mcp import MCPClient\n\nclient = MCPClient(host='localhost', port=8000)\n\ninput_data = {\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Explain the concept of quantum entanglement in simple terms.\"}\n    ],\n    \"stream\": True\n}\n\nasync for chunk in await client.stream('deepsearch', 'chat_completion', input_data=input_data):\n    print(chunk)\n    # Output: DeepSearchChatChunk objects as they arrive\n```\n\n## Error Handling\n\nThe server includes error handling for common issues:\n\n*   **Authentication Errors:** If the `JINA_API_KEY` is missing or invalid.\n*   **Invalid Requests:** If the input data format is incorrect.\n*   **Rate Limits:** If the API rate limits are exceeded.\n*   **Server Errors:** If the DeepSearch API encounters internal errors.\n*   **Network/Timeout Errors:** If the connection to the API fails or times out.\n\nErrors from the DeepSearch API will be propagated as exceptions, which FastMCP should translate into appropriate MCP error responses.\n"
    }
  ]
}