{
  "files": [
    {
      "name": "main.py",
      "content": "import logging\nimport os\nfrom typing import Dict, Any\n\nfrom dotenv import load_dotenv\nfrom mcp.server.fastmcp import FastMCP\n\nfrom api import DeepSearchClient, DeepSearchAPIError\nfrom models import DeepSearchChatInput, DeepSearchChatResponse\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch_jina\",\n    description=\"MCP service for Jina AI's DeepSearch API. DeepSearch combines web searching, reading, and reasoning to provide comprehensive answers to complex questions, especially those requiring up-to-date information or iterative investigation. It is designed to be compatible with the OpenAI Chat API schema.\"\n)\n\n# Initialize API Client\napi_key = os.getenv(\"JINA_API_KEY\")\nif not api_key:\n    logger.warning(\"JINA_API_KEY environment variable not set. API calls will likely fail.\")\n    # Optionally raise an error or exit if the key is absolutely required to start\n    # raise ValueError(\"JINA_API_KEY must be set in the environment.\")\n\napi_client = DeepSearchClient(api_key=api_key)\n\n@mcp.tool(input_model=DeepSearchChatInput, returns=DeepSearchChatResponse)\nasync def chat_completion(params: DeepSearchChatInput) -> Dict[str, Any]:\n    \"\"\"Sends a chat conversation to the DeepSearch model ('jina-deepsearch-v1') for processing.\n\n    The model performs iterative search, reading, and reasoning to generate a\n    comprehensive answer, citing sources. Supports text, images (webp, png, jpeg\n    encoded as data URIs), and documents (txt, pdf encoded as data URIs) within\n    messages. Streaming is recommended and handled by default; this tool returns\n    the aggregated final response.\n\n    Args:\n        params: Input parameters for the DeepSearch chat completion, conforming to DeepSearchChatInput model.\n\n    Returns:\n        The aggregated final response from the DeepSearch model as a dictionary,\n        conforming to DeepSearchChatResponse model, or an error dictionary.\n    \"\"\"\n    logger.info(f\"Received chat_completion request with model: {params.model}\")\n    try:\n        # Ensure streaming is enabled if not explicitly set to false, as recommended\n        if params.stream is None:\n            params.stream = True\n            logger.info(\"Defaulting to stream=True for chat_completion\")\n        elif not params.stream:\n            logger.warning(\"Using stream=False. This might lead to timeouts for complex queries.\")\n\n        response = await api_client.chat_completion(params)\n        logger.info(f\"Successfully completed chat_completion request ID: {response.id}\")\n        # FastMCP expects a dictionary, so convert the Pydantic model\n        return response.dict(exclude_none=True)\n\n    except DeepSearchAPIError as e:\n        logger.error(f\"API error during chat_completion: {e.status_code} - {e.message}\")\n        return {\"error\": f\"API Error: {e.status_code} - {e.message}\"}\n    except httpx.TimeoutException as e:\n        logger.error(f\"Timeout error during chat_completion: {e}\")\n        return {\"error\": f\"Request timed out: {e}\"}\n    except httpx.RequestError as e:\n        logger.error(f\"Request error during chat_completion: {e}\")\n        return {\"error\": f\"HTTP Request failed: {e}\"}\n    except Exception as e:\n        logger.exception(\"Unexpected error during chat_completion\")\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\nif __name__ == \"__main__\":\n    # Note: Authentication (checking JINA_API_KEY) happens implicitly\n    # within the DeepSearchClient initialization and API calls.\n    mcp.run()\n"
    },
    {
      "name": "models.py",
      "content": "from typing import List, Optional, Union, Dict, Any, Literal\nfrom pydantic import BaseModel, Field\n\n# --- Input Models ---\n\nclass ChatMessageContentPartText(BaseModel):\n    type: Literal[\"text\"]\n    text: str\n\nclass ChatMessageContentPartImage(BaseModel):\n    type: Literal[\"image_url\"]\n    image_url: Dict[str, str] # e.g. {\"url\": \"data:image/jpeg;base64,...\"}\n\nclass ChatMessageContentPartDocument(BaseModel):\n    type: Literal[\"document_url\"]\n    document_url: Dict[str, str] # e.g. {\"url\": \"data:application/pdf;base64,...\"}\n\nChatMessageContent = Union[\n    str, # Simple text content\n    List[Union[ # List for multimodal content\n        ChatMessageContentPartText,\n        ChatMessageContentPartImage,\n        ChatMessageContentPartDocument\n    ]]\n]\n\nclass ChatMessage(BaseModel):\n    \"\"\"Represents a single message in the chat conversation.\"\"\"\n    role: Literal['user', 'assistant', 'system'] = Field(..., description=\"The role of the message author.\")\n    content: ChatMessageContent = Field(..., description=\"The content of the message. Can be a simple string for text, or a list of content blocks for multi-modal input (e.g., text + image/document). Image/document content should be provided as data URIs.\")\n    name: Optional[str] = Field(None, description=\"An optional name for the participant.\")\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the DeepSearch chat completion tool.\"\"\"\n    messages: List[ChatMessage] = Field(..., description=\"A list of messages comprising the conversation history. Include user queries and any previous assistant responses. Supports text, image, and document content.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use. Currently only 'jina-deepsearch-v1' is supported.\")\n    stream: Optional[bool] = Field(True, description=\"Whether to stream the response. Strongly recommended to be true to avoid timeouts. The MCP tool will handle stream aggregation internally and return the final result.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains reasoning effort. Supported values: 'low', 'medium', 'high'. Lower effort may yield faster responses with fewer reasoning tokens.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum tokens allowed for the DeepSearch process. Overrides 'reasoning_effort'. Larger budgets may improve quality for complex queries.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Overrides 'reasoning_effort'. Allows trying different reasoning approaches.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces thinking/search steps even for seemingly trivial queries.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk, sorted by relevance.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"A JSON schema to ensure the final answer conforms to the specified structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        use_enum_values = True # Ensure Literal values are used correctly\n\n# --- Output Models ---\n\nclass ResponseMessage(BaseModel):\n    \"\"\"Represents the message generated by the model in the response.\"\"\"\n    role: Literal['assistant'] = Field(..., description=\"The role of the message author, typically 'assistant'.\")\n    content: Optional[str] = Field(None, description=\"The content of the message generated by the model.\")\n\nclass ResponseChoice(BaseModel):\n    \"\"\"Represents a single choice in the chat completion response.\"\"\"\n    index: int = Field(..., description=\"The index of the choice in the list of choices.\")\n    message: ResponseMessage = Field(..., description=\"The message generated by the model.\")\n    finish_reason: Optional[str] = Field(None, description=\"The reason the model stopped generating tokens (e.g., 'stop', 'length').\")\n\nclass UsageStats(BaseModel):\n    \"\"\"Usage statistics for the completion request.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Number of tokens in the prompt.\")\n    completion_tokens: Optional[int] = Field(None, description=\"Number of tokens in the generated completion.\")\n    total_tokens: Optional[int] = Field(None, description=\"Total number of tokens used in the request (prompt + completion).\")\n\nclass DeepSearchChatResponse(BaseModel):\n    \"\"\"Represents the final aggregated response from a DeepSearch chat completion request.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(..., description=\"The object type, typically 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"This fingerprint represents the backend configuration that the model runs with.\")\n    choices: List[ResponseChoice] = Field(..., description=\"A list of chat completion choices. DeepSearch typically returns one choice.\")\n    usage: Optional[UsageStats] = Field(None, description=\"Usage statistics for the completion request.\")\n    # DeepSearch specific fields\n    visitedURLs: Optional[List[str]] = Field(None, description=\"List of all URLs visited during the search process.\")\n    readURLs: Optional[List[str]] = Field(None, description=\"List of URLs whose content was read and used for generating the answer.\")\n    numURLs: Optional[int] = Field(None, description=\"Total number of unique URLs encountered.\")\n\n# --- Stream Chunk Models (for internal aggregation) ---\n\nclass StreamDelta(BaseModel):\n    role: Optional[Literal['assistant']] = None\n    content: Optional[str] = None\n\nclass StreamChoice(BaseModel):\n    index: int\n    delta: StreamDelta\n    finish_reason: Optional[str] = None\n\nclass StreamChunk(BaseModel):\n    id: str\n    object: str # e.g., 'chat.completion.chunk'\n    created: int\n    model: str\n    system_fingerprint: Optional[str] = None\n    choices: List[StreamChoice]\n    # DeepSearch specific fields might appear in the *last* chunk or a separate event\n    usage: Optional[UsageStats] = None\n    visitedURLs: Optional[List[str]] = None\n    readURLs: Optional[List[str]] = None\n    numURLs: Optional[int] = None\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport json\nimport logging\nimport os\nfrom typing import Optional, AsyncGenerator, Dict, Any\n\nfrom models import (\n    DeepSearchChatInput,\n    DeepSearchChatResponse,\n    StreamChunk,\n    ResponseMessage,\n    ResponseChoice,\n    UsageStats\n)\n\nlogger = logging.getLogger(__name__)\n\n# Define a custom exception for API errors\nclass DeepSearchAPIError(Exception):\n    def __init__(self, status_code: int, message: str):\n        self.status_code = status_code\n        self.message = message\n        super().__init__(f\"[{status_code}] {message}\")\n\nclass DeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://deepsearch.jina.ai/v1\"\n    DEFAULT_TIMEOUT = 180.0  # seconds, increased for potentially long searches\n    # Rate limit info (for reference, not strictly enforced by client)\n    RATE_LIMIT_REQUESTS = 10\n    RATE_LIMIT_PERIOD = \"per_minute\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = DEFAULT_TIMEOUT):\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"JINA_API_KEY is required. Pass it to the constructor or set the environment variable.\")\n\n        self.base_url = base_url or self.DEFAULT_BASE_URL\n        self.timeout = timeout\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Explicitly accept JSON for non-streamed\n        }\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=self.timeout\n        )\n\n    async def _request(\n        self,\n        method: str,\n        endpoint: str,\n        payload: Optional[Dict[str, Any]] = None,\n        stream: bool = False\n    ) -> Union[Dict[str, Any], AsyncGenerator[str, None]]:\n        \"\"\"Makes an HTTP request to the DeepSearch API.\"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        try:\n            if stream:\n                # For streaming, we need to handle SSE\n                self.headers[\"Accept\"] = \"text/event-stream\"\n                req = self.client.build_request(method, endpoint, json=payload)\n                response_stream = await self.client.send(req, stream=True)\n                response_stream.raise_for_status() # Raise HTTP errors before streaming\n                return self._process_stream(response_stream)\n            else:\n                self.headers[\"Accept\"] = \"application/json\"\n                response = await self.client.request(method, endpoint, json=payload)\n                response.raise_for_status() # Raise HTTP errors (4xx, 5xx)\n                return response.json()\n\n        except httpx.HTTPStatusError as e:\n            # Attempt to parse error details from response body\n            error_message = f\"HTTP error occurred: {e.response.status_code} {e.response.reason_phrase}\"\n            try:\n                error_details = e.response.json()\n                if isinstance(error_details, dict) and 'error' in error_details:\n                    if isinstance(error_details['error'], dict) and 'message' in error_details['error']:\n                        error_message = error_details['error']['message']\n                    elif isinstance(error_details['error'], str):\n                         error_message = error_details['error']\n                elif isinstance(error_details, dict) and 'detail' in error_details:\ # Handle FastAPI validation errors etc.\n                    error_message = str(error_details['detail'])\n                logger.error(f\"API Error Response Body: {error_details}\")\n            except json.JSONDecodeError:\n                error_message = f\"{error_message}. Response body: {e.response.text[:500]}\" # Log first 500 chars\n\n            logger.error(f\"HTTP Status Error: {e.request.method} {e.request.url} - Status {e.response.status_code} - Message: {error_message}\")\n            raise DeepSearchAPIError(status_code=e.response.status_code, message=error_message) from e\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out: {e.request.method} {e.request.url}\")\n            raise # Re-raise timeout to be handled by caller\n        except httpx.RequestError as e:\n            logger.error(f\"Request error: {e.request.method} {e.request.url} - {e}\")\n            raise DeepSearchAPIError(status_code=500, message=f\"Request failed: {e}\") from e\n        except Exception as e:\n            logger.exception(f\"Unexpected error during API request to {url}\")\n            raise DeepSearchAPIError(status_code=500, message=f\"An unexpected error occurred: {str(e)}\") from e\n\n    async def _process_stream(self, response: httpx.Response) -> AsyncGenerator[str, None]:\n        \"\"\"Processes the SSE stream from the API.\"\"\"\n        async for line in response.aiter_lines():\n            if line.startswith('data: '):\n                data_content = line[len('data: '):].strip()\n                if data_content == \"[DONE]\":\n                    logger.info(\"Stream finished with [DONE] marker.\")\n                    break\n                if data_content:\n                    yield data_content\n            elif line.strip(): # Log other non-empty lines if needed\n                logger.debug(f\"Received non-data line in stream: {line}\")\n        # Ensure the response is closed\n        await response.aclose()\n\n    async def _aggregate_streamed_response(self, stream_generator: AsyncGenerator[str, None]) -> DeepSearchChatResponse:\n        \"\"\"Aggregates chunks from a stream into a final response object.\"\"\"\n        final_response_data = {}\n        aggregated_content = \"\"\n        final_choice = None\n        usage_stats = None\n        deepsearch_meta = {}\n\n        try:\n            async for data_json_str in stream_generator:\n                try:\n                    chunk_data = json.loads(data_json_str)\n                    chunk = StreamChunk.parse_obj(chunk_data)\n\n                    # Store initial metadata (id, model, created, etc.) from the first chunk\n                    if not final_response_data:\n                        final_response_data = {\n                            \"id\": chunk.id,\n                            \"object\": \"chat.completion\", # Final object type\n                            \"created\": chunk.created,\n                            \"model\": chunk.model,\n                            \"system_fingerprint\": chunk.system_fingerprint\n                        }\n\n                    if chunk.choices:\n                        delta = chunk.choices[0].delta\n                        if delta.content:\n                            aggregated_content += delta.content\n                        # Capture the finish reason and final choice structure\n                        if chunk.choices[0].finish_reason:\n                            final_choice = ResponseChoice(\n                                index=chunk.choices[0].index,\n                                message=ResponseMessage(role='assistant', content=aggregated_content),\n                                finish_reason=chunk.choices[0].finish_reason\n                            )\n\n                    # Capture usage and DeepSearch metadata (often in the last chunk)\n                    if chunk.usage:\n                        usage_stats = UsageStats.parse_obj(chunk.usage)\n                    if chunk.visitedURLs is not None:\n                        deepsearch_meta['visitedURLs'] = chunk.visitedURLs\n                    if chunk.readURLs is not None:\n                        deepsearch_meta['readURLs'] = chunk.readURLs\n                    if chunk.numURLs is not None:\n                        deepsearch_meta['numURLs'] = chunk.numURLs\n\n                except json.JSONDecodeError:\n                    logger.error(f\"Failed to decode JSON from stream chunk: {data_json_str}\")\n                    continue # Skip malformed chunks\n                except Exception as e:\n                    logger.exception(f\"Error processing stream chunk: {chunk_data}\")\n                    continue # Skip problematic chunks\n\n            if not final_response_data:\n                 raise DeepSearchAPIError(status_code=500, message=\"Stream ended unexpectedly without yielding data.\")\n\n            # Assemble the final response\n            if not final_choice:\n                # If stream ended without a finish_reason, construct a basic choice\n                logger.warning(\"Stream ended without a finish_reason. Assembling partial response.\")\n                final_choice = ResponseChoice(\n                    index=0,\n                    message=ResponseMessage(role='assistant', content=aggregated_content),\n                    finish_reason='unknown' # Indicate potential truncation or error\n                )\n            else:\n                 # Ensure the aggregated content is in the final choice message\n                 final_choice.message.content = aggregated_content\n\n            final_response_data['choices'] = [final_choice]\n            if usage_stats:\n                final_response_data['usage'] = usage_stats\n            final_response_data.update(deepsearch_meta)\n\n            return DeepSearchChatResponse.parse_obj(final_response_data)\n\n        except DeepSearchAPIError: # Propagate API errors immediately\n            raise\n        except Exception as e:\n            logger.exception(\"Error during stream aggregation\")\n            raise DeepSearchAPIError(status_code=500, message=f\"Failed to aggregate stream: {str(e)}\") from e\n\n    async def chat_completion(self, params: DeepSearchChatInput) -> DeepSearchChatResponse:\n        \"\"\"Sends a chat completion request to the DeepSearch API.\n\n        Handles both streaming and non-streaming requests.\n        For streaming requests, aggregates the response chunks.\n        \"\n        endpoint = \"/chat/completions\"\n        # Convert Pydantic model to dict, excluding None values for clean payload\n        payload = params.dict(exclude_none=True)\n\n        if params.stream:\n            logger.info(f\"Initiating streamed chat completion request to {endpoint}\")\n            stream_generator = await self._request(method=\"POST\", endpoint=endpoint, payload=payload, stream=True)\n            # Type hint check\n            if not isinstance(stream_generator, AsyncGenerator):\n                 raise TypeError(\"Expected AsyncGenerator from _request for streaming.\")\n            aggregated_response = await self._aggregate_streamed_response(stream_generator)\n            return aggregated_response\n        else:\n            logger.info(f\"Initiating non-streamed chat completion request to {endpoint}\")\n            response_data = await self._request(method=\"POST\", endpoint=endpoint, payload=payload, stream=False)\n            # Type hint check\n            if not isinstance(response_data, dict):\n                 raise TypeError(\"Expected dict from _request for non-streaming.\")\n            return DeepSearchChatResponse.parse_obj(response_data)\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\nhttpx>=0.23.0,<0.28.0\npydantic>=2.0.0,<3.0.0\npython-dotenv>=1.0.0\n# sseclient-py is not needed as basic SSE parsing is implemented in api.py\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain your key from https://jina.ai/cloud/\nJINA_API_KEY=\"your_jina_api_key_here\"\n\n# Optional: Override the default DeepSearch API base URL\n# JINA_API_BASE_URL=\"https://deepsearch.jina.ai/v1\"\n"
    },
    {
      "name": "README.md",
      "content": "# Jina DeepSearch MCP Server\n\nThis repository contains an MCP (Model Context Protocol) server implementation for interacting with the [Jina AI DeepSearch API](https://jina.ai/deepsearch/) using [FastMCP](https://github.com/cognosis-ai/mcp?).\n\nDeepSearch combines web searching, reading, and reasoning to provide comprehensive answers to complex questions, especially those requiring up-to-date information or iterative investigation. It is designed to be compatible with the OpenAI Chat API schema.\n\n## Features\n\n*   Provides an MCP interface to the Jina DeepSearch `/v1/chat/completions` endpoint.\n*   Handles authentication using a Jina API key.\n*   Supports streaming responses (recommended and default) with internal aggregation.\n*   Includes Pydantic models for request validation and response parsing.\n*   Basic error handling for API errors, timeouts, and request issues.\n\n## Setup\n\n1.  **Clone the repository (or create the files):**\n    ```bash\n    # If cloned:\n    # git clone <repository_url>\n    # cd <repository_directory>\n    ```\n\n2.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n3.  **Configure Environment Variables:**\n    Create a `.env` file in the project root directory by copying the example:\n    ```bash\n    cp .env.example .env\n    ```\n    Edit the `.env` file and add your Jina AI API key:\n    ```env\n    JINA_API_KEY=\"your_jina_api_key_here\"\n    ```\n    You can obtain an API key from the [Jina AI Cloud dashboard](https://jina.ai/cloud/).\n\n## Running the Server\n\nStart the MCP server using:\n\n```bash\npython main.py\n```\n\nThe server will typically start on `http://localhost:8000` (or the default FastMCP port).\n\n## Available Tools\n\n### `chat_completion`\n\nSends a chat conversation to the DeepSearch model (`jina-deepsearch-v1`) for processing.\n\n**Description:** The model performs iterative search, reading, and reasoning to generate a comprehensive answer, citing sources. Supports text, images (webp, png, jpeg encoded as data URIs), and documents (txt, pdf encoded as data URIs) within messages. Streaming is recommended and handled by default; this tool returns the aggregated final response.\n\n**Input Parameters (`DeepSearchChatInput` model):**\n\n*   `messages` (List[`ChatMessage`], **required**): A list of messages comprising the conversation history. See `ChatMessage` structure below.\n*   `model` (str, optional, default: `\"jina-deepsearch-v1\"`): ID of the model to use.\n*   `stream` (bool, optional, default: `True`): Whether to stream the response. Strongly recommended. The tool aggregates the stream.\n*   `reasoning_effort` (str, optional, default: `\"medium\"`): Constrains reasoning effort (`'low'`, `'medium'`, `'high'`).\n*   `budget_tokens` (int, optional): Maximum tokens allowed for the DeepSearch process.\n*   `max_attempts` (int, optional): Maximum number of retries for solving the problem.\n*   `no_direct_answer` (bool, optional, default: `False`): Forces thinking/search steps.\n*   `max_returned_urls` (int, optional): Maximum number of URLs in the final answer.\n*   `structured_output` (Dict[str, Any], optional): A JSON schema for structured output.\n*   `good_domains` (List[str], optional): List of domains to prioritize.\n*   `bad_domains` (List[str], optional): List of domains to exclude.\n*   `only_domains` (List[str], optional): List of domains to exclusively include.\n\n**`ChatMessage` Structure:**\n\n```python\n{\n  \"role\": \"user\" | \"assistant\" | \"system\",\n  \"content\": \"text content\" | [\n    {\"type\": \"text\", \"text\": \"...\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,...\"}},\n    {\"type\": \"document_url\", \"document_url\": {\"url\": \"data:application/pdf;base64,...\"}}\n  ],\n  \"name\": Optional[str]\n}\n```\n\n**Returns (`DeepSearchChatResponse` model):**\n\nA dictionary containing the aggregated response from the API, including:\n\n*   `id` (str): Unique ID for the completion.\n*   `object` (str): Object type (`chat.completion`).\n*   `created` (int): Timestamp.\n*   `model` (str): Model used.\n*   `choices` (List[`ResponseChoice`]): List containing the generated message and finish reason.\n*   `usage` (Optional[`UsageStats`]): Token usage information.\n*   `visitedURLs` (Optional[List[str]]): URLs visited during search.\n*   `readURLs` (Optional[List[str]]): URLs read for the answer.\n*   `numURLs` (Optional[int]): Total unique URLs encountered.\n*   `error` (str, optional): Present if an error occurred during processing.\n\n## Authentication\n\nThe server uses Bearer Token authentication. The `JINA_API_KEY` from your `.env` file is automatically included in the `Authorization` header for requests made to the Jina DeepSearch API.\n\n## Error Handling\n\nThe server attempts to catch and log common errors:\n\n*   **API Errors:** Catches `HTTPStatusError` from `httpx` and wraps them in `DeepSearchAPIError`, returning a JSON error message with the status code and message from the API response if available.\n*   **Timeouts:** Catches `httpx.TimeoutException`.\n*   **Request Errors:** Catches other `httpx.RequestError` issues (e.g., connection problems).\n*   **Unexpected Errors:** Catches general exceptions during tool execution.\n\n## Rate Limits\n\nThe Jina DeepSearch API has rate limits (e.g., 10 RPM for standard keys). If you exceed the rate limit, the API will return a `429 Too Many Requests` error, which will be caught and reported by the MCP tool. Implementations consuming this MCP should handle potential rate limit errors gracefully (e.g., with backoff and retry logic).\n"
    }
  ]
}