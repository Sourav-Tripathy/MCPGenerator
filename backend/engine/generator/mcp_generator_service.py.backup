from typing import Dict, Any, Optional, List, Union
import os
import logging
import json
import tempfile
import shutil
from pathlib import Path
import asyncio
import sys
import uuid
import aiofiles  # Add import for aiofiles
import re
import time

# Add parent directory to sys.path to allow imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
# Remove SQLAlchemy imports and add Supabase client
from db.supabase_client import templateOperations, serverOperations, chatSessionOperations

from .doc_processor import DocProcessor, JinaDocumentProcessor
from .llm_workflow import LLMWorkflow, AgentState

# Configure logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MCPGeneratorService:
    """
    Service for generating MCP servers from API documentation.
    
    This service integrates document processing and LLM-based code generation
    to create custom MCP servers based on API documentation.
    """
    
    def __init__(self):
        """Initialize the MCP generator service."""
        self.doc_processor = DocProcessor()
        self.jina_processor = JinaDocumentProcessor()
        self.llm_workflow = LLMWorkflow()
        
        # Base directory for storing generated templates
        self.templates_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
            "engine", "templates", "generated"
        )
        
        # Create directory if it doesn't exist
        try:
            os.makedirs(self.templates_dir, exist_ok=True)
            logger.info(f"Created templates directory: {self.templates_dir}")
        except Exception as e:
            logger.error(f"Error creating templates directory: {str(e)}")
    
    async def generate_mcp_server(
        self,
        user_id: str,
        request_message: str,
        doc_url: List[str],
        api_credentials: Dict[str, Any],
        existing_template_id: Optional[str] = None,
        existing_server_id: Optional[str] = None,
        chat_session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate an MCP server from API documentation.
        
        Args:
            user_id: User ID
            request_message: User request message
            doc_url: List of URLs to API documentation
            api_credentials: API credentials for authentication
            existing_template_id: Optional existing template ID
            existing_server_id: Optional existing server ID
            chat_session_id: Optional chat session ID to associate with this generation
            
        Returns:
            Dictionary with generation results
        """
        start_time = time.time()
        logger.info(f"[TRACK] Generation started at {start_time}")
        
        try:
            # Process documentation for all URLs
            logger.info(f"[TRACK] Processing documentation from URLs: {doc_url}")
            
            combined_documentation = ""
            combined_sections = {}
            doc_sources = []

            # Process each URL and combine documentation
            for url in doc_url:
            # Use Jina for documentation extraction (returns markdown)
                doc_fetch_start = time.time()
                raw_doc = await self.jina_processor.process_url(url)
                doc_fetch_end = time.time()
                logger.info(f"[TRACK] Documentation fetched from {url} in {doc_fetch_end - doc_fetch_start:.2f}s, size: {len(raw_doc)} chars")
                
                combined_documentation += f"\n\n## Documentation from {url}\n\n{raw_doc}"
                doc_sources.append(url)
                
                # Extract sections from individual URLs
                sections = self._extract_sections_from_markdown(raw_doc)
                for section_title, section_content in sections.items():
                    combined_key = f"{url} - {section_title}"
                    combined_sections[combined_key] = section_content
            
            # For structured data extraction, we'll do a simple conversion from markdown
            documentation = {
                "title": "Combined API Documentation",
                "source_urls": doc_sources,
                "content": combined_documentation,
                "sections": combined_sections
            }
            
            # Initialize workflow state
            logger.info(f"[TRACK] Initializing workflow state with {len(documentation['sections'])} sections from {len(doc_url)} URLs")
            state = AgentState(
                user_id=user_id,
                latest_user_message=request_message,
                messages=[],
                documentation=documentation,
                raw_documentation=combined_documentation,
                implementation_plan="",
                generated_code={},
                api_credentials=api_credentials,
                error=None,
                template_id=existing_template_id,
                server_id=existing_server_id
            )
            
            # Run workflow
            logger.info("[TRACK] Starting LLM workflow")
            workflow_start = time.time()
            result = await self.llm_workflow.process(state)
            workflow_end = time.time()
            logger.info(f"[TRACK] LLM workflow completed in {workflow_end - workflow_start:.2f}s")
            
            # Always try to save files if we have a template_id
            template_id = result.get("template_id", existing_template_id)
            logger.info(f"[TRACK] Using template ID: {template_id}")
            
            # Ensure we have a template_id even if none was provided
            if not template_id:
                template_id = str(uuid.uuid4())
                result["template_id"] = template_id
                logger.info(f"[TRACK] Generated new template ID: {template_id}")
            
            # Extract generated code and raw response
            raw_response = result.get("raw_response", "")
            generated_code = result.get("generated_code", {})
            logger.info(f"[TRACK] Extracted raw_response ({len(raw_response)} chars) and generated_code")
            
            # Make sure we have raw response directly stored
            if not raw_response and isinstance(generated_code, dict) and "files" in generated_code:
                for file in generated_code["files"]:
                    if file.get("name") == "debug_raw_response.txt":
                        raw_response = file.get("content", "")
                        logger.info(f"[TRACK] Found raw_response in debug file ({len(raw_response)} chars)")
                        break
            
            # Check if we have valid JSON code in raw_response
            if raw_response and not generated_code.get("files"):
                try:
                    # Try to parse the raw response as JSON
                    parsed_json = json.loads(raw_response)
                    if isinstance(parsed_json, dict) and "files" in parsed_json:
                        generated_code = parsed_json
                        logger.info("[TRACK] Successfully parsed raw response as JSON with 'files' key")
                except json.JSONDecodeError:
                    # If raw response isn't valid JSON, check if it contains code blocks
                    logger.warning("[TRACK] Raw response isn't valid JSON, looking for code blocks")
                    # Further processing could be done here to extract code blocks
            
            # Ensure template directory exists
            template_dir = os.path.join(self.templates_dir, template_id)
            logger.info(f"[TRACK] Ensuring template directory exists: {template_dir}")
            
            try:
                os.makedirs(template_dir, exist_ok=True)
                logger.info(f"[TRACK] Successfully created/verified template directory: {template_dir}")
            except Exception as e:
                logger.error(f"[TRACK] Error creating template directory: {str(e)}")
            
            # Save the files
            try:
                logger.info(f"[TRACK] Attempting to save files for template ID: {template_id}")
                
                # Wait for save operation with timeout
                try:
                    # If files exist in generated_code, save those
                    if isinstance(generated_code, dict) and "files" in generated_code:
                        logger.info(f"[TRACK] Found {len(generated_code['files'])} files in generated_code")
                        files_save_start = time.time()
                        await asyncio.wait_for(
                            self._save_template_files(template_id, raw_response, generated_code["files"]),
                            timeout=15.0
                        )
                        files_save_end = time.time()
                        logger.info(f"[TRACK] Saved {len(generated_code['files'])} files in {files_save_end - files_save_start:.2f}s")
                    else:
                        # If we don't have structured files, try to extract from raw response
                        logger.info("[TRACK] No structured files found, extracting from raw response")
                        try:
                            # Parse raw response and extract files
                            parse_start = time.time()
                            parsed_files = self._parse_files_from_raw_response(raw_response)
                            parse_end = time.time()
                            logger.info(f"[TRACK] Parsed files from raw response in {parse_end - parse_start:.2f}s")
                            
                            if parsed_files:
                                logger.info(f"[TRACK] Found {len(parsed_files)} files in raw response")
                                files_save_start = time.time()
                                await asyncio.wait_for(
                                    self._save_template_files(template_id, raw_response, parsed_files),
                                    timeout=15.0
                                )
                                files_save_end = time.time()
                                logger.info(f"[TRACK] Saved {len(parsed_files)} files in {files_save_end - files_save_start:.2f}s")
                            else:
                                logger.warning("[TRACK] Couldn't extract files from raw response")
                        except Exception as parse_error:
                            logger.error(f"[TRACK] Error parsing files from raw response: {str(parse_error)}")
                            
                except asyncio.TimeoutError:
                    logger.error("[TRACK] Save operation timed out after 15 seconds")
                    
                # Verify files were saved
                template_dir = os.path.join(self.templates_dir, template_id)
                
                if os.path.exists(template_dir):
                    files = os.listdir(template_dir)
                    logger.info(f"[TRACK] Files in template directory: {files}")
                    
                    # If directory exists but is empty and we have raw_response, write it directly
                    if not files and raw_response:
                        logger.info("[TRACK] Directory exists but no files, writing raw response directly")
                        try:
                            main_file_path = os.path.join(template_dir, "main.py")
                            with open(main_file_path, "w", encoding="utf-8") as f:
                                # If raw_response looks like valid Python code, save it directly
                                if "def " in raw_response and "import " in raw_response:
                                    f.write(raw_response)
                                else:
                                    # Otherwise create a simple file with raw_response as a comment
                                    f.write(f"# Generated MCP Server\n\n'''\nRaw LLM Response:\n{raw_response}\n'''\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP('deepsearch_mcp')\n\n# TODO: Implement tools based on the raw LLM response\n\nif __name__ == '__main__':\n    mcp.run()")
                            logger.info(f"[TRACK] Wrote raw response to {main_file_path}")
                        except Exception as e:
                            logger.error(f"[TRACK] Error writing raw response: {str(e)}")
                else:
                    logger.error(f"[TRACK] Template directory doesn't exist after save operation: {template_dir}")
                
            except Exception as e:
                logger.error(f"[TRACK] Error saving files: {str(e)}")
            
            end_time = time.time()
            logger.info(f"[TRACK] Generation completed in {end_time - start_time:.2f}s")
            
            # Save raw response to chat session if we have a chat_session_id
            if raw_response and user_id:
                try:
                    # If no chat_session_id is provided, create a new one
                    if not chat_session_id:
                        # Create a new chat session
                        session_data = {
                            "user_id": user_id,
                            "title": f"MCP Generation ({', '.join(doc_url)})",
                            "messages": json.dumps([
                                {"role": "user", "content": request_message}
                            ]),
                            "doc_urls": json.dumps(doc_url),
                            "api_key": api_credentials.get("api_key", ""),
                            "request_message": request_message,
                            "has_response": True,
                            "raw_response": raw_response
                        }
                        
                        # Create the chat session
                        chat_session = await chatSessionOperations.createChatSession(session_data)
                        if chat_session:
                            chat_session_id = chat_session.get("id")
                            logger.info(f"[TRACK] Created new chat session with ID: {chat_session_id}")
                    else:
                        # Update the existing chat session with the raw response
                        await chatSessionOperations.saveChatSessionResponse(chat_session_id, raw_response)
                        logger.info(f"[TRACK] Updated chat session {chat_session_id} with raw response")
                except Exception as session_error:
                    logger.error(f"[TRACK] Error saving chat session: {str(session_error)}")
            
            # Return the result directly, which now always has success=True
            # Include chat_session_id in the response if we have one
            if chat_session_id:
                result["chat_session_id"] = chat_session_id
                
            return result
            
        except Exception as e:
            end_time = time.time()
            logger.error(f"[TRACK] Error in generate_mcp_server after {end_time - start_time:.2f}s: {str(e)}")
            # Return a success response with error details
            response = {
                "success": True,
                "template_id": existing_template_id or str(uuid.uuid4()),
                "server_id": existing_server_id,
                "message": f"Processed with error: {str(e)}",
                "error_details": str(e)
            }
            
            # Include chat_session_id in the response if we have one
            if chat_session_id:
                response["chat_session_id"] = chat_session_id
                
            return response
    
    def _extract_sections_from_markdown(self, markdown_content: str) -> Dict[str, str]:
        """
        Extract sections from markdown content.
        
        Args:
            markdown_content: Markdown content
            
        Returns:
            Dictionary of section titles to content
        """
        sections = {}
        current_section = "Introduction"
        current_content = []
        
        for line in markdown_content.split("\n"):
            if line.startswith("# "):
                # Found a top-level heading (h1)
                if current_content:
                    sections[current_section] = "\n".join(current_content)
                current_section = line[2:].strip()
                current_content = []
            elif line.startswith("## "):
                # Found a second-level heading (h2)
                if current_content:
                    sections[current_section] = "\n".join(current_content)
                current_section = line[3:].strip()
                current_content = []
            else:
                current_content.append(line)
        
        # Add the last section
        if current_content:
            sections[current_section] = "\n".join(current_content)
            
        return sections
    
    async def deploy_mcp_server(
        self,
        user_id: str,
        template_id: str,
        server_name: str,
        server_description: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Deploy an MCP server from a template.
        
        Args:
            user_id: ID of the user making the request
            template_id: ID of the template to deploy
            server_name: Name for the deployed server
            server_description: Optional description for the server
            config: Configuration for the server
            
        Returns:
            Result of the deployment process
        """
        try:
            # Get template
            template = await templateOperations.getTemplateById(template_id)
            
            if not template:
                return {
                    "success": False,
                    "message": f"Template not found: {template_id}",
                    "error": "Template not found"
                }
                
            # Create server record
            server_data = {
                "name": server_name,
                "description": server_description or f"MCP Server based on template {template_id}",
                "status": "created",
                "user_id": user_id,
                "template_id": template_id,
                "config": json.dumps(config) if config else "{}"
            }
            
            # Create server
            server = await serverOperations.createServer(server_data)
            
            if not server:
                return {
                    "success": False,
                    "message": "Failed to create server record",
                    "error": "Database error"
                }
                
            # For now, just return success - in a real deployment
            # we would trigger an actual deployment process
            return {
                "success": True,
                "server_id": server.get("id"),
                "message": f"Server {server_name} created successfully",
                "deployment_url": f"http://localhost:8000/servers/{server.get('id')}"
            }
            
        except Exception as e:
            logger.error(f"Error in deploy_mcp_server: {str(e)}")
            return {
                "success": False,
                "message": f"Deployment failed: {str(e)}",
                "error": str(e)
            }
    
    def _parse_files_from_raw_response(self, raw_response: str) -> Dict[str, str]:
        """
        Parse files from raw LLM response.
        
        Args:
            raw_response: Raw LLM response
            
        Returns:
            Dictionary of filenames to file contents
        """
        try:
            # Check if the response is JSON
            try:
                parsed_json = json.loads(raw_response)
                if isinstance(parsed_json, dict) and "files" in parsed_json:
                    return parsed_json["files"]
            except json.JSONDecodeError:
                pass
                
            # Try to extract code blocks using regex
            files = {}
            
            # Pattern for markdown code blocks: ```filename.ext\ncode\n```
            code_blocks = re.findall(r'```(?:python)?\s*(?:([a-zA-Z0-9_\-\.]+))?\n(.*?)```', raw_response, re.DOTALL)
            
            for i, (filename, code) in enumerate(code_blocks):
                # Clean up the code - remove trailing whitespace
                code = code.strip()
                
                # If no filename was provided, try to guess based on content
                if not filename:
                    if "def main" in code or "@mcp.tool" in code:
                        filename = "main.py"
                    elif "BaseModel" in code or "Field(" in code:
                        filename = "models.py"
                    elif "class API" in code or "httpx.AsyncClient" in code:
                        filename = "api.py"
                    elif "class Settings" in code or "BaseSettings" in code:
                        filename = "config.py"
                    elif "mcp" in code and "requirements" in raw_response.lower():
                        filename = "requirements.txt"
                    elif "API_KEY" in code:
                        filename = ".env.example"
                    elif "# " in code and "Usage" in code:
                        filename = "README.md"
                    else:
                        filename = f"file_{i+1}.py"
                
                files[filename] = code
            
            # Look for Python file content without code blocks
            if not files:
                if "def " in raw_response and "import " in raw_response:
                    files["main.py"] = raw_response
            
            return files
        except Exception as e:
            logger.error(f"Error parsing files from raw response: {str(e)}")
            return {}

    def _extract_files_from_text(self, content: str) -> Dict[str, str]:
        """
        Extract files from text content based on delimiters when JSON parsing fails.
        
        Args:
            content: Raw text content that may contain file definitions
            
        Returns:
            Dictionary mapping filenames to content
        """
        files = {}
        
        # Pattern to match file name and content
        # Looks for "name": "filename.ext", followed by "content": "..."
        name_content_pattern = re.compile(r'"name"\s*:\s*"([^"]+)"\s*,\s*"content"\s*:\s*"((?:\\"|[^"])*)"', re.DOTALL)
        
        # Pattern for content that uses triple quotes or multiline format
        multiline_content_pattern = re.compile(r'"name"\s*:\s*"([^"]+)"\s*,\s*"content"\s*:\s*"(.+?)"\s*[,}]', re.DOTALL)
        
        # Try to find all name-content pairs
        matches = name_content_pattern.findall(content)
        
        if not matches:
            # Fall back to the multiline pattern
            matches = multiline_content_pattern.findall(content)
        
        for filename, file_content in matches:
            # Unescape any escaped quotes and newlines
            file_content = file_content.replace('\\"', '"').replace('\\n', '\n')
            files[filename] = file_content
            logger.info(f"Extracted file {filename} using delimiter-based extraction")
        
        # If we couldn't extract with regex patterns, try looking for specific file names
        if not files:
            common_files = ["main.py", "models.py", "api.py", "requirements.txt", ".env.example", "README.md"]
            for filename in common_files:
                # Try to find content between filename and the next filename
                pattern = fr'"name"\s*:\s*"{filename}"\s*,\s*"content"\s*:\s*"(.*?)(?:"name"|$)'
                match = re.search(pattern, content, re.DOTALL)
                if match:
                    file_content = match.group(1).strip()
                    # Remove trailing comma and quotes if present
                    if file_content.endswith('",'):
                        file_content = file_content[:-2]
                    # Unescape content
                    file_content = file_content.replace('\\"', '"').replace('\\n', '\n')
                    files[filename] = file_content
                    logger.info(f"Extracted file {filename} using filename-based extraction")
        
        return files

    async def _save_template_files(self, template_id: str, raw_response: str, generated_code: Dict) -> bool:
        """Save template files from the generated code."""
        try:
            logger.info(f"[TRACK] Attempting to save files for template ID: {template_id}")
            start_time = time.time()
            
            # Ensure template directory exists
            template_dir = os.path.join(self.templates_dir, template_id)
            os.makedirs(template_dir, exist_ok=True)
            
            # Save raw response to a file for debugging
            raw_response_path = os.path.join(template_dir, "raw_response.txt")
            with open(raw_response_path, "w", encoding="utf-8") as f:
                if raw_response:
                    f.write(raw_response)
                else:
                    f.write("No raw response available")
            logger.info(f"[TRACK] Saved raw response ({len(raw_response) if raw_response else 0} chars) to {raw_response_path}")
            
            # Check if we have structured files or need to extract from raw response
            if generated_code and "files" in generated_code:
                # We have structured files
                files = generated_code["files"]
                logger.info(f"[TRACK] Found {len(files)} structured files")
                
                # Save each file
                for file_data in files:
                    file_name = file_data.get("name", "")
                    file_content = file_data.get("content", "")
                    
                    if file_name and file_content:
                        # Create proper path and ensure directories exist
                        file_path = os.path.join(template_dir, file_name)
                        os.makedirs(os.path.dirname(file_path), exist_ok=True)
                        
                        # Write file content
                        with open(file_path, "w") as f:
                            f.write(file_content)
                            
                        logger.info(f"[TRACK] Saved file: {file_name}")
                    else:
                        logger.warning(f"[TRACK] Skipping invalid file data: {file_data}")
            else:
                # No structured files, try to extract from raw response
                logger.info(f"[TRACK] No structured files found, extracting from raw response")
                files = self._extract_files_from_text(raw_response)
                extract_time = time.time()
                logger.info(f"[TRACK] Parsed files from raw response in {extract_time - start_time:.2f}s")
                
                if files:
                    for file_name, file_content in files.items():
                        # Create proper path and ensure directories exist
                        file_path = os.path.join(template_dir, file_name)
                        os.makedirs(os.path.dirname(file_path), exist_ok=True)
                        
                        # Write file content
                        with open(file_path, "w") as f:
                            f.write(file_content)
                            
                        logger.info(f"[TRACK] Saved extracted file: {file_name}")
                else:
                    logger.warning(f"[TRACK] Couldn't extract files from raw response")
                
            return True
            
        except Exception as e:
            logger.error(f"Error in _save_template_files: {str(e)}")
            return False
    
    async def _write_file_async(self, filepath: str, content: str) -> None:
        """
        Write file content asynchronously.
        
        Args:
            filepath: Path to the file
            content: Content to write
        """
        try:
            # Log file writing attempt
            logger.info(f"Attempting to write file: {filepath}")
            
            # Create directory if it doesn't exist
            directory = os.path.dirname(filepath)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
                
            # Use aiofiles for non-blocking I/O if available
            try:
                import aiofiles
                async with aiofiles.open(filepath, "w", encoding="utf-8") as f:
                    await f.write(content)
                logger.info(f"Successfully wrote file: {filepath}")
            except ImportError:
                # Fallback to running blocking I/O in a thread pool
                logger.warning("aiofiles not available, falling back to blocking I/O")
                loop = asyncio.get_running_loop()
                await loop.run_in_executor(None, self._write_file_sync, filepath, content)
                logger.info(f"Successfully wrote file (sync): {filepath}")
        except Exception as e:
            logger.error(f"Error writing file {filepath}: {str(e)}")
    
    def _write_file_sync(self, filepath: str, content: str) -> None:
        """
        Write file content synchronously (used as a fallback).
        
        Args:
            filepath: Path to the file
            content: Content to write
        """
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)

            f.write(content) 