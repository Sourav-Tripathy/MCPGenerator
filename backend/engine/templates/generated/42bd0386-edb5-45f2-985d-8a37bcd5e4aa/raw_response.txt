{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import Dict, Any, Optional, List, Union, Literal\n\n\nclass MessageContentPart(BaseModel):\n    \"\"\"Represents a part of the content in a message, can be text or image/document URI.\"\"\"\n    type: Literal[\"text\", \"image_url\", \"document_url\"] = Field(..., description=\"Type of content part, e.g., 'text', 'image_url', 'document_url'\")\n    text: Optional[str] = Field(None, description=\"The text content.\")\n    image_url: Optional[Dict[str, str]] = Field(None, description=\"Dictionary containing 'url' key with data URI for image (webp, png, jpeg).\")\n    document_url: Optional[Dict[str, str]] = Field(None, description=\"Dictionary containing 'url' key with data URI for document (txt, pdf, max 10MB).\")\n\n    class Config:\n        extra = 'forbid' # Ensure no unexpected fields\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal[\"user\", \"assistant\"] = Field(..., description=\"The role of the message author ('user' or 'assistant').\")\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"The content of the message. Can be a simple string or a list of content parts for multimodal input.\")\n\n    class Config:\n        extra = 'forbid'\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the DeepSearch chat_completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history. Must include at least one user message. Supports text, images (webp, png, jpeg as data URIs), and files (txt, pdf as data URIs up to 10MB).\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(True, description=\"Whether to stream back partial progress. If true, delivers events as Server-Sent Events. Strongly recommended to be true to avoid timeouts for long-running requests.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains effort on reasoning. Supported values: 'low', 'medium', 'high'. Default is 'medium'. Lower effort can lead to faster responses but potentially less depth.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides 'reasoning_effort'. Larger budgets can improve quality for complex queries.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Allows trying different reasoning approaches. Overrides 'reasoning_effort'.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces the model to perform thinking/search steps even for seemingly trivial queries. Useful when certainty of needing deep search is high.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk, sorted by relevance.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"Enables structured output, ensuring the final answer matches the provided JSON schema.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"A list of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"A list of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"A list of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        extra = 'forbid'\n\n# Placeholder models for response types - structure might vary based on actual API output\nclass DeepSearchUsage(BaseModel):\n    prompt_tokens: Optional[int] = None\n    completion_tokens: Optional[int] = None\n    total_tokens: Optional[int] = None\n\nclass DeepSearchChoiceDelta(BaseModel):\n    role: Optional[Literal['assistant']] = None\n    content: Optional[str] = None\n\nclass DeepSearchChoice(BaseModel):\n    index: int\n    delta: Optional[DeepSearchChoiceDelta] = None # For streaming\n    message: Optional[Message] = None # For non-streaming\n    finish_reason: Optional[str] = None\n    urls: Optional[List[str]] = None\n\nclass DeepSearchResponseChunk(BaseModel):\n    \"\"\"Represents a chunk of the response when streaming.\"\"\"\n    id: str\n    object: str = \"chat.completion.chunk\"\n    created: int\n    model: str\n    choices: List[DeepSearchChoice]\n    usage: Optional[DeepSearchUsage] = None # Usually present in the final chunk\n\nclass DeepSearchResponse(BaseModel):\n    \"\"\"Represents the full response when not streaming.\"\"\"\n    id: str\n    object: str = \"chat.completion\"\n    created: int\n    model: str\n    choices: List[DeepSearchChoice]\n    usage: DeepSearchUsage\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport logging\nimport os\nimport json\nfrom typing import AsyncGenerator, Union, Dict, Any\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n\nfrom models import DeepSearchChatInput, DeepSearchResponse, DeepSearchResponseChunk\n\nlogger = logging.getLogger(__name__)\n\n# Define specific exceptions for clarity\nclass DeepSearchAPIError(Exception):\n    \"\"\"Custom exception for DeepSearch API errors.\"\"\"\n    def __init__(self, status_code: int, detail: Any):\n        self.status_code = status_code\n        self.detail = detail\n        super().__init__(f\"DeepSearch API Error {status_code}: {detail}\")\n\nclass DeepSearchAPIClient:\n    \"\"\"Client for interacting with the Jina DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: str = \"https://deepsearch.jina.ai\", timeout: float = 180.0):\n        \"\"\"\n        Initializes the DeepSearch API client.\n\n        Args:\n            api_key: The Jina API key. Reads from JINA_API_KEY env var if not provided.\n            base_url: The base URL for the DeepSearch API.\n            timeout: Default timeout for API requests in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        self.base_url = base_url\n        self.timeout = timeout\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n        }\n        if self.api_key:\n            self.headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n        else:\n            logger.warning(\"JINA_API_KEY not found. Using DeepSearch without an API key, which has lower rate limits (2 RPM).\")\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=self.timeout\n        )\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        retry=retry_if_exception_type((httpx.TimeoutException, httpx.NetworkError, httpx.HTTPStatusError)),\n        reraise=True # Reraise the exception after retries are exhausted\n    )\n    async def _make_request(\n        self,\n        method: str,\n        endpoint: str,\n        payload: Optional[Dict[str, Any]] = None,\n        stream: bool = False\n    ) -> Union[httpx.Response, AsyncGenerator[str, None]]:\n        \"\"\"Makes an HTTP request to the DeepSearch API with retry logic.\"\"\"\n        try:\n            if stream:\n                # For streaming, return the stream directly\n                req = self.client.build_request(method, endpoint, json=payload)\n                response_stream = await self.client.send(req, stream=True)\n                response_stream.raise_for_status() # Raise HTTP errors immediately\n                return response_stream\n            else:\n                response = await self.client.request(method, endpoint, json=payload)\n                response.raise_for_status() # Raise HTTP errors (4xx, 5xx)\n                return response\n\n        except httpx.HTTPStatusError as e:\n            status_code = e.response.status_code\n            try:\n                detail = e.response.json()\n            except json.JSONDecodeError:\n                detail = e.response.text\n            logger.error(f\"HTTP error {status_code} calling {e.request.url}: {detail}\")\n            raise DeepSearchAPIError(status_code=status_code, detail=detail) from e\n        except httpx.TimeoutException as e:\n            logger.error(f\"Timeout error calling {e.request.url}: {e}\")\n            raise # Reraised for retry logic\n        except httpx.NetworkError as e:\n            logger.error(f\"Network error calling {e.request.url}: {e}\")\n            raise # Reraised for retry logic\n        except Exception as e:\n            logger.error(f\"Unexpected error during API request to {endpoint}: {e}\")\n            raise DeepSearchAPIError(status_code=500, detail=str(e)) from e # Treat unexpected errors as internal\n\n    async def chat_completion(\n        self,\n        params: DeepSearchChatInput\n    ) -> Union[DeepSearchResponse, AsyncGenerator[DeepSearchResponseChunk, None]]:\n        \"\"\"\n        Performs a deep search chat completion.\n\n        Args:\n            params: Input parameters conforming to DeepSearchChatInput model.\n\n        Returns:\n            If stream=False, returns a DeepSearchResponse object.\n            If stream=True, returns an async generator yielding DeepSearchResponseChunk objects.\n\n        Raises:\n            DeepSearchAPIError: If the API returns an error.\n        \"\"\"\n        endpoint = \"/v1/chat/completions\"\n        # Use exclude_unset=True to only send parameters that were explicitly set\n        payload = params.dict(exclude_unset=True)\n\n        logger.info(f\"Sending request to DeepSearch API: stream={params.stream}\")\n        # logger.debug(f\"Payload: {payload}\") # Be careful logging payload with potentially sensitive data\n\n        if params.stream:\n            response_stream = await self._make_request(\"POST\", endpoint, payload=payload, stream=True)\n            return self._process_stream(response_stream)\n        else:\n            response = await self._make_request(\"POST\", endpoint, payload=payload, stream=False)\n            try:\n                response_data = response.json()\n                logger.info(\"Received non-streaming response from DeepSearch API.\")\n                return DeepSearchResponse(**response_data)\n            except json.JSONDecodeError as e:\n                logger.error(f\"Failed to decode JSON response: {response.text}\")\n                raise DeepSearchAPIError(status_code=500, detail=f\"Invalid JSON response: {e}\") from e\n            except Exception as e:\n                logger.error(f\"Error parsing non-streaming response: {e}\")\n                raise DeepSearchAPIError(status_code=500, detail=f\"Error parsing response: {e}\") from e\n\n    async def _process_stream(\n        self, response_stream: httpx.Response\n    ) -> AsyncGenerator[DeepSearchResponseChunk, None]:\n        \"\"\"Processes the Server-Sent Events stream from the API.\"\"\"\n        buffer = \"\"\n        async with response_stream:\n            async for line_bytes in response_stream.aiter_lines():\n                line = line_bytes.strip()\n                # logger.debug(f\"Received stream line: {line}\")\n                if not line:\n                    # Empty line signifies end of an event\n                    if buffer.startswith(\"data:\"):\n                        data_str = buffer[len(\"data:\"):].strip()\n                        if data_str == \"[DONE]\":\n                            logger.info(\"Stream finished with [DONE] message.\")\n                            buffer = \"\"\n                            break # End of stream\n                        try:\n                            data = json.loads(data_str)\n                            chunk = DeepSearchResponseChunk(**data)\n                            yield chunk\n                        except json.JSONDecodeError:\n                            logger.error(f\"Failed to decode JSON chunk: {data_str}\")\n                        except Exception as e:\n                            logger.error(f\"Error processing stream chunk '{data_str}': {e}\")\n                    buffer = \"\"\n                else:\n                    buffer += line + \"\\n\" # Rebuild potential multi-line data\n\n            # Check if there's anything left in the buffer after loop finishes (e.g., if stream ends without empty line)\n            if buffer.startswith(\"data:\"):\n                data_str = buffer[len(\"data:\"):].strip()\n                if data_str != \"[DONE]\":\n                    try:\n                        data = json.loads(data_str)\n                        chunk = DeepSearchResponseChunk(**data)\n                        yield chunk\n                    except json.JSONDecodeError:\n                        logger.error(f\"Failed to decode final JSON chunk: {data_str}\")\n                    except Exception as e:\n                        logger.error(f\"Error processing final stream chunk '{data_str}': {e}\")\n\n        logger.info(\"Finished processing DeepSearch stream.\")\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n        logger.info(\"DeepSearch API client closed.\")\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, ToolContext\nfrom typing import Union, AsyncGenerator\nimport logging\nimport os\nfrom dotenv import load_dotenv\nimport asyncio\n\nfrom models import DeepSearchChatInput, DeepSearchResponse, DeepSearchResponseChunk\nfrom api import DeepSearchAPIClient, DeepSearchAPIError\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP interface for the Jina DeepSearch API. Provides web searching, content reading, and reasoning capabilities.\"\n)\n\n# Initialize API Client\n# The API key is read from the JINA_API_KEY environment variable within the client\napi_client = DeepSearchAPIClient()\n\n@mcp.tool()\nasync def chat_completion(\n    ctx: ToolContext,\n    params: DeepSearchChatInput\n) -> Union[DeepSearchResponse, AsyncGenerator[DeepSearchResponseChunk, None]]:\n    \"\"\"\n    Performs a deep search and reasoning process to answer a user query based on conversational context.\n\n    It iteratively searches the web, reads relevant content, and reasons until it finds an accurate answer\n    or reaches resource limits. Supports text, image, and document inputs within messages.\n\n    Args:\n        ctx: The ToolContext provided by FastMCP.\n        params: Input parameters including messages, model, streaming options, etc.\n\n    Returns:\n        If stream=False, returns a single DeepSearchResponse object.\n        If stream=True, returns an async generator yielding DeepSearchResponseChunk objects.\n\n    Raises:\n        DeepSearchAPIError: If the API call fails after retries.\n        Exception: For other unexpected errors during execution.\n    \"\"\"\n    logger.info(f\"Received chat_completion request (stream={params.stream})\")\n    try:\n        # The API client handles both streaming and non-streaming responses internally\n        result = await api_client.chat_completion(params)\n\n        if isinstance(result, AsyncGenerator):\n            logger.info(\"Streaming response back to client.\")\n            # If it's a generator, return it directly for FastMCP to handle streaming\n            return result\n        else:\n            logger.info(\"Returning non-streaming response to client.\")\n            # If it's a single response object, return it\n            return result\n\n    except DeepSearchAPIError as e:\n        logger.error(f\"DeepSearch API error in chat_completion: {e.status_code} - {e.detail}\")\n        # Re-raise the specific API error to potentially inform the client\n        # FastMCP might map this to a standard MCP error response\n        raise\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred in chat_completion tool\")\n        # Raise a generic exception for FastMCP to handle\n        raise RuntimeError(f\"An unexpected error occurred: {e}\") from e\n\n# Graceful shutdown\n@mcp.on_shutdown\nasync def shutdown():\n    logger.info(\"Shutting down DeepSearch MCP server...\")\n    await api_client.close()\n    logger.info(\"DeepSearch API client closed.\")\n\nif __name__ == \"__main__\":\n    # Example of how to run (though typically you'd use uvicorn or similar)\n    # mcp.run() # This uses a basic development server\n\n    # For production, use an ASGI server like uvicorn:\n    # uvicorn main:mcp.app --host 0.0.0.0 --port 8000\n    logger.info(\"Starting DeepSearch MCP server.\")\n    logger.info(\"Run with: uvicorn main:mcp.app --host 0.0.0.0 --port <your_port>\")\n    # To run directly for simple testing:\n    # import uvicorn\n    # uvicorn.run(mcp.app, host=\"127.0.0.1\", port=8000)\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\npydantic>=1.8,<2.0\n# Note: If using Pydantic v2+, ensure compatibility or adjust models as needed.\nhttpx>=0.23.0\ntenacity>=8.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.15.0\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina DeepSearch API Key\n# Get your key from https://jina.ai/cloud/\n# Providing a key increases rate limits (e.g., from 2 RPM to 10 RPM or higher for premium keys).\n# If left blank, the service will run with the lowest rate limit.\nJINA_API_KEY=your_jina_api_key_here\n"
    },
    {
      "name": "README.md",
      "content": "# DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for the [Jina DeepSearch API](https://jina.ai/deepsearch/). It allows you to interact with DeepSearch using a standardized MCP interface, making it easy to integrate its advanced web search, content reading, and reasoning capabilities into applications compatible with MCP.\n\nThis server is built using [FastMCP](https://github.com/your-repo/fastmcp). <!-- Replace with actual FastMCP link if available -->\n\n## Features\n\n*   Provides an MCP interface for Jina DeepSearch's chat completions endpoint.\n*   Supports text, image (data URI), and document (data URI) inputs.\n*   Handles both streaming (Server-Sent Events) and non-streaming responses.\n*   Configurable reasoning effort, token budgets, and domain filtering.\n*   Built-in retry logic for transient network or server errors.\n*   Authentication via Jina API Key.\n\n## Prerequisites\n\n*   Python 3.8+\n*   A Jina API Key (optional but recommended for higher rate limits). Get one from [Jina AI Cloud](https://jina.ai/cloud/).\n\n## Setup\n\n1.  **Clone the repository (or create the files):**\n    ```bash\n    # If cloned from a repo:\n    # git clone <repository-url>\n    # cd <repository-directory>\n\n    # If creating files manually, ensure you have:\n    # main.py, models.py, api.py, requirements.txt, .env.example\n    ```\n\n2.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n3.  **Configure Environment Variables:**\n    Create a `.env` file by copying the example:\n    ```bash\n    cp .env.example .env\n    ```\n    Edit the `.env` file and add your Jina API key:\n    ```\n    JINA_API_KEY=your_jina_api_key_here\n    ```\n    If you don't provide an API key, the service will still work but with significantly lower rate limits (2 requests per minute).\n\n## Running the Server\n\nUse an ASGI server like Uvicorn to run the application:\n\n```bash\nuvicorn main:mcp.app --host 0.0.0.0 --port 8000\n```\n\nReplace `8000` with your desired port.\n\nThe MCP server will now be running and accessible at `http://localhost:8000` (or the host/port you configured).\n\n## Available Tools\n\nThe server exposes the following MCP tool:\n\n### `chat_completion`\n\nPerforms a deep search and reasoning process based on conversation history.\n\n**Input Parameters (`DeepSearchChatInput` model):**\n\n*   `messages` (List[`Message`], required): Conversation history. See `Message` structure below.\n*   `model` (str, optional, default: `jina-deepsearch-v1`): The model ID to use.\n*   `stream` (bool, optional, default: `True`): Whether to stream results using Server-Sent Events. Highly recommended.\n*   `reasoning_effort` (str, optional, default: `medium`): Controls reasoning depth ('low', 'medium', 'high').\n*   `budget_tokens` (int, optional): Max tokens for the process (overrides `reasoning_effort`).\n*   `max_attempts` (int, optional): Max reasoning retries (overrides `reasoning_effort`).\n*   `no_direct_answer` (bool, optional, default: `False`): Force search/reasoning steps.\n*   `max_returned_urls` (int, optional): Max URLs in the final answer.\n*   `structured_output` (dict, optional): JSON schema for structured output.\n*   `good_domains` (List[str], optional): Prioritized domains.\n*   `bad_domains` (List[str], optional): Excluded domains.\n*   `only_domains` (List[str], optional): Exclusively included domains.\n\n**`Message` Structure:**\n\n*   `role` (str, required): 'user' or 'assistant'.\n*   `content` (Union[str, List[`MessageContentPart`]], required): Message content.\n\n**`MessageContentPart` Structure (for multimodal content):**\n\n*   `type` (str, required): 'text', 'image_url', or 'document_url'.\n*   `text` (str, optional): Text content.\n*   `image_url` (dict, optional): `{\"url\": \"data:image/...;base64,...\"}` (webp, png, jpeg).\n*   `document_url` (dict, optional): `{\"url\": \"data:application/pdf;base64,...\"}` or `{\"url\": \"data:text/plain;base64,...\"}` (txt, pdf, max 10MB).\n\n**Returns:**\n\n*   If `stream=True`: An async generator yielding `DeepSearchResponseChunk` objects.\n*   If `stream=False`: A single `DeepSearchResponse` object.\n\n## Authentication\n\nThe server uses Bearer token authentication, expecting a Jina API key provided in the `JINA_API_KEY` environment variable. This key is automatically included in the `Authorization` header for requests to the DeepSearch API.\n\n## Rate Limits\n\nThe Jina DeepSearch API has rate limits based on your API key:\n\n*   **No Key:** 2 requests per minute (RPM)\n*   **Standard Key:** 10 RPM\n*   **Premium Key:** 100 RPM\n\nThe MCP server itself doesn't enforce these limits but relies on the underlying API. You may receive `429 Too Many Requests` errors if you exceed the limit associated with your key.\n\n## Error Handling\n\nThe server attempts to handle common API errors:\n\n*   **HTTP Errors (4xx, 5xx):** Logged and potentially raised as `DeepSearchAPIError`.\n*   **Timeouts/Network Errors:** Retried automatically up to 3 times with exponential backoff.\n*   **Invalid Responses:** Logged, and may result in errors.\n\n## Example Usage (using `mcp client`)\n\nAssuming the server is running on `http://localhost:8000`:\n\n**Non-streaming:**\n\n```bash\nmcp client call http://localhost:8000 chat_completion \\\n  params='{\n    \"messages\": [{\"role\": \"user\", \"content\": \"What were the main outcomes of the COP28 conference?\"}],\n    \"stream\": false\n  }'\n```\n\n**Streaming:**\n\n```bash\nmcp client call http://localhost:8000 chat_completion \\\n  params='{\n    \"messages\": [{\"role\": \"user\", \"content\": \"Explain the theory of relativity simply.\"}],\n    \"stream\": true\n  }'\n```\n\n*(Note: `mcp client` might need specific flags or handling for streaming output depending on its implementation.)*\n"
    }
  ]
}