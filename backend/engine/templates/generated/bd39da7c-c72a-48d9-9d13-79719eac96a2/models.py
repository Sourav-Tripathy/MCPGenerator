from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union, Literal

# --- Type Definitions from Plan ---

class Message(BaseModel):
    """Represents a single message in the conversation."""
    role: Literal['user', 'assistant', 'system'] = Field(..., description="The role of the message author ('user', 'assistant', or 'system').")
    content: Union[str, List[Dict[str, Any]]] = Field(..., description="The content of the message. Can be plain text or a list for multimodal inputs (e.g., text and image data URIs).")

class UrlCitation(BaseModel):
    """Details of a URL citation provided in the response."""
    title: Optional[str] = Field(None, description="The title of the cited web page.")
    exactQuote: Optional[str] = Field(None, description="The exact quote from the source used in the response.")
    url: str = Field(..., description="The URL of the citation source.")
    dateTime: Optional[str] = Field(None, description="Timestamp associated with the citation.")

# --- Input Model for chat_completion Tool ---

class DeepSearchChatParams(BaseModel):
    """Input parameters for the DeepSearch chat completion request."""
    messages: List[Message] = Field(..., description="A list of messages comprising the conversation history. Can include text, images (webp, png, jpeg as data URI), or files (txt, pdf as data URI up to 10MB).")
    model: str = Field("jina-deepsearch-v1", description="ID of the model to use.")
    stream: bool = Field(True, description="Whether to deliver events as Server-Sent Events. Strongly recommended to be true to avoid timeouts, as DeepSearch requests can be long.")
    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field("medium", description="Constrains reasoning effort. Supported values: 'low', 'medium', 'high'. Affects response time and token usage.")
    budget_tokens: Optional[int] = Field(None, description="Maximum number of tokens allowed for the DeepSearch process. Overrides reasoning_effort.")
    max_attempts: Optional[int] = Field(None, description="Maximum number of retries for solving the problem. Allows different reasoning approaches. Overrides reasoning_effort.")
    no_direct_answer: Optional[bool] = Field(False, description="Forces further thinking/search steps even if the query seems trivial.")
    max_returned_urls: Optional[int] = Field(None, description="Maximum number of URLs to include in the final answer, sorted by relevance.")
    structured_output: Optional[Dict[str, Any]] = Field(None, description="JSON schema to ensure the final answer matches the structure.")
    good_domains: Optional[List[str]] = Field(None, description="List of domains to prioritize for content retrieval.")
    bad_domains: Optional[List[str]] = Field(None, description="List of domains to strictly exclude from content retrieval.")
    only_domains: Optional[List[str]] = Field(None, description="List of domains to exclusively include in content retrieval.")

    class Config:
        # Ensure that default values are included when exporting
        # and None values are excluded if not explicitly set.
        # Use model_dump(exclude_none=True) for API payload.
        pass

# --- Output Models for chat_completion Tool (Based on OpenAI Schema & Plan) ---

class DeepSearchUsage(BaseModel):
    """Token usage statistics for the DeepSearch request."""
    prompt_tokens: Optional[int] = Field(None, description="Number of tokens in the prompt.")
    completion_tokens: Optional[int] = Field(None, description="Number of tokens in the generated completion.")
    total_tokens: Optional[int] = Field(None, description="Total number of tokens used in the request.")

class DeepSearchChatResponseChoiceMessage(BaseModel):
    """The message generated by the model in a non-streaming response."""
    role: Literal['assistant'] = Field(..., description="The role of the author of this message.")
    content: Optional[str] = Field(None, description="The contents of the message.")

class DeepSearchChatResponseChoice(BaseModel):
    """A single choice provided in the chat completion response."""
    index: int = Field(..., description="The index of the choice in the list of choices.")
    message: DeepSearchChatResponseChoiceMessage = Field(..., description="A chat completion message generated by the model.")
    finish_reason: Optional[str] = Field(None, description="The reason the model stopped generating tokens.")

class DeepSearchChatResponse(BaseModel):
    """The full response object for a non-streaming chat completion request."""
    id: str = Field(..., description="A unique identifier for the chat completion.")
    object: Literal['chat.completion'] = Field(..., description="The object type, which is always 'chat.completion'.")
    created: int = Field(..., description="The Unix timestamp (in seconds) of when the chat completion was created.")
    model: str = Field(..., description="The model used for the chat completion.")
    choices: List[DeepSearchChatResponseChoice] = Field(..., description="A list of chat completion choices. Can be more than one if n > 1 was requested.")
    usage: Optional[DeepSearchUsage] = Field(None, description="Usage statistics for the completion request.")
    url_citations: Optional[List[UrlCitation]] = Field(None, description="List of URL citations considered or used during the generation.") # Added based on plan

# --- Output Models for Streaming Response ---

class DeepSearchChatStreamChoiceDelta(BaseModel):
    """The delta content for a streaming response chunk."""
    role: Optional[Literal['assistant']] = Field(None, description="The role of the author of this message.")
    content: Optional[str] = Field(None, description="The contents of the chunk message.")

class DeepSearchChatStreamChoice(BaseModel):
    """A single choice within a streaming response chunk."""
    index: int = Field(..., description="The index of the choice in the list of choices.")
    delta: DeepSearchChatStreamChoiceDelta = Field(..., description="A chat completion delta generated by the model.")
    finish_reason: Optional[str] = Field(None, description="The reason the model stopped generating tokens. Sent in the final chunk.")

class DeepSearchChatStreamResponse(BaseModel):
    """A single chunk received during a streaming chat completion request."""
    id: str = Field(..., description="A unique identifier for the chat completion. Same across chunks.")
    object: Literal['chat.completion.chunk'] = Field(..., description="The object type, which is always 'chat.completion.chunk'.")
    created: int = Field(..., description="The Unix timestamp (in seconds) of when the chunk was created.")
    model: str = Field(..., description="The model used for the chat completion.")
    choices: List[DeepSearchChatStreamChoice] = Field(..., description="A list of chat completion choices for this chunk.")
    usage: Optional[DeepSearchUsage] = Field(None, description="Usage statistics. May be sent in the final chunk.")
    url_citations: Optional[List[UrlCitation]] = Field(None, description="List of URL citations. May be sent in intermediate or final chunks.") # Added based on plan
