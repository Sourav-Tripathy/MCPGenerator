{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Union, Dict, Any, Literal\n\n# --- Message Content Parts ---\n\nclass TextMessageContentPart(BaseModel):\n    \"\"\"Text content part.\"\"\"\n    type: Literal['text'] = Field(..., description=\"Type identifier.\")\n    text: str = Field(..., description=\"The text content.\")\n\nclass ImageUrl(BaseModel):\n    \"\"\"URL structure for image content.\"\"\"\n    url: str = Field(..., description=\"Data URI for the image (e.g., 'data:image/jpeg;base64,...').\")\n\nclass ImageMessageContentPart(BaseModel):\n    \"\"\"Image content part using data URI.\"\"\"\n    type: Literal['image_url'] = Field(..., description=\"Type identifier.\")\n    image_url: ImageUrl = Field(..., description=\"The image URL object containing the data URI.\")\n\nclass FileUrl(BaseModel):\n    \"\"\"URL structure for file content.\"\"\"\n    url: str = Field(..., description=\"Data URI for the file (e.g., 'data:text/plain;base64,...', 'data:application/pdf;base64,...').\")\n\nclass FileMessageContentPart(BaseModel):\n    \"\"\"File content part using data URI.\"\"\"\n    type: Literal['file_url'] = Field(..., description=\"Type identifier.\")\n    file_url: FileUrl = Field(..., description=\"The file URL object containing the data URI.\")\n\nMessageContentPart = Union[TextMessageContentPart, ImageMessageContentPart, FileMessageContentPart]\n\n# --- Message --- \n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant'] = Field(..., description=\"The role of the author ('user' or 'assistant').\")\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"The content of the message. Can be a simple string or a list of content parts for multi-modal input.\")\n\n# --- Input Model --- \n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the Jina DeepSearch chat completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use. Currently only 'jina-deepsearch-v1' is available.\")\n    stream: bool = Field(True, description=\"Whether to stream back partial progress. Recommended to keep true.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constraint on reasoning effort ('low', 'medium', 'high').\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the entire DeepSearch process.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem.\")\n    no_direct_answer: bool = Field(False, description=\"Force the model to perform search/thinking steps.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema to ensure the final answer conforms to the specified structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        # Ensure default values are excluded if not provided\n        exclude_defaults = True \n\n# --- Response Models (Based on OpenAI Schema) ---\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation within the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"URL of the source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation associated with the response content.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass Delta(BaseModel):\n    \"\"\"The delta content for a streaming chunk.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None, description=\"Role of the author (usually 'assistant').\")\n    content: Optional[str] = Field(None, description=\"The content delta. Can contain text or XML tags like <think> for reasoning steps.\")\n    type: Optional[str] = Field(None, description=\"Type of content (e.g., 'text').\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations associated with this delta.\")\n\nclass LogProbs(BaseModel):\n    \"\"\"Log probability information (typically null for DeepSearch).\"\"\"\n    pass # Usually null or empty for this API\n\nclass ChoiceChunk(BaseModel):\n    \"\"\"A choice within a streaming response chunk.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    delta: Delta = Field(..., description=\"The content delta.\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the stream finished (e.g., 'stop', 'length').\")\n    logprobs: Optional[LogProbs] = Field(None, description=\"Log probability information.\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics.\"\"\"\n    prompt_tokens: int = Field(..., description=\"Tokens in the prompt.\")\n    completion_tokens: int = Field(..., description=\"Tokens in the completion.\")\n    total_tokens: int = Field(..., description=\"Total tokens used.\")\n\nclass DeepSearchChatResponseChunk(BaseModel):\n    \"\"\"Schema for a chunk in a streaming chat completion response.\"\"\"\n    id: str = Field(..., description=\"Unique identifier for the chunk.\")\n    object: str = Field(..., description=\"Object type, typically 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"Unix timestamp of when the chunk was created.\")\n    model: str = Field(..., description=\"Model used for the completion.\")\n    choices: List[ChoiceChunk] = Field(..., description=\"List of choices in the chunk.\")\n    usage: Optional[Usage] = Field(None, description=\"Token usage stats (usually only in the final chunk).\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint.\")\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of visited URLs (usually only in the final chunk).\")\n\nclass Choice(BaseModel):\n    \"\"\"A choice in a non-streaming response.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    message: Message = Field(..., description=\"The assistant's response message.\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the completion finished.\")\n    logprobs: Optional[LogProbs] = Field(None, description=\"Log probability information.\")\n\nclass DeepSearchChatResponse(BaseModel):\n    \"\"\"Schema for a non-streaming chat completion response.\"\"\"\n    id: str = Field(..., description=\"Unique identifier for the response.\")\n    object: str = Field(..., description=\"Object type, typically 'chat.completion'.\")\n    created: int = Field(..., description=\"Unix timestamp of when the response was created.\")\n    model: str = Field(..., description=\"Model used for the completion.\")\n    choices: List[Choice] = Field(..., description=\"List of completion choices.\")\n    usage: Optional[Usage] = Field(None, description=\"Token usage statistics.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint.\")\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process.\")\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import AsyncIterator, Union, Optional\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatInput, DeepSearchChatResponse, DeepSearchChatResponseChunk\n\nlogger = logging.getLogger(__name__)\n\nclass JinaDeepSearchClient:\n    \"\"\"Asynchronous client for the Jina AI DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://deepsearch.jina.ai\"\n    API_ENDPOINT = \"/v1/chat/completions\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = 180.0):\n        \"\"\"\n        Initializes the JinaDeepSearchClient.\n\n        Args:\n            api_key: The Jina API key. Reads from JINA_API_KEY env var if not provided.\n            base_url: The base URL for the Jina API. Defaults to https://deepsearch.jina.ai.\n            timeout: Default timeout for API requests in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        self.base_url = base_url or self.DEFAULT_BASE_URL\n        self.timeout = timeout\n\n        if not self.api_key:\n            logger.warning(\"JINA_API_KEY not found in environment variables. Rate limits will be lower (2 RPM).\")\n            self.headers = {\"Content-Type\": \"application/json\"}\n        else:\n            self.headers = {\n                \"Content-Type\": \"application/json\",\n                \"Authorization\": f\"Bearer {self.api_key}\"\n            }\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=self.timeout\n        )\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n\n    async def chat_completion(self,\n                              input_data: DeepSearchChatInput\n                              ) -> Union[DeepSearchChatResponse, AsyncIterator[DeepSearchChatResponseChunk]]:\n        \"\"\"\n        Performs a deep search chat completion.\n\n        Args:\n            input_data: The input data model containing messages and parameters.\n\n        Returns:\n            If stream=False, returns a DeepSearchChatResponse object.\n            If stream=True, returns an async iterator yielding DeepSearchChatResponseChunk objects.\n\n        Raises:\n            httpx.HTTPStatusError: If the API returns an error status code (4xx or 5xx).\n            httpx.RequestError: For network-related errors (timeout, connection error).\n            ValidationError: If the API response doesn't match the expected Pydantic model.\n            Exception: For other unexpected errors.\n        \"\"\"\n        try:\n            # Use exclude_unset=True to avoid sending default values unless explicitly set\n            payload = input_data.model_dump(exclude_unset=True, by_alias=True)\n            logger.info(f\"Sending request to {self.base_url}{self.API_ENDPOINT} with stream={input_data.stream}\")\n            # logger.debug(f\"Request payload: {payload}\") # Be cautious logging full payload if sensitive\n\n            if input_data.stream:\n                return self._stream_chat_completion(payload)\n            else:\n                return await self._non_stream_chat_completion(payload)\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n            # You might want to raise a custom exception here or re-raise\n            raise\n        except httpx.RequestError as e:\n            logger.error(f\"Network error occurred: {e}\")\n            raise\n        except ValidationError as e:\n            logger.error(f\"API response validation error: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n            raise\n\n    async def _non_stream_chat_completion(self, payload: dict) -> DeepSearchChatResponse:\n        \"\"\"Handles non-streaming requests.\"\"\"\n        response = await self.client.post(self.API_ENDPOINT, json=payload)\n        response.raise_for_status() # Raises HTTPStatusError for 4xx/5xx\n        response_data = response.json()\n        logger.info(f\"Received non-streaming response. ID: {response_data.get('id')}\")\n        return DeepSearchChatResponse.model_validate(response_data)\n\n    async def _stream_chat_completion(self, payload: dict) -> AsyncIterator[DeepSearchChatResponseChunk]:\n        \"\"\"Handles streaming requests.\"\"\"\n        async with self.client.stream(\"POST\", self.API_ENDPOINT, json=payload) as response:\n            response.raise_for_status() # Check status before starting iteration\n            logger.info(\"Starting to stream response.\")\n            buffer = \"\"\n            async for line in response.aiter_lines():\n                if not line:\n                    continue # Skip empty lines\n                buffer += line\n                # Jina API streams JSON objects separated by double newlines\n                while '\\n\\n' in buffer:\n                    chunk_str, buffer = buffer.split('\\n\\n', 1)\n                    if chunk_str.startswith(\"data: \"):\n                        chunk_str = chunk_str[len(\"data: \"):]\n                    if chunk_str == \"[DONE]\":\n                        logger.info(\"Stream finished with [DONE] message.\")\n                        break # End of stream marker\n                    try:\n                        chunk_data = json.loads(chunk_str)\n                        chunk = DeepSearchChatResponseChunk.model_validate(chunk_data)\n                        # logger.debug(f\"Yielding chunk: {chunk.model_dump_json(exclude_none=True)}\")\n                        yield chunk\n                    except json.JSONDecodeError:\n                        logger.warning(f\"Failed to decode JSON chunk: {chunk_str}\")\n                        # Decide how to handle malformed chunks, e.g., skip or raise\n                        continue\n                    except ValidationError as e:\n                        logger.warning(f\"Failed to validate chunk: {chunk_str} - Error: {e}\")\n                        continue # Skip invalid chunks\n            # Process any remaining data in the buffer if needed, though unlikely with \\n\\n separation\n            if buffer.strip() and buffer.strip() != \"[DONE]\":\n                 logger.warning(f\"Data remaining in buffer after stream ended: {buffer}\")\n        logger.info(\"Finished streaming response.\")\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, ToolContext\nfrom typing import Dict, Any, Optional, List, Union, AsyncIterator\nimport logging\nimport os\nfrom dotenv import load_dotenv\nimport asyncio\n\n# Import models and API client\nfrom models import (\n    DeepSearchChatInput, \n    DeepSearchChatResponse, \n    DeepSearchChatResponseChunk,\n    Message # Import Message if needed directly in tool signature, though using Input model is better\n)\nfrom api import JinaDeepSearchClient\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"jina_deepsearch\",\n    description=\"MCP server for Jina AI DeepSearch, providing advanced search, reading, and reasoning capabilities.\"\n)\n\n# Initialize API Client\n# Consider adding error handling for missing API key during initialization if strict\njina_client = JinaDeepSearchClient()\n\n@mcp.tool()\nasync def chat_completion(\n    ctx: ToolContext,\n    messages: List[Dict[str, Any]] = [], # Use Dict temporarily, convert to Message inside\n    model: str = \"jina-deepsearch-v1\",\n    stream: bool = True,\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = \"medium\", # type: ignore\n    budget_tokens: Optional[int] = None,\n    max_attempts: Optional[int] = None,\n    no_direct_answer: bool = False,\n    max_returned_urls: Optional[int] = None,\n    structured_output: Optional[Dict[str, Any]] = None,\n    good_domains: Optional[List[str]] = None,\n    bad_domains: Optional[List[str]] = None,\n    only_domains: Optional[List[str]] = None\n) -> Union[Dict[str, Any], AsyncIterator[Dict[str, Any]]]:\n    \"\"\"\n    Performs a deep search and reasoning process based on a conversation history.\n\n    This tool simulates an agent that searches the web, reads relevant content, \n    and iteratively refines its understanding to answer complex questions.\n\n    Args:\n        ctx: The MCP ToolContext.\n        messages: A list of messages comprising the conversation history.\n                  Each message should be a dict with 'role' and 'content'.\n                  Content can be str or list of content parts (text, image_url, file_url).\n        model: ID of the model to use (default: 'jina-deepsearch-v1').\n        stream: Whether to stream back partial progress (default: True).\n        reasoning_effort: Constraint on reasoning effort ('low', 'medium', 'high', default: 'medium').\n        budget_tokens: Maximum number of tokens allowed.\n        max_attempts: Maximum number of retries.\n        no_direct_answer: Force search/thinking steps (default: False).\n        max_returned_urls: Maximum number of URLs in the final answer.\n        structured_output: JSON schema for the final answer structure.\n        good_domains: List of domains to prioritize.\n        bad_domains: List of domains to strictly exclude.\n        only_domains: List of domains to exclusively include.\n\n    Returns:\n        If stream=False, a dictionary representing DeepSearchChatResponse.\n        If stream=True, an async iterator yielding dictionaries representing DeepSearchChatResponseChunk.\n    \"\"\"\n    logger.info(f\"Received chat_completion request. Stream: {stream}\")\n    try:\n        # --- Input Validation and Preparation ---\n        # Pydantic expects List[Message], so we convert the input dict list\n        # This provides validation for the message structure\n        validated_messages = [Message.model_validate(msg) for msg in messages]\n\n        input_data = DeepSearchChatInput(\n            messages=validated_messages,\n            model=model,\n            stream=stream,\n            reasoning_effort=reasoning_effort,\n            budget_tokens=budget_tokens,\n            max_attempts=max_attempts,\n            no_direct_answer=no_direct_answer,\n            max_returned_urls=max_returned_urls,\n            structured_output=structured_output,\n            good_domains=good_domains,\n            bad_domains=bad_domains,\n            only_domains=only_domains\n        )\n\n        # --- API Call --- \n        result = await jina_client.chat_completion(input_data)\n\n        # --- Response Handling --- \n        if isinstance(result, AsyncIterator):\n            # Streaming response\n            logger.info(\"Returning streaming response iterator.\")\n            async def stream_wrapper():\n                try:\n                    async for chunk in result:\n                        yield chunk.model_dump(exclude_none=True)\n                except Exception as e:\n                    logger.error(f\"Error during stream processing: {e}\", exc_info=True)\n                    # Depending on desired behavior, you might yield an error chunk\n                    # or just stop the stream.\n                    yield {\"error\": f\"Stream processing error: {str(e)}\"}\n            return stream_wrapper()\n        else:\n            # Non-streaming response\n            logger.info(f\"Returning non-streaming response. ID: {result.id}\")\n            return result.model_dump(exclude_none=True)\n\n    except Exception as e:\n        logger.error(f\"Error in chat_completion tool: {e}\", exc_info=True)\n        # Return a structured error that MCP can handle\n        # TODO: Define a standard error structure if needed\n        return {\"error\": str(e), \"details\": \"Failed to process Jina DeepSearch request.\"} \n\nasync def shutdown():\n    \"\"\"Gracefully shutdown the API client.\"\"\"\n    logger.info(\"Shutting down Jina DeepSearch client...\")\n    await jina_client.close()\n    logger.info(\"Jina DeepSearch client shut down.\")\n\nmcp.add_event_handler(\"shutdown\", shutdown)\n\nif __name__ == \"__main__\":\n    # Note: FastMCP's run() method handles the ASGI server setup.\n    # You might configure host, port, etc., via environment variables\n    # or directly in the run() call if needed.\n    logger.info(\"Starting Jina DeepSearch MCP server...\")\n    mcp.run()\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\nhttpx>=0.25.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\n# uvicorn is usually needed to run FastMCP/FastAPI applications\nuvicorn>=0.20.0\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain from https://jina.ai/\n# Required for higher rate limits (10 RPM vs 2 RPM free tier).\n# New keys get 1M free tokens.\nJINA_API_KEY=your_jina_api_key_here\n\n# Optional: Override Jina API base URL (usually not needed)\n# JINA_BASE_URL=https://deepsearch.jina.ai\n\n# Optional: MCP Server configuration\n# HOST=0.0.0.0\n# PORT=8000\n"
    },
    {
      "name": "README.md",
      "content": "# Jina AI DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for interacting with the Jina AI DeepSearch API using FastMCP.\n\n## Description\n\nJina AI DeepSearch provides advanced search, reading, and reasoning capabilities. It acts as an autonomous agent that iteratively searches the web, reads content, and reasons to find the best answer for complex queries requiring up-to-date information or multi-hop reasoning. This MCP server exposes the DeepSearch functionality through a standardized tool interface, fully compatible with the OpenAI Chat API schema.\n\n## Features\n\n*   Integrates with Jina AI DeepSearch API (`/v1/chat/completions`).\n*   Supports both streaming and non-streaming responses.\n*   Handles multi-modal inputs (text, images via data URI, files via data URI).\n*   Provides comprehensive configuration options for the search and reasoning process.\n*   Includes robust error handling and logging.\n*   Manages API key authentication.\n\n## Tools\n\n### `chat_completion`\n\nPerforms a deep search and reasoning process based on a conversation history.\n\n**Input Parameters:**\n\n*   `messages` (List[Dict]): Conversation history. Each dict must have `role` ('user' or 'assistant') and `content`. `content` can be a string or a list of content parts (see below). (Required)\n    *   Text Part: `{\"type\": \"text\", \"text\": \"...\"}`\n    *   Image Part: `{\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,...\"}}`\n    *   File Part: `{\"type\": \"file_url\", \"file_url\": {\"url\": \"data:application/pdf;base64,...\"}}`\n*   `model` (str): Model ID (default: `jina-deepsearch-v1`).\n*   `stream` (bool): Enable streaming response (default: `True`). Recommended for long queries.\n*   `reasoning_effort` (str): Reasoning effort ('low', 'medium', 'high', default: `medium`).\n*   `budget_tokens` (int, optional): Max tokens for the process.\n*   `max_attempts` (int, optional): Max retries with different approaches.\n*   `no_direct_answer` (bool): Force search/thinking steps (default: `False`).\n*   `max_returned_urls` (int, optional): Max URLs in the final response.\n*   `structured_output` (Dict, optional): JSON schema for the final answer.\n*   `good_domains` (List[str], optional): Prioritized domains.\n*   `bad_domains` (List[str], optional): Excluded domains.\n*   `only_domains` (List[str], optional): Exclusive domains.\n\n**Returns:**\n\n*   If `stream=True`: An asynchronous iterator yielding JSON objects representing `DeepSearchChatResponseChunk`.\n*   If `stream=False`: A JSON object representing `DeepSearchChatResponse`.\n\n(See `models.py` for the detailed structure of response objects).\n\n## Authentication\n\nThe server uses an API Key for authentication with the Jina AI API.\n\n1.  Obtain an API key from [Jina AI](https://jina.ai/).\n2.  Set the `JINA_API_KEY` environment variable.\n\nAn API key is recommended for higher rate limits (10 RPM standard vs 2 RPM free). Premium keys offer 100 RPM. New keys typically come with 1 million free tokens.\n\n## Setup\n\n1.  **Clone the repository (or create files):**\n    ```bash\n    # If cloned\n    # git clone <repository_url>\n    # cd <repository_directory>\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure environment variables:**\n    *   Copy `.env.example` to `.env`:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina AI API key:\n        ```\n        JINA_API_KEY=your_jina_api_key_here\n        ```\n\n## Running the Server\n\n```bash\npython main.py\n```\n\nBy default, the server will run on `http://127.0.0.1:8000`. You can configure the host and port using environment variables (`HOST`, `PORT`) or by modifying the `mcp.run()` call in `main.py`.\n\n## Usage Example (Conceptual MCP Client)\n\n```python\nimport mcp\nimport asyncio\n\nasync def main():\n    client = mcp.Client(\"http://127.0.0.1:8000\") # URL of your running MCP server\n\n    # Non-streaming example\n    response = await client.tools.jina_deepsearch.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"What were the key announcements at the latest Apple event?\"}],\n        stream=False,\n        max_returned_urls=3\n    )\n    print(\"--- Non-Streaming Response ---\")\n    print(response)\n\n    # Streaming example\n    print(\"\\n--- Streaming Response ---\")\n    async for chunk in await client.tools.jina_deepsearch.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"Compare the performance of Llama 3 and GPT-4o on coding tasks.\"}],\n        stream=True\n    ):\n        print(chunk)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Error Handling\n\nThe server attempts to catch common errors:\n\n*   **HTTP Errors:** 4xx (e.g., 400 Bad Request, 401 Unauthorized, 429 Too Many Requests) and 5xx (e.g., 500 Internal Server Error, 504 Gateway Timeout) from the Jina API are logged and potentially returned as errors in the MCP response.\n*   **Network Errors:** Timeouts or connection issues are caught and logged.\n*   **Validation Errors:** Issues with the structure of the API response are logged.\n*   **Input Errors:** Invalid input to the tool (e.g., malformed messages) might be caught by Pydantic validation.\n\nCheck the server logs for detailed error information.\n\n## Rate Limits\n\nBe mindful of Jina AI's rate limits:\n\n*   **Free Tier:** 2 requests per minute (RPM)\n*   **Standard Key:** 10 RPM\n*   **Premium Key:** 100 RPM\n\nLimits are applied per API key. Exceeding limits will result in `429 Too Many Requests` errors.\n"
    }
  ]
}