{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field, HttpUrl\nfrom typing import List, Dict, Any, Union, Literal\n\n# --- Input Models ---\n\nclass TextContent(BaseModel):\n    type: Literal[\"text\"] = \"text\"\n    text: str = Field(..., description=\"The text content.\")\n\nclass ImageUrl(BaseModel):\n    url: str = Field(..., description=\"Data URI (e.g., 'data:image/jpeg;base64,...') or HTTPS URL of the image.\")\n\nclass ImageContent(BaseModel):\n    type: Literal[\"image_url\"] = \"image_url\"\n    image_url: ImageUrl = Field(..., description=\"The image URL object.\")\n\nclass DocumentUrl(BaseModel):\n    url: str = Field(..., description=\"Data URI (e.g., 'data:application/pdf;base64,...') of the document.\")\n\nclass DocumentContent(BaseModel):\n    type: Literal[\"document_url\"] = \"document_url\"\n    document_url: DocumentUrl = Field(..., description=\"The document URL object.\")\n\n# Use Union for content type flexibility, matching OpenAI's schema\nContentType = Union[TextContent, ImageContent, DocumentContent]\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation history.\"\"\"\n    role: Literal[\"user\", \"assistant\", \"system\"] = Field(..., description=\"The role of the message sender.\")\n    content: Union[str, List[ContentType]] = Field(..., description=\"The content of the message. Can be a simple string or a list of content blocks for multimodal input (text, image, document).\")\n    name: str | None = Field(None, description=\"An optional name for the participant.\")\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the DeepSearch chat_completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history. Follows OpenAI schema.\")\n    stream: bool = Field(False, description=\"Whether to stream the response. If true, intermediate results might be returned. MCP currently handles the final aggregated response.\")\n    # Add other potential OpenAI compatible parameters if needed, e.g., max_tokens, temperature\n    # model: str = Field(\"jina-deepsearch\", description=\"The model identifier, defaults to Jina DeepSearch.\") # Usually set in the client\n\n# --- Output Models ---\n\nclass Source(BaseModel):\n    \"\"\"Represents a cited source used to generate the answer.\"\"\"\n    url: HttpUrl = Field(..., description=\"The URL of the source.\")\n    title: str | None = Field(None, description=\"The title of the source page.\")\n    snippet: str | None = Field(None, description=\"A relevant snippet from the source.\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: int = Field(..., description=\"Number of tokens in the prompt.\")\n    completion_tokens: int = Field(..., description=\"Number of tokens in the generated completion.\")\n    total_tokens: int = Field(..., description=\"Total number of tokens used.\")\n\nclass DeepSearchChatOutput(BaseModel):\n    \"\"\"Output model for the DeepSearch chat_completion tool.\"\"\"\n    answer: str = Field(..., description=\"The final generated answer from DeepSearch.\")\n    sources: List[Source] = Field([], description=\"A list of sources cited in the answer.\")\n    usage: Usage | None = Field(None, description=\"Token usage information for the request.\")\n    # Include other fields from the API response if necessary\n    id: str | None = Field(None, description=\"A unique identifier for the chat completion.\")\n    object: str | None = Field(None, description=\"The object type, typically 'chat.completion'.\")\n    created: int | None = Field(None, description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str | None = Field(None, description=\"The model used for the chat completion.\")\n    system_fingerprint: str | None = Field(None, description=\"This fingerprint represents the backend configuration that the model runs with.\")\n    finish_reason: str | None = Field(None, description=\"The reason the model stopped generating tokens.\")\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport logging\nimport os\nfrom typing import Dict, Any, AsyncGenerator\nfrom models import DeepSearchChatInput, DeepSearchChatOutput, Source, Usage\n\nlogger = logging.getLogger(__name__)\n\nclass DeepSearchAPIError(Exception):\n    \"\"\"Custom exception for DeepSearch API errors.\"\"\"\n    def __init__(self, status_code: int, error_info: Dict[str, Any]):\n        self.status_code = status_code\n        self.error_info = error_info\n        super().__init__(f\"DeepSearch API Error {status_code}: {error_info}\")\n\nclass DeepSearchAPIClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://api.jina.ai/v1\"\n    CHAT_COMPLETIONS_ENDPOINT = \"/chat/completions\"\n\n    def __init__(self, api_key: str | None = None, base_url: str | None = None, timeout: float = 120.0):\n        \"\"\"\n        Initializes the DeepSearchAPIClient.\n\n        Args:\n            api_key: The Jina AI API key. Defaults to JINA_API_KEY environment variable.\n            base_url: The base URL for the DeepSearch API. Defaults to https://api.jina.ai/v1.\n            timeout: Default timeout for API requests in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API key not provided. Set JINA_API_KEY environment variable or pass it during initialization.\")\n\n        self.base_url = base_url or self.DEFAULT_BASE_URL\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\"\n        }\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=timeout\n        )\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n\n    async def _request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"Makes an asynchronous HTTP request to the API.\"\"\"\n        try:\n            response = await self.client.request(method, endpoint, **kwargs)\n            response.raise_for_status()  # Raises HTTPStatusError for 4xx/5xx responses\n            return response.json()\n        except httpx.HTTPStatusError as e:\n            error_info = e.response.json() if e.response.content else {\"error\": \"Unknown API error\"}\n            logger.error(f\"HTTP error {e.response.status_code} calling {e.request.url}: {error_info}\")\n            raise DeepSearchAPIError(status_code=e.response.status_code, error_info=error_info) from e\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out calling {e.request.url}: {e}\")\n            raise\n        except httpx.RequestError as e:\n            logger.error(f\"Request error calling {e.request.url}: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred: {e}\")\n            raise\n\n    async def chat_completion(self, params: DeepSearchChatInput) -> DeepSearchChatOutput:\n        \"\"\"\n        Initiates a DeepSearch chat completion process.\n\n        Args:\n            params: The input parameters including the message history.\n\n        Returns:\n            The processed response including the answer, sources, and usage.\n\n        Raises:\n            DeepSearchAPIError: If the API returns an error.\n            httpx.TimeoutException: If the request times out.\n            httpx.RequestError: For other network-related errors.\n        \"\"\"\n        payload = params.dict(exclude_unset=True)\n        # Ensure the model is set, Jina typically uses 'jina-deepsearch'\n        if 'model' not in payload:\n             payload['model'] = 'jina-deepsearch'\n\n        # The API expects content as a string or list of dicts, Pydantic handles serialization\n        # Ensure messages are serialized correctly\n        serialized_messages = []\n        for msg in params.messages:\n            msg_dict = msg.dict(exclude_unset=True)\n            if isinstance(msg.content, list):\n                # Ensure content items are dicts\n                msg_dict['content'] = [item.dict() for item in msg.content]\n            serialized_messages.append(msg_dict)\n        payload['messages'] = serialized_messages\n\n        logger.info(f\"Sending chat completion request to DeepSearch API with {len(params.messages)} messages.\")\n\n        # Note: Streaming is handled differently. This implementation assumes non-streaming\n        # or that the MCP framework doesn't require explicit generator handling here.\n        # If streaming is True, the API might return a stream, which needs specific handling.\n        # For simplicity, we'll assume the API returns the full response even if stream=True,\n        # or we only process the final result in this non-streaming client method.\n        if params.stream:\n            logger.warning(\"Streaming requested, but this client method currently processes the final aggregated response.\")\n            # To implement streaming properly, this method should return an AsyncGenerator\n            # and handle the server-sent events (SSE) from the API.\n\n        response_data = await self._request(\"POST\", self.CHAT_COMPLETIONS_ENDPOINT, json=payload)\n\n        # --- Response Parsing --- \n        # Extract data according to Jina's OpenAI-compatible schema + extensions\n        try:\n            answer = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n            \n            # Sources might be in a custom field or potentially metadata. Adjust as needed.\n            # Assuming sources are provided in a top-level 'sources' key or similar.\n            # This is an assumption - check Jina's exact response structure.\n            raw_sources = response_data.get(\"sources\", []) \n            if not raw_sources and isinstance(response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"context\"), dict):\n                 # Fallback: Check if sources are nested under message.context (example structure)\n                 raw_sources = response_data[\"choices\"][0][\"message\"][\"context\"].get(\"sources\", [])\n            \n            sources = [Source(**source_data) for source_data in raw_sources if isinstance(source_data, dict)]\n\n            usage_data = response_data.get(\"usage\")\n            usage = Usage(**usage_data) if usage_data else None\n\n            return DeepSearchChatOutput(\n                answer=answer,\n                sources=sources,\n                usage=usage,\n                # Pass through other standard OpenAI fields\n                id=response_data.get(\"id\"),\n                object=response_data.get(\"object\"),\n                created=response_data.get(\"created\"),\n                model=response_data.get(\"model\"),\n                system_fingerprint=response_data.get(\"system_fingerprint\"),\n                finish_reason=response_data.get(\"choices\", [{}])[0].get(\"finish_reason\")\n            )\n        except (KeyError, IndexError, TypeError) as e:\n            logger.error(f\"Error parsing DeepSearch response: {e}. Response data: {response_data}\")\n            raise ValueError(f\"Failed to parse essential fields from DeepSearch API response: {e}\") from e\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP\nfrom typing import Dict, Any\nimport logging\nimport asyncio\nimport os\nfrom dotenv import load_dotenv\n\nfrom models import DeepSearchChatInput, DeepSearchChatOutput, Message, TextContent\nfrom api import DeepSearchAPIClient, DeepSearchAPIError\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP server for Jina AI's DeepSearch API. Provides advanced search capabilities combining web search, content reading, and reasoning.\"\n)\n\n# Initialize API Client\n# The client will automatically pick up JINA_API_KEY from the environment\ntry:\n    api_client = DeepSearchAPIClient()\n    logger.info(\"DeepSearch API Client initialized successfully.\")\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize DeepSearch API Client: {e}\")\n    # Optionally, exit or prevent server start if API key is essential\n    # raise SystemExit(f\"Error: {e}\")\n    api_client = None # Set to None to handle gracefully in tool calls\n\n@mcp.tool()\nasync def chat_completion(params: DeepSearchChatInput) -> Dict[str, Any]:\n    \"\"\"\n    Initiates a DeepSearch process based on a conversation history.\n\n    Args:\n        params (DeepSearchChatInput): Input parameters containing the list of messages and optional settings.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the search results (answer, sources, usage) \n                      or an error message.\n    \"\"\"\n    if not api_client:\n        logger.error(\"DeepSearch API Client is not initialized. Cannot process request.\")\n        return {\"error\": \"DeepSearch API Client not initialized. Check API key configuration.\"}\n\n    logger.info(f\"Received chat_completion request with {len(params.messages)} messages.\")\n    \n    # Basic validation: Ensure there's at least one message\n    if not params.messages:\n        logger.warning(\"Received chat_completion request with no messages.\")\n        return {\"error\": \"Input must contain at least one message.\"}\n\n    try:\n        # Ensure content is correctly formatted before sending to API client\n        # (Pydantic validation handles basic structure, but complex content needs checking)\n        # The API client now handles the detailed serialization\n        pass \n            \n        result: DeepSearchChatOutput = await api_client.chat_completion(params)\n        logger.info(f\"Successfully completed DeepSearch request. Answer length: {len(result.answer)}\")\n        # Return the Pydantic model converted to a dictionary\n        return result.dict(exclude_none=True) \n\n    except DeepSearchAPIError as e:\n        logger.error(f\"DeepSearch API error: Status={e.status_code}, Info={e.error_info}\")\n        return {\"error\": f\"DeepSearch API Error: {e.status_code} - {e.error_info}\"}\n    except httpx.TimeoutException:\n        logger.error(\"DeepSearch API request timed out.\")\n        return {\"error\": \"Request to DeepSearch API timed out.\"}\n    except httpx.RequestError as e:\n        logger.error(f\"Network error connecting to DeepSearch API: {e}\")\n        return {\"error\": f\"Network error communicating with DeepSearch API: {e}\"}\n    except ValueError as e:\n        # Handle parsing errors from API response or validation errors\n        logger.error(f\"Data validation or parsing error: {e}\")\n        return {\"error\": f\"Data validation or parsing error: {e}\"}\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred in chat_completion tool.\") # Use exception to log stack trace\n        return {\"error\": f\"An unexpected internal error occurred: {str(e)}\"}\n\n# Graceful shutdown\n@mcp.app.on_event(\"shutdown\")\nasync def shutdown_event():\n    if api_client:\n        logger.info(\"Closing DeepSearch API client...\")\n        await api_client.close()\n        logger.info(\"DeepSearch API client closed.\")\n\nif __name__ == \"__main__\":\n    # MCP's run() method starts the Uvicorn server\n    # Configuration can be done via environment variables like MCP_PORT, MCP_HOST\n    logger.info(\"Starting DeepSearch MCP server...\")\n    mcp.run()\n"
    },
    {
      "name": "requirements.txt",
      "content": "mcp>=0.1.0\nfastmcp>=0.1.0\nhttpx>=0.25.0,<0.28.0\npydantic>=2.0.0,<3.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.23.0,<0.28.0 # Usually a dependency of fastmcp, but good to specify\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain your API key from Jina AI Cloud (https://cloud.jina.ai/)\nJINA_API_KEY=\"your_jina_api_key_here\"\n\n# Optional: Override the default DeepSearch API base URL\n# JINA_API_BASE_URL=\"https://api.jina.ai/v1\"\n\n# Optional: MCP Server Configuration\n# MCP_HOST=\"0.0.0.0\"\n# MCP_PORT=8000\n"
    },
    {
      "name": "README.md",
      "content": "# DeepSearch MCP Server\n\nThis repository contains an MCP (Model Context Protocol) server implementation for the Jina AI DeepSearch API, built using FastMCP.\n\n## Overview\n\nThe Jina AI DeepSearch API provides advanced search capabilities by combining web searching, content reading, and iterative reasoning. It's designed to answer complex questions requiring real-time information, multi-hop reasoning, or in-depth research. The API follows the OpenAI Chat API schema.\n\nThis MCP server exposes the DeepSearch functionality as a standard MCP tool, allowing easy integration into multi-agent systems or other applications using MCP.\n\n## Features\n\n*   **DeepSearch Integration:** Connects to the Jina AI DeepSearch API.\n*   **OpenAI Schema Compatibility:** Uses Pydantic models matching the OpenAI chat completion schema for input and output.\n*   **Multimodal Support:** Accepts text, images (via data URI or URL), and documents (via data URI) as input within the message history.\n*   **Structured Output:** Returns the answer, cited sources, and token usage information.\n*   **Error Handling:** Includes robust error handling for API errors, network issues, and timeouts.\n*   **Authentication:** Uses API key authentication (via `JINA_API_KEY` environment variable).\n*   **Configuration:** Configurable via environment variables.\n\n## Tools\n\n### `chat_completion`\n\n*   **Description:** Initiates a DeepSearch process based on a conversation history. The API iteratively searches the web, reads relevant content, and performs reasoning steps to arrive at the most accurate answer possible. The final response includes the answer, cited sources (URLs), and token usage statistics.\n*   **Input:** `DeepSearchChatInput` model (see `models.py`)\n    *   `messages`: A list of `Message` objects representing the conversation history. Each message has a `role` (`user`, `assistant`, `system`) and `content`. `content` can be a string or a list for multimodal input (text, images, documents).\n    *   `stream`: (Optional) Boolean, defaults to `false`. While the API supports streaming, this server currently returns the final aggregated response.\n*   **Output:** `DeepSearchChatOutput` model (see `models.py`)\n    *   `answer`: The final generated answer.\n    *   `sources`: A list of cited sources (`url`, `title`, `snippet`).\n    *   `usage`: Token usage statistics (`prompt_tokens`, `completion_tokens`, `total_tokens`).\n    *   Other standard OpenAI response fields (`id`, `object`, `created`, `model`, etc.).\n\n## Setup and Installation\n\n1.  **Clone the repository (or create the files):**\n    ```bash\n    # If cloning a repo:\n    # git clone <repository_url>\n    # cd <repository_directory>\n\n    # If creating files manually, ensure you have:\n    # main.py, api.py, models.py, requirements.txt, .env.example, README.md\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina AI API key:\n        ```env\n        JINA_API_KEY=\"your_jina_api_key_here\"\n        ```\n    *   You can obtain a Jina AI API key from [Jina AI Cloud](https://cloud.jina.ai/).\n    *   Optionally, set `MCP_HOST` and `MCP_PORT` in the `.env` file if you want to run the server on a different address or port (defaults usually to `0.0.0.0:8000`).\n\n## Running the Server\n\nStart the MCP server using:\n\n```bash\npython main.py\n```\n\nThe server will start, typically on `http://0.0.0.0:8000` (or as configured).\n\n## Usage Example (using `mcp-client`)\n\n```python\nimport asyncio\nfrom mcp.client.aio import MCPClient\n\nasync def main():\n    client = MCPClient(\"http://localhost:8000\") # Adjust URL if needed\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": \"What were the key announcements from the latest Apple event?\"\n        }\n    ]\n\n    try:\n        response = await client.run_tool(\n            tool_name=\"chat_completion\",\n            params={\"messages\": messages}\n        )\n        \n        if \"error\" in response:\n            print(f\"Error: {response['error']}\")\n        else:\n            print(\"Answer:\", response.get(\"answer\"))\n            print(\"\\nSources:\")\n            for source in response.get(\"sources\", []):\n                print(f\"- {source.get('title', 'N/A')}: {source.get('url')}\")\n            print(\"\\nUsage:\", response.get(\"usage\"))\n            \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        await client.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n## Development Notes\n\n*   **Streaming:** The DeepSearch API supports streaming responses. While the `stream` parameter is included in the input model, the current `api.py` client implementation waits for and processes the final aggregated response. To fully support streaming, the `api_client.chat_completion` method and the MCP tool would need to be adapted to handle Server-Sent Events (SSE) and potentially yield intermediate results.\n*   **Error Handling:** The server includes basic error handling, but specific API error codes from Jina might require more tailored handling based on their documentation.\n*   **Model Customization:** The client currently defaults to the `jina-deepsearch` model. This could be made configurable if needed.\n"
    }
  ]
}