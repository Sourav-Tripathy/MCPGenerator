{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import Optional, List, Dict, Any, Union, Literal\n\n# Type Definitions from Implementation Plan\n\nclass MessageContentPart(BaseModel):\n    \"\"\"Represents a part of the message content, which can be text or an image/file URL.\"\"\"\n    type: Literal['text', 'image_url', 'file_url'] = Field(..., description=\"Type of content part ('text', 'image_url', 'file_url').\")\n    text: Optional[str] = Field(None, description=\"The text content, if type is 'text'.\")\n    image_url: Optional[Dict[str, str]] = Field(None, description=\"The image URL (data URI), if type is 'image_url'. Structure: {'url': 'data:image/...'}\")\n    file_url: Optional[Dict[str, str]] = Field(None, description=\"The file URL (data URI), if type is 'file_url'. Structure: {'url': 'data:application/...'}\")\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant', 'system'] = Field(..., description=\"The role of the author ('user', 'assistant', or 'system').\")\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"The content of the message. Can be a simple string or a list of content parts for multimodal input.\")\n\nclass URLCitation(BaseModel):\n    \"\"\"Details of a URL citation within the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited page.\")\n    exactQuote: Optional[str] = Field(None, alias=\"exactQuote\", description=\"The exact quote cited from the source.\")\n    url: str = Field(..., description=\"URL of the cited source.\")\n    dateTime: Optional[str] = Field(None, alias=\"dateTime\", description=\"Timestamp related to the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation associated with the response content, e.g., a URL citation.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[URLCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass Delta(BaseModel):\n    \"\"\"The delta content for a streaming chunk.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None, description=\"Role associated with the chunk ('assistant').\")\n    content: Optional[str] = Field(None, description=\"The text content of the chunk. May include XML tags like <think> for reasoning steps.\")\n    type: Optional[str] = Field(None, description=\"Type of content (e.g., 'text').\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations for the content chunk.\")\n\nclass ChoiceChunk(BaseModel):\n    \"\"\"Represents a choice in a streaming response chunk.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    delta: Delta = Field(..., description=\"The content delta for a streaming chunk.\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probability information (currently null).\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the generation finished (e.g., 'stop').\")\n\nclass ChoiceResponse(BaseModel):\n    \"\"\"Represents a choice in a non-streaming response.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    message: Message = Field(..., description=\"The full message for a non-streaming response.\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probability information (currently null).\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the generation finished (e.g., 'stop').\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: int = Field(..., description=\"Tokens used by the prompt.\")\n    completion_tokens: int = Field(..., description=\"Tokens generated for the completion.\")\n    total_tokens: int = Field(..., description=\"Total tokens used.\")\n\nclass DeepSearchChatChunk(BaseModel):\n    \"\"\"Structure of a single chunk received during streaming.\"\"\"\n    id: str = Field(..., description=\"Unique identifier for the chat completion chunk.\")\n    object: str = Field(..., description=\"Object type, e.g., 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"Timestamp of creation.\")\n    model: str = Field(..., description=\"Model used.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint.\")\n    choices: List[ChoiceChunk] = Field(..., description=\"List of choices, usually one.\")\n    usage: Optional[Usage] = Field(None, description=\"Token usage (present in the final chunk).\")\n    visitedURLs: Optional[List[str]] = Field(None, description=\"URLs visited during the search process (present in the final chunk).\")\n    readURLs: Optional[List[str]] = Field(None, description=\"URLs read during the search process (present in the final chunk).\") # Added based on plan description\n    numURLs: Optional[int] = Field(None, description=\"Number of URLs (present in the final chunk).\") # Added based on plan description\n\nclass DeepSearchChatResponse(BaseModel):\n    \"\"\"Structure of the response received for non-streaming requests.\"\"\"\n    id: str = Field(..., description=\"Unique identifier for the chat completion.\")\n    object: str = Field(..., description=\"Object type, e.g., 'chat.completion'.\")\n    created: int = Field(..., description=\"Timestamp of creation.\")\n    model: str = Field(..., description=\"Model used.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint.\")\n    choices: List[ChoiceResponse] = Field(..., description=\"List of choices, usually one.\")\n    usage: Usage = Field(..., description=\"Token usage statistics.\")\n    visitedURLs: Optional[List[str]] = Field(None, description=\"URLs visited during the search process.\")\n    readURLs: Optional[List[str]] = Field(None, description=\"URLs read during the search process.\")\n    numURLs: Optional[int] = Field(None, description=\"Number of URLs.\")\n\n# Input Model Definition\n\nclass DeepSearchChatRequest(BaseModel):\n    \"\"\"Request model for the Jina DeepSearch chat completions endpoint.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation so far. Supports text, images (webp, png, jpeg as data URI), and files (txt, pdf as data URI up to 10MB).\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use. Currently 'jina-deepsearch-v1'.\")\n    stream: bool = Field(True, description=\"Whether to stream back partial progress (reasoning steps and final answer). Strongly recommended to be true to avoid timeouts.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains effort on reasoning. Supported values: 'low', 'medium', 'high'. Overridden by budget_tokens or max_attempts.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides reasoning_effort.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Overrides reasoning_effort.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces the model to take further thinking/search steps even for trivial queries.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema to ensure the final answer matches the structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        use_enum_values = True # Ensure Literal values are handled correctly\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import AsyncGenerator, Union, Dict, Any\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatRequest, DeepSearchChatResponse, DeepSearchChatChunk\n\nlogger = logging.getLogger(__name__)\n\nclass DeepSearchApiClient:\n    \"\"\"Client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the API client, loading configuration from environment variables.\"\"\"\n        self.api_key = os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"JINA_API_KEY environment variable not set.\")\n\n        self.base_url = os.getenv(\"JINA_DEEPSEARCH_BASE_URL\", \"https://deepsearch.jina.ai\")\n        self.api_endpoint = \"/v1/chat/completions\"\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Important for non-streaming\n        }\n        # Increased timeout for potentially long reasoning/search, especially non-streaming\n        self.client = httpx.AsyncClient(base_url=self.base_url, headers=self.headers, timeout=300.0)\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n\n    async def chat_completions(\n        self,\n        request_data: DeepSearchChatRequest\n    ) -> Union[DeepSearchChatResponse, AsyncGenerator[DeepSearchChatChunk, None]]:\n        \"\"\"\"\"\"\n        Calls the DeepSearch chat completions endpoint.\n\n        Args:\n            request_data: The request data conforming to DeepSearchChatRequest model.\n\n        Returns:\n            If stream=False, returns a DeepSearchChatResponse object.\n            If stream=True, returns an async generator yielding DeepSearchChatChunk objects.\n\n        Raises:\n            httpx.HTTPStatusError: If the API returns an error status code (4xx or 5xx).\n            httpx.RequestError: If there's a network issue connecting to the API.\n            ValueError: If the response cannot be parsed.\n            ValidationError: If the response data doesn't match the Pydantic models.\n        \"\"\"\"\n        payload = request_data.model_dump(exclude_none=True, by_alias=True)\n        logger.info(f\"Sending request to {self.base_url}{self.api_endpoint} with stream={request_data.stream}\")\n        # logger.debug(f\"Request payload: {payload}\") # Be cautious logging potentially sensitive message content\n\n        try:\n            if request_data.stream:\n                return self._stream_chat_completions(payload)\n            else:\n                response = await self.client.post(self.api_endpoint, json=payload)\n                response.raise_for_status() # Raise exception for 4xx/5xx errors\n                response_json = response.json()\n                logger.info(\"Received non-streaming response.\")\n                # logger.debug(f\"Non-streaming response data: {response_json}\")\n                return DeepSearchChatResponse.model_validate(response_json)\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n            # Attempt to parse error details if available\n            try:\n                error_details = e.response.json()\n            except json.JSONDecodeError:\n                error_details = e.response.text\n            raise httpx.HTTPStatusError(message=f\"API Error: {error_details}\", request=e.request, response=e.response)\n        except httpx.RequestError as e:\n            logger.error(f\"Network error connecting to DeepSearch API: {e}\")\n            raise\n        except ValidationError as e:\n            logger.error(f\"Failed to validate API response: {e}\")\n            raise ValueError(f\"Invalid response format received from API: {e}\")\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON response: {e}\")\n            raise ValueError(f\"Failed to decode JSON response: {e}\")\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred: {e}\")\n            raise\n\n    async def _stream_chat_completions(\n        self,\n        payload: Dict[str, Any]\n    ) -> AsyncGenerator[DeepSearchChatChunk, None]:\n        \"\"\"Handles the streaming response from the API.\"\"\"\n        try:\n            async with self.client.stream(\"POST\", self.api_endpoint, json=payload) as response:\n                response.raise_for_status() # Check status before starting iteration\n                logger.info(\"Streaming response started.\")\n                async for line in response.aiter_lines():\n                    if line.startswith(\"data: \"):\n                        data_str = line[len(\"data: \"):]\n                        if data_str.strip() == \"[DONE]\":\n                            logger.info(\"Streaming response finished ([DONE] received).\")\n                            break\n                        if not data_str.strip():\n                            continue\n                        try:\n                            chunk_json = json.loads(data_str)\n                            # logger.debug(f\"Received stream chunk: {chunk_json}\")\n                            yield DeepSearchChatChunk.model_validate(chunk_json)\n                        except json.JSONDecodeError:\n                            logger.warning(f\"Skipping invalid JSON data in stream: {data_str}\")\n                            continue\n                        except ValidationError as e:\n                            logger.warning(f\"Skipping invalid chunk structure in stream: {e}. Data: {data_str}\")\n                            continue\n                    elif line.strip(): # Log unexpected non-empty lines\n                        logger.warning(f\"Received unexpected line in stream: {line}\")\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP error during streaming: {e.response.status_code}\")\n            # Attempt to read body for more details, might fail if stream already started erroring\n            try:\n                error_body = await e.response.aread()\n                logger.error(f\"Error response body: {error_body.decode()}\")\n                error_details = error_body.decode()\n            except Exception:\n                error_details = \"(Could not read error body)\"\n            # Raise a new error to stop the generator and signal the problem\n            raise httpx.HTTPStatusError(message=f\"API Error during stream: {error_details}\", request=e.request, response=e.response)\n        except httpx.RequestError as e:\n            logger.error(f\"Network error during streaming: {e}\")\n            raise\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during streaming: {e}\")\n            raise\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, ToolContext\nfrom typing import AsyncGenerator, Union, Any\nimport logging\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\n\nfrom models import (DeepSearchChatRequest, DeepSearchChatResponse, DeepSearchChatChunk)\nfrom api import DeepSearchApiClient\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"jina_deepsearch\",\n    description=\"MCP server for Jina AI DeepSearch, an API that combines web searching, reading, and reasoning to provide comprehensive answers to complex questions. It is designed to be compatible with the OpenAI Chat API schema.\"\n)\n\n# Initialize API Client\n# Use a context manager approach for client lifecycle\n@mcp.context_lifespan\nasync def api_client_lifespan(context: ToolContext):\n    \"\"\"Manage the lifecycle of the DeepSearchApiClient.\"\"\"\n    logger.info(\"Initializing DeepSearch API client...\")\n    client = DeepSearchApiClient()\n    context.state.api_client = client\n    try:\n        yield # Server runs here\n    finally:\n        logger.info(\"Closing DeepSearch API client...\")\n        await client.close()\n\n# Define MCP Tools\n@mcp.tool()\nasync def chat_completions(\n    context: ToolContext,\n    request: DeepSearchChatRequest\n) -> Union[DeepSearchChatResponse, AsyncGenerator[DeepSearchChatChunk, None]]:\n    \"\"\"\"\"\"\n    Generates a response based on iterative search, reading, and reasoning using Jina DeepSearch.\n\n    Accepts a list of messages and various parameters to control the search and reasoning process.\n    Supports streaming responses.\n\n    Args:\n        context: The MCP ToolContext, providing access to shared state (like the API client).\n        request: The chat completion request parameters conforming to DeepSearchChatRequest.\n\n    Returns:\n        If request.stream is False, returns a single DeepSearchChatResponse object.\n        If request.stream is True, returns an async generator yielding DeepSearchChatChunk objects.\n\n    Raises:\n        Exception: Propagates exceptions from the API client (e.g., HTTP errors, validation errors).\n    \"\"\"\"\n    api_client: DeepSearchApiClient = context.state.api_client\n    logger.info(f\"Executing chat_completions tool (stream={request.stream})\")\n    try:\n        result = await api_client.chat_completions(request)\n        return result\n    except Exception as e:\n        logger.exception(f\"Error executing chat_completions tool: {e}\")\n        # Re-raise the exception to let FastMCP handle standard error responses\n        raise\n\n# --- Main Execution --- #\nif __name__ == \"__main__\":\n    # This block is for local development/debugging if needed.\n    # Production deployment should use a command like:\n    # uvicorn main:mcp.app --host 0.0.0.0 --port 8000\n    logger.info(\"Starting Jina DeepSearch MCP server locally...\")\n\n    # Example of how to run the server directly (for simple testing)\n    # Note: `mcp.run()` is a simplified way, `uvicorn` is preferred for production\n    # mcp.run() # This might block, consider async setup if needed here\n\n    # To run with uvicorn programmatically (alternative):
    # import uvicorn
    # uvicorn.run(mcp.app, host=\"127.0.0.1\", port=8000)
    
    print(\"MCP server defined. Run with: uvicorn main:mcp.app --reload\")\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\nhttpx>=0.25.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.15.0 # For running the ASGI app\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI DeepSearch API Credentials\n# Get your API key from Jina AI\nJINA_API_KEY=your_jina_api_key_here\n\n# Base URL for the Jina DeepSearch API (Optional, defaults to https://deepsearch.jina.ai)\n# JINA_DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai\n"
    },
    {
      "name": "README.md",
      "content": "# Jina AI DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for the [Jina AI DeepSearch API](https://jina.ai/deepsearch/). DeepSearch combines web searching, reading, and reasoning to provide comprehensive answers to complex questions, offering an API compatible with the OpenAI Chat API schema.\n\nThis server is built using [FastMCP](https://github.com/cognita-ai/fastmcp).\n\n## Features\n\n*   Provides an MCP interface to the Jina DeepSearch `/v1/chat/completions` endpoint.\n*   Supports both streaming and non-streaming responses.\n*   Handles complex input including text, images (data URI), and files (data URI).\n*   Configurable reasoning effort, token budgets, domain filtering, and more.\n*   Built-in Pydantic models for request and response validation.\n*   Asynchronous API client using `httpx`.\n*   Authentication via Bearer token (API Key).\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    Copy the example environment file:\n    ```bash\n    cp .env.example .env\n    ```\n    Edit the `.env` file and add your Jina AI API key:\n    ```env\n    JINA_API_KEY=your_jina_api_key_here\n    # JINA_DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai # Optional override\n    ```\n    You can obtain an API key from the [Jina AI website](https://jina.ai/).\n\n## Running the Server\n\nUse `uvicorn` to run the ASGI application:\n\n```bash\n# Basic execution\nuvicorn main:mcp.app --host 0.0.0.0 --port 8000\n\n# With auto-reload for development\nuvicorn main:mcp.app --reload --port 8000\n```\n\nThe MCP server will be available at `http://localhost:8000`.\n\n## Available Tools\n\n### `chat_completions`\n\nGenerates a response based on iterative search, reading, and reasoning using Jina DeepSearch.\n\n**Description:** Corresponds to the `/v1/chat/completions` endpoint of the Jina DeepSearch API.\n\n**Input:** `DeepSearchChatRequest` model\n\n*   `messages` (List[Message], required): Conversation history. Messages can contain text, `image_url` (data URI), or `file_url` (data URI for txt/pdf up to 10MB).\n*   `model` (str, optional, default: \"jina-deepsearch-v1\"): Model ID.\n*   `stream` (bool, optional, default: True): Enable streaming response. **Strongly recommended** to avoid timeouts.\n*   `reasoning_effort` (str, optional, default: \"medium\"): Control reasoning ('low', 'medium', 'high').\n*   `budget_tokens` (int, optional): Max token budget.\n*   `max_attempts` (int, optional): Max retry attempts.\n*   `no_direct_answer` (bool, optional, default: False): Force search even for trivial questions.\n*   `max_returned_urls` (int, optional): Max URLs in the final answer.\n*   `structured_output` (dict, optional): JSON schema for structured output.\n*   `good_domains` (List[str], optional): Prioritized domains.\n*   `bad_domains` (List[str], optional): Excluded domains.\n*   `only_domains` (List[str], optional): Exclusively included domains.\n\n**Output:**\n\n*   If `stream=True`: An `AsyncGenerator` yielding `DeepSearchChatChunk` objects.\n*   If `stream=False`: A single `DeepSearchChatResponse` object.\n\n**Authentication:** Requires the `JINA_API_KEY` environment variable to be set for Bearer token authentication.\n\n**Rate Limits:** The Jina DeepSearch API has rate limits (e.g., 10 requests/minute). This server does not implement explicit rate limiting; ensure your usage complies with Jina AI's terms.\n\n**Error Handling:** The tool propagates errors from the API, including HTTP status errors (4xx, 5xx), network errors, and validation errors.\n\n## Example Usage (Conceptual MCP Client)\n\n```python\nfrom mcp.client.fastmcp import FastMCPClient\n\nasync def main():\n    client = FastMCPClient(base_url=\"http://localhost:8000\")\n\n    request_data = {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What were the key advancements in AI in 2023?\"}\n        ],\n        \"stream\": True,\n        \"reasoning_effort\": \"medium\"\n    }\n\n    try:\n        # Streaming example\n        async for chunk in await client.tools.jina_deepsearch.chat_completions(request=request_data):\n            # Process each chunk (e.g., print content delta)\n            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n            # Final chunk might contain usage, visitedURLs etc.\n            if chunk.usage:\n                print(f\"\\n\\nUsage: {chunk.usage}\")\n                print(f\"Visited URLs: {chunk.visitedURLs}\")\n\n        # Non-streaming example\n        # request_data[\"stream\"] = False\n        # response = await client.tools.jina_deepsearch.chat_completions(request=request_data)\n        # print(response.choices[0].message.content)\n        # print(response.usage)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        await client.close()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n"
    }
  ]
}