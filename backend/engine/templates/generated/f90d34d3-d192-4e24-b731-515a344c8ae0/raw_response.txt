{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Union\n\n# --- Input Models ---\n\nclass MessageContentPart(BaseModel):\n    \"\"\"Represents a part of the message content, which can be text or an image/document URL (Data URI).\"\"\"\n    type: str = Field(..., description=\"Type of content part ('text' or 'image_url').\")\n    text: Optional[str] = Field(None, description=\"Text content, if type is 'text'.\")\n    image_url: Optional[Dict[str, str]] = Field(None, description=\"Image/Document URL object, if type is 'image_url'. Expects {'url': 'data:...'}\")\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: str = Field(..., description=\"The role of the message author ('user' or 'assistant').\")\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"The content of the message. Can be a simple string or a list of content parts for multimodal input.\")\n\nclass DeepSearchChatParams(BaseModel):\n    \"\"\"Input parameters for the Jina DeepSearch chat completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history. Can include text and data URIs for images (webp, png, jpeg) or documents (txt, pdf) up to 10MB.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(True, description=\"If true, delivers events as server-sent events. The MCP tool handles streaming internally, but setting this to false is discouraged and may cause timeouts.\")\n    reasoning_effort: Optional[str] = Field(\"medium\", description=\"Constrains effort on reasoning. Supported values: 'low', 'medium', 'high'. Lower effort may yield faster responses with fewer reasoning tokens.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides 'reasoning_effort'. Larger budgets can improve quality for complex queries.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Allows trying different reasoning approaches. Overrides 'reasoning_effort'.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces further thinking/search steps even for seemingly trivial queries.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer, sorted by relevance.\")\n    structured_output: Optional[Any] = Field(None, description=\"Enables structured output matching a supplied JSON schema.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        extra = 'allow' # Allow extra fields if needed\n\n# --- Output Models ---\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"The URL of the source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation within the response content, e.g., a URL citation.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass ResponseMessage(BaseModel):\n    \"\"\"Represents the message content generated by the model.\"\"\"\n    role: str = Field(\"assistant\", description=\"Typically 'assistant'.\")\n    content: Optional[str] = Field(None, description=\"The textual content of the response.\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations related to the content, like citations.\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Number of tokens in the prompt.\")\n    completion_tokens: Optional[int] = Field(None, description=\"Number of tokens in the generated completion.\")\n    total_tokens: Optional[int] = Field(None, description=\"Total number of tokens used in the request.\")\n    search_queries_count: Optional[int] = Field(None, description=\"Number of search queries performed.\")\n    visited_urls_count: Optional[int] = Field(None, description=\"Number of URLs visited.\")\n    read_urls_count: Optional[int] = Field(None, description=\"Number of URLs read.\")\n    reasoning_steps_count: Optional[int] = Field(None, description=\"Number of reasoning steps taken.\")\n\nclass Choice(BaseModel):\n    \"\"\"A single choice generated by the model.\"\"\"\n    index: int = Field(..., description=\"The index of the choice.\")\n    message: ResponseMessage = Field(..., description=\"The message generated by the model.\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the model stopped generating tokens.\")\n\nclass ChatCompletionResponse(BaseModel):\n    \"\"\"The final aggregated response from the DeepSearch process.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(\"chat.completion\", description=\"The object type, which is always 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[Choice] = Field(..., description=\"A list of chat completion choices. Currently, only one choice is supported.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics for the completion request.\")\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process.\")\n    read_urls: Optional[List[str]] = Field(None, description=\"List of URLs read during the search process.\")\n\nclass ChatCompletionStreamChoiceDelta(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n    annotations: Optional[List[Annotation]] = None\n\nclass ChatCompletionStreamChoice(BaseModel):\n    index: int\n    delta: ChatCompletionStreamChoiceDelta\n    finish_reason: Optional[str] = None\n\nclass ChatCompletionChunk(BaseModel):\n    \"\"\"Represents a chunk received during streaming.\"\"\"\n    id: str\n    object: str = \"chat.completion.chunk\"\n    created: int\n    model: str\n    choices: List[ChatCompletionStreamChoice]\n    usage: Optional[Usage] = None # Usage might appear in the last chunk\n    visited_urls: Optional[List[str]] = None # May appear in intermediate or last chunk\n    read_urls: Optional[List[str]] = None # May appear in intermediate or last chunk\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nfrom typing import AsyncGenerator, Dict, Any\nimport json\n\nfrom models import DeepSearchChatParams\n\nlogger = logging.getLogger(__name__)\n\n# Default timeout for non-streaming requests (seconds)\n# Increased significantly due to potential long processing times, even though streaming is preferred.\nDEFAULT_TIMEOUT = 300.0\n# Timeout for establishing a connection\nCONNECT_TIMEOUT = 10.0\n\nclass JinaDeepSearchAPIClient:\n    \"\"\"Asynchronous client for interacting with the Jina DeepSearch API.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the API client, loading configuration from environment variables.\"\"\"\n        self.api_key = os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"JINA_API_KEY environment variable not set.\")\n\n        self.base_url = \"https://deepsearch.jina.ai\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\", # Ensure JSON responses\n        }\n        self._client = None\n\n    async def get_client(self) -> httpx.AsyncClient:\n        \"\"\"Returns an initialized httpx.AsyncClient instance.\"\"\"\n        if self._client is None or self._client.is_closed:\n            logger.info(\"Initializing httpx.AsyncClient for Jina DeepSearch\")\n            # Separate timeouts for connect vs read/write\n            timeout_config = httpx.Timeout(DEFAULT_TIMEOUT, connect=CONNECT_TIMEOUT)\n            self._client = httpx.AsyncClient(\n                base_url=self.base_url,\n                headers=self.headers,\n                timeout=timeout_config,\n                event_hooks={'response': [self._log_response], 'request': [self._log_request]}\n            )\n        return self._client\n\n    async def _log_request(self, request: httpx.Request):\n        \"\"\"Logs outgoing request details (excluding sensitive headers).\"\"\"\n        # Avoid logging Authorization header\n        headers = {k: v for k, v in request.headers.items() if k.lower() != 'authorization'}\n        try:\n            content = json.loads(request.content) if request.content else \"<No Body>\"\n        except json.JSONDecodeError:\n            content = \"<Non-JSON Body>\"\n        logger.debug(f\"--> {request.method} {request.url}\\nHeaders: {headers}\\nBody: {content}\")\n\n    async def _log_response(self, response: httpx.Response):\n        \"\"\"Logs incoming response details.\"\"\"\n        # Ensure response content is read before logging if it's a streaming response\n        # This might interfere with streaming if not handled carefully, logging only status for streams.\n        if 'stream' in str(response.request.url):\n             logger.debug(f\"<-- {response.request.method} {response.request.url} - Status {response.status_code} (Streaming)\")\n        else:\n            await response.aread()\n            try:\n                content = response.json()\n            except json.JSONDecodeError:\n                content = response.text\n            logger.debug(f\"<-- {response.request.method} {response.request.url} - Status {response.status_code}\\nBody: {content}\")\n\n\n    async def chat_completion_stream(self, params: DeepSearchChatParams) -> AsyncGenerator[bytes, None]:\n        \"\"\"\n        Performs a chat completion request to Jina DeepSearch and streams the response.\n\n        Args:\n            params: The parameters for the chat completion request.\n\n        Yields:\n            Bytes representing Server-Sent Events (SSE).\n\n        Raises:\n            httpx.HTTPStatusError: If the API returns an error status code.\n            httpx.TimeoutException: If the request times out.\n            Exception: For other unexpected errors.\n        \"\"\"\n        client = await self.get_client()\n        endpoint = \"/v1/chat/completions\"\n        # Ensure stream is True for this method\n        payload = params.model_dump(exclude_none=True)\n        payload['stream'] = True\n\n        logger.info(f\"Initiating stream request to {self.base_url}{endpoint}\")\n        try:\n            async with client.stream(\"POST\", endpoint, json=payload) as response:\n                # Raise exceptions for 4xx/5xx responses immediately\n                if response.status_code >= 400:\n                    # Attempt to read body for error details, even on stream\n                    error_body = await response.aread()\n                    try:\n                        error_details = json.loads(error_body.decode())\n                    except json.JSONDecodeError:\n                        error_details = error_body.decode()\n                    logger.error(f\"API Error {response.status_code}: {error_details}\")\n                    response.raise_for_status() # Raise HTTPStatusError\n\n                # Stream the response content chunks\n                async for chunk in response.aiter_bytes():\n                    yield chunk\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n            raise  # Re-raise the original exception\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out: {e}\")\n            raise\n        except httpx.RequestError as e:\n            logger.error(f\"Request error: {e}\")\n            raise\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during chat completion stream: {e}\")\n            raise\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTP client.\"\"\"\n        if self._client and not self._client.is_closed:\n            await self._client.aclose()\n            logger.info(\"Closed httpx.AsyncClient for Jina DeepSearch\")\n        self._client = None\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, ToolContext\nfrom typing import Dict, Any, List, Optional\nimport logging\nimport asyncio\nimport os\nimport json\nfrom dotenv import load_dotenv\nimport httpx\n\nfrom models import (\n    DeepSearchChatParams,\n    ChatCompletionResponse,\n    ChatCompletionChunk,\n    ResponseMessage,\n    Choice,\n    Usage,\n    Annotation\n)\nfrom api import JinaDeepSearchAPIClient\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=os.getenv(\"LOG_LEVEL\", \"INFO\").upper(),\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"jina_deepsearch\",\n    description=\"Provides access to Jina DeepSearch, an AI agent that combines web searching, reading, and reasoning to answer complex questions requiring iterative investigation and up-to-date information. It is compatible with the OpenAI Chat API schema.\"\n)\n\n# Initialize API Client\ntry:\n    api_client = JinaDeepSearchAPIClient()\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize API client: {e}\")\n    # Allow MCP server to start, but tools will fail\n    api_client = None\n\nasync def parse_sse_chunk(line: str) -> Optional[ChatCompletionChunk]:\n    \"\"\"Parses a single line from an SSE stream into a ChatCompletionChunk.\"\"\"\n    if line.startswith('data: '):\n        data_str = line[len('data: '):].strip()\n        if data_str == \"[DONE]\":\n            return None\n        try:\n            data_json = json.loads(data_str)\n            return ChatCompletionChunk.model_validate(data_json)\n        except json.JSONDecodeError:\n            logger.warning(f\"Failed to decode JSON from SSE line: {data_str}\")\n        except Exception as e:\n            logger.warning(f\"Failed to validate SSE chunk: {e}, data: {data_str}\")\n    return None\n\nasync def aggregate_stream(stream: AsyncGenerator[bytes, None]) -> ChatCompletionResponse:\n    \"\"\"Aggregates SSE chunks from the stream into a final ChatCompletionResponse.\"\"\"\n    final_response_data = {\n        \"id\": None,\n        \"object\": \"chat.completion\",\n        \"created\": None,\n        \"model\": None,\n        \"choices\": [],\n        \"usage\": {},\n        \"visited_urls\": [],\n        \"read_urls\": []\n    }\n    accumulated_choices = {}\n    current_line = b''\n\n    async for chunk_bytes in stream:\n        current_line += chunk_bytes\n        # Process lines separated by \\n\\n, \\r\\n\\r\\n, or \\r\\r\n        while b'\\n\\n' in current_line or b'\\r\\n\\r\\n' in current_line or b'\\r\\r' in current_line:\n            if b'\\n\\n' in current_line:\n                line_bytes, current_line = current_line.split(b'\\n\\n', 1)\n            elif b'\\r\\n\\r\\n' in current_line:\n                line_bytes, current_line = current_line.split(b'\\r\\n\\r\\n', 1)\n            else: # \\r\\r\n                line_bytes, current_line = current_line.split(b'\\r\\r', 1)\n\n            line = line_bytes.decode('utf-8').strip()\n            if not line:\n                continue\n\n            # SSE format can have multiple lines per event, split by \\n or \\r\n            for single_line in line.replace('\\r\\n', '\\n').replace('\\r', '\\n').split('\\n'):\n                chunk = await parse_sse_chunk(single_line.strip())\n                if chunk:\n                    # Capture top-level fields from the first chunk\n                    if final_response_data[\"id\"] is None: final_response_data[\"id\"] = chunk.id\n                    if final_response_data[\"created\"] is None: final_response_data[\"created\"] = chunk.created\n                    if final_response_data[\"model\"] is None: final_response_data[\"model\"] = chunk.model\n\n                    # Aggregate choices\n                    for choice_chunk in chunk.choices:\n                        idx = choice_chunk.index\n                        if idx not in accumulated_choices:\n                            accumulated_choices[idx] = {\n                                \"index\": idx,\n                                \"message\": {\"role\": None, \"content\": \"\", \"annotations\": []},\n                                \"finish_reason\": None\n                            }\n\n                        delta = choice_chunk.delta\n                        if delta.role is not None:\n                            accumulated_choices[idx][\"message\"][\"role\"] = delta.role\n                        if delta.content is not None:\n                            accumulated_choices[idx][\"message\"][\"content\"] += delta.content\n                        if delta.annotations is not None:\n                            # Assuming annotations list replaces previous ones or appends?\n                            # OpenAI typically appends content, but other fields might replace.\n                            # Let's assume replacement for simplicity unless API docs specify otherwise.\n                            # If they are additive, logic needs adjustment.\n                            # Jina's example suggests annotations might come at the end or replace.\n                            # Let's try merging/appending carefully.\n                            if accumulated_choices[idx][\"message\"][\"annotations\"] is None:\n                                accumulated_choices[idx][\"message\"][\"annotations\"] = []\n                            # Simple append might duplicate if sent multiple times. Need smarter merge.\n                            # For now, let's just take the last non-null list.\n                            accumulated_choices[idx][\"message\"][\"annotations\"] = delta.annotations\n\n                        if choice_chunk.finish_reason is not None:\n                            accumulated_choices[idx][\"finish_reason\"] = choice_chunk.finish_reason\n\n                    # Capture usage, visited_urls, read_urls (often in the last chunk)\n                    if chunk.usage:\n                        final_response_data[\"usage\"] = chunk.usage.model_dump(exclude_none=True)\n                    if chunk.visited_urls:\n                        final_response_data[\"visited_urls\"] = chunk.visited_urls\n                    if chunk.read_urls:\n                        final_response_data[\"read_urls\"] = chunk.read_urls\n\n    # Assemble final response\n    final_choices = []\n    for idx in sorted(accumulated_choices.keys()):\n        choice_data = accumulated_choices[idx]\n        # Ensure annotations list is initialized if it remained None\n        if choice_data[\"message\"][\"annotations\"] is None:\n            choice_data[\"message\"][\"annotations\"] = []\n        final_choices.append(\n            Choice(\n                index=choice_data[\"index\"],\n                message=ResponseMessage.model_validate(choice_data[\"message\"]),\n                finish_reason=choice_data[\"finish_reason\"]\n            )\n        )\n\n    final_response_data[\"choices\"] = final_choices\n    final_response_data[\"usage\"] = Usage.model_validate(final_response_data[\"usage\"])\n\n    # Validate the final aggregated structure\n    try:\n        return ChatCompletionResponse.model_validate(final_response_data)\n    except Exception as e:\n        logger.error(f\"Failed to validate final aggregated response: {e}\\nData: {final_response_data}\")\n        raise ValueError(f\"Failed to construct final response: {e}\")\n\n@mcp.tool()\nasync def chat_completion(params: DeepSearchChatParams, context: ToolContext) -> Dict[str, Any]:\n    \"\"\"\n    Performs a deep search and reasoning process based on a conversation history\n    to generate a comprehensive answer. Suitable for complex questions requiring\n    iterative research, world-knowledge, or up-to-date information.\n    Handles streaming internally and returns the final aggregated response.\n\n    Args:\n        params: Parameters for the chat completion request, including messages,\n                model, and other options.\n        context: The MCP ToolContext.\n\n    Returns:\n        A dictionary representing the final aggregated ChatCompletionResponse.\n    \"\"\"\n    if not api_client:\n        logger.error(\"API client not initialized. Cannot call chat_completion.\")\n        return {\"error\": \"API client not initialized. Check JINA_API_KEY.\"}\n\n    logger.info(f\"Received chat_completion request for model {params.model}\")\n    # Ensure stream=True is used for the API call, as aggregation is handled here.\n    params.stream = True\n\n    try:\n        stream = api_client.chat_completion_stream(params)\n        aggregated_response = await aggregate_stream(stream)\n        logger.info(f\"Successfully aggregated stream for request ID: {aggregated_response.id}\")\n        return aggregated_response.model_dump(exclude_none=True)\n\n    except httpx.HTTPStatusError as e:\n        error_body = \"<Could not read error body>\"\n        try:\n            error_body = e.response.json() if e.response else str(e)\n        except Exception:\n            try:\n                 error_body = e.response.text if e.response else str(e)\n            except Exception:\n                 pass # Keep the default message\n        logger.error(f\"API returned an error: {e.status_code} - {error_body}\")\n        return {\"error\": f\"API Error: {e.status_code}\", \"details\": error_body}\n    except httpx.TimeoutException:\n        logger.error(\"API request timed out.\")\n        return {\"error\": \"Request timed out\"}\n    except httpx.RequestError as e:\n        logger.error(f\"Network error connecting to API: {e}\")\n        return {\"error\": f\"Network error: {e}\"}\n    except ValueError as e:\n        logger.error(f\"Data validation or aggregation error: {e}\")\n        return {\"error\": f\"Data error: {e}\"}\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred in chat_completion tool\")\n        return {\"error\": f\"An unexpected server error occurred: {str(e)}\"}\n\n@mcp.on_shutdown\nasync def shutdown():\n    \"\"\"Cleanly shuts down the API client when the MCP server stops.\"\"\"\n    if api_client:\n        await api_client.close()\n        logger.info(\"Jina DeepSearch API client closed.\")\n\nif __name__ == \"__main__\":\n    # Note: FastMCP's run() method handles the ASGI server setup.\n    # You might need to run this with uvicorn directly for more control:\n    # uvicorn main:mcp.app --host 0.0.0.0 --port 8000\n    logger.info(\"Starting Jina DeepSearch MCP Server\")\n    # mcp.run() # This is a simplified way, might not exist or work as expected.\n    # Use uvicorn programmatically or via command line.\n    import uvicorn\n    uvicorn.run(mcp.app, host=\"0.0.0.0\", port=int(os.getenv(\"PORT\", 8080)))\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\npydantic>=2.0,<3.0\nhttpx>=0.25.0,<0.28.0\npython-dotenv>=1.0.0\nuvicorn>=0.20.0 # For running the server\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key (Required)\n# Get your key from https://jina.ai/cloud/\nJINA_API_KEY=your_jina_api_key_here\n\n# Optional: Set log level (e.g., DEBUG, INFO, WARNING, ERROR)\n# LOG_LEVEL=INFO\n\n# Optional: Set the port the server runs on\n# PORT=8080\n"
    },
    {
      "name": "README.md",
      "content": "# Jina DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for interacting with the Jina DeepSearch API using FastMCP.\n\nJina DeepSearch is an AI agent designed to answer complex questions by combining web searching, reading, and reasoning capabilities. This MCP server exposes its functionality through a standard interface compatible with the OpenAI Chat API schema.\n\n## Features\n\n*   Provides access to Jina DeepSearch via the `chat_completion` tool.\n*   Handles conversation history, including multimodal inputs (text, images, documents via data URIs).\n*   Supports various parameters to control reasoning effort, token budget, domain filtering, and more.\n*   Manages API authentication using a Jina API key.\n*   Handles API streaming internally and returns the final aggregated response.\n*   Includes error handling, logging, and proper request/response modeling using Pydantic.\n\n## Service Name\n\n`jina_deepsearch`\n\n## Tools\n\n### 1. `chat_completion`\n\n*   **Description**: Performs a deep search and reasoning process based on a conversation history to generate a comprehensive answer. Suitable for complex questions requiring iterative research, world-knowledge, or up-to-date information. Handles streaming internally and returns the final aggregated response.\n*   **Input**: `DeepSearchChatParams` (See `models.py` for details)\n    *   `messages` (List[Message], **required**): Conversation history. Each message has `role` ('user' or 'assistant') and `content` (string or list of `MessageContentPart` for multimodal).\n    *   `model` (str, optional, default: `jina-deepsearch-v1`): Model ID.\n    *   `reasoning_effort` (str, optional, default: `medium`): 'low', 'medium', or 'high'.\n    *   `budget_tokens` (int, optional): Max tokens for the process.\n    *   `max_attempts` (int, optional): Max reasoning retries.\n    *   `no_direct_answer` (bool, optional, default: `false`): Force search steps.\n    *   `max_returned_urls` (int, optional): Max URLs in the final answer.\n    *   `structured_output` (Any, optional): JSON schema for structured output.\n    *   `good_domains` (List[str], optional): Prioritized domains.\n    *   `bad_domains` (List[str], optional): Excluded domains.\n    *   `only_domains` (List[str], optional): Exclusively included domains.\n    *   `stream` (bool): Although present in the model, the tool forces this to `true` internally for API calls and handles aggregation. The client should not set this.\n*   **Returns**: `ChatCompletionResponse` (Dictionary, see `models.py` for details)\n    *   `id` (str): Unique completion ID.\n    *   `object` (str): 'chat.completion'.\n    *   `created` (int): Timestamp.\n    *   `model` (str): Model used.\n    *   `choices` (List[Choice]): List containing the generated message (`ResponseMessage` with `role`, `content`, `annotations`) and `finish_reason`.\n    *   `usage` (Usage): Token usage statistics.\n    *   `visited_urls` (List[str], optional): URLs visited.\n    *   `read_urls` (List[str], optional): URLs read.\n\n## Authentication\n\nThis server requires a Jina AI API key.\n\n1.  Obtain an API key from [Jina AI Cloud](https://jina.ai/cloud/).\n2.  Set the `JINA_API_KEY` environment variable.\n\nThe server reads this key from the environment and includes it as a Bearer token in the `Authorization` header for all requests to the Jina DeepSearch API.\n\n## Setup\n\n1.  **Clone the repository (or save the generated files):**\n    ```bash\n    # If you have the files locally\n    cd /path/to/your/project\n    ```\n\n2.  **Create and activate a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure environment variables:**\n    *   Create a `.env` file in the project root directory.\n    *   Copy the contents of `.env.example` into `.env`.\n    *   Replace `your_jina_api_key_here` with your actual Jina API key.\n    ```env\n    # .env\n    JINA_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n    # Optional:\n    # LOG_LEVEL=DEBUG\n    # PORT=8080\n    ```\n\n## Running the Server\n\nUse Uvicorn to run the ASGI application defined in `main.py`:\n\n```bash\nuvicorn main:mcp.app --host 0.0.0.0 --port 8080 --reload\n```\n\n*   `--host 0.0.0.0`: Makes the server accessible on your network.\n*   `--port 8080`: Specifies the port (defaults to 8080 if `PORT` env var is not set).\n*   `--reload`: Automatically restarts the server when code changes (useful for development).\n\nThe server will start, and you can interact with it using an MCP client at `http://localhost:8080` (or the appropriate host/port).\n\n## Example Usage (Conceptual MCP Client)\n\n```python\nimport mcp.client\n\nasync def main():\n    # Connect to the running MCP server\n    client = mcp.client.Client(\"http://localhost:8080\")\n\n    try:\n        response = await client.tools.jina_deepsearch.chat_completion(\n            params={\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"What were the main announcements from the latest Apple event regarding the Vision Pro?\"}\n                ],\n                \"reasoning_effort\": \"high\"\n            }\n        )\n\n        print(\"Jina DeepSearch Response:\")\n        print(response)\n\n        # Access specific parts\n        if response and not response.get('error'):\n            answer = response.get('choices', [{}])[0].get('message', {}).get('content')\n            print(\"\\nAnswer:\", answer)\n            usage = response.get('usage')\n            print(\"\\nUsage:\", usage)\n            citations = response.get('choices', [{}])[0].get('message', {}).get('annotations')\n            if citations:\n                print(\"\\nCitations:\")\n                for anno in citations:\n                    if anno.get('type') == 'url_citation' and anno.get('url_citation'):\n                        print(f\"- {anno['url_citation'].get('title')}: {anno['url_citation'].get('url')}\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        await client.close()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n**Note:** The exact client usage depends on the specific MCP client library implementation.\n"
    }
  ]
}