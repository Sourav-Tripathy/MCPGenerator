from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Literal

# --- Type Definitions from Plan ---

class Message(BaseModel):
    """Represents a single message in the conversation."""
    role: Literal['user', 'assistant'] = Field(..., description="The role of the message author ('user' or 'assistant').")
    content: str = Field(..., description="The content of the message. Can be plain text or a data URI for images/documents.")

class UrlCitation(BaseModel):
    """Represents a citation linked to a URL found during the search."""
    title: Optional[str] = Field(None, description="The title of the cited web page.")
    exactQuote: Optional[str] = Field(None, alias="exactQuote", description="The exact quote from the source.")
    url: str = Field(..., description="The URL of the source.")
    dateTime: Optional[str] = Field(None, alias="dateTime", description="The timestamp when the URL was accessed or content was published (ISO 8601 format).")

# --- Input Model for chat_completion Tool ---

class DeepSearchChatInput(BaseModel):
    """Input model for the Jina DeepSearch chat completion tool."""
    messages: List[Message] = Field(..., description="A list of messages comprising the conversation so far. The last message should be from the user. Content can include text or data URIs for images/documents (up to 10MB).")
    model: str = Field("jina-deepsearch-v1", description="ID of the model to use.")
    stream: bool = Field(True, description="Whether to stream the response. Strongly recommended to keep true to avoid timeouts. The MCP tool will handle aggregation if true.")
    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field("medium", description="Constrains reasoning effort. Supported values: 'low', 'medium', 'high'.")
    budget_tokens: Optional[int] = Field(None, description="Maximum tokens allowed for the DeepSearch process. Overrides reasoning_effort.")
    max_attempts: Optional[int] = Field(None, description="Maximum number of retries for solving the problem. Overrides reasoning_effort.")
    no_direct_answer: Optional[bool] = Field(False, description="Forces search/thinking steps even for seemingly trivial queries.")
    max_returned_urls: Optional[int] = Field(None, description="Maximum number of URLs to include in the final answer.")
    structured_output: Optional[Dict[str, Any]] = Field(None, description="JSON schema to ensure the final answer matches the structure.")
    good_domains: Optional[List[str]] = Field(None, description="List of domains to prioritize for content retrieval.")
    bad_domains: Optional[List[str]] = Field(None, description="List of domains to strictly exclude from content retrieval.")
    only_domains: Optional[List[str]] = Field(None, description="List of domains to exclusively include in content retrieval.")

    class Config:
        # Ensure Pydantic uses the alias for serialization
        populate_by_name = True

# --- Output/Response Models ---

class Usage(BaseModel):
    """Token usage statistics."""
    prompt_tokens: int = Field(..., description="Number of tokens in the prompt.")
    completion_tokens: Optional[int] = Field(None, description="Number of tokens in the generated completion.") # Optional as it might not be present in all stream chunks
    total_tokens: int = Field(..., description="Total number of tokens used in the request (prompt + completion).")

class FinishDetails(BaseModel):
    """Details about why the generation finished, including citations."""
    type: str = Field(..., description="Reason the generation finished (e.g., 'stop', 'length').")
    stop: Optional[str] = Field(None, description="The stop sequence that triggered the finish, if any.")
    url_citations: Optional[List[UrlCitation]] = Field(None, alias="url_citations", description="List of URLs cited in the response.")

    class Config:
        populate_by_name = True

class ResponseMessage(BaseModel):
    """Message object within the response choice."""
    role: Literal['assistant'] = Field(..., description="Role of the message author.")
    content: Optional[str] = Field(None, description="The content of the message.") # Content can be None initially and built up

class Choice(BaseModel):
    """A single response choice."""
    index: int = Field(..., description="Index of the choice.")
    message: ResponseMessage = Field(..., description="The message generated by the model.")
    finish_details: Optional[FinishDetails] = Field(None, alias="finish_details", description="Details about why the generation finished.")
    # finish_reason is common in OpenAI spec, but DeepSearch uses finish_details
    # finish_reason: Optional[str] = Field(None, description="Reason the generation finished.")

    class Config:
        populate_by_name = True

class DeepSearchChatResponse(BaseModel):
    """The aggregated final response from the DeepSearch process."""
    id: str = Field(..., description="A unique identifier for the chat completion.")
    object: str = Field(..., description="The object type, which is always 'chat.completion'.")
    created: int = Field(..., description="The Unix timestamp (in seconds) of when the chat completion was created.")
    model: str = Field(..., description="The model used for the chat completion.")
    choices: List[Choice] = Field(..., description="A list of chat completion choices. Usually only one.")
    usage: Optional[Usage] = Field(None, description="Usage statistics for the completion request.")

# --- Models for Streaming Chunks (Internal use in API client) ---

class StreamChoiceDelta(BaseModel):
    """Delta content within a streaming choice."""
    role: Optional[Literal['assistant']] = Field(None)
    content: Optional[str] = Field(None)

class StreamChoice(BaseModel):
    """A single choice within a streaming chunk."""
    index: int
    delta: StreamChoiceDelta
    finish_details: Optional[FinishDetails] = Field(None, alias="finish_details")
    # finish_reason: Optional[str] = None # Not typically in delta, but in final chunk's choice

    class Config:
        populate_by_name = True

class DeepSearchStreamChunk(BaseModel):
    """Represents a single chunk received during streaming."""
    id: str
    object: str # e.g., 'chat.completion.chunk'
    created: int
    model: str
    choices: List[StreamChoice]
    usage: Optional[Usage] = None # Usually present only in the last chunk
