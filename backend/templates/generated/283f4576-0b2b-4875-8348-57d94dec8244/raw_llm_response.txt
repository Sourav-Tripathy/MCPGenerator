Here's the complete implementation of the MCP server for DeepSearch:

```json
{
    "files": {
        "main.py": "from fastapi import FastAPI\nfrom fastmcp import MCP, ToolResponse\nimport httpx\nimport os\nfrom typing import Optional, Dict, Any\nfrom pydantic import BaseModel\nimport json\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\napp = FastAPI()\nmcp = MCP(app)\n\nclass SearchRequest(BaseModel):\n    query: str\n    reasoning_effort: str = \"medium\"\n    stream: bool = False\n\nclass StreamSearchRequest(BaseModel):\n    query: str\n    reasoning_effort: str = \"medium\"\n\nclass StructuredOutputRequest(BaseModel):\n    query: str\n    schema: Dict[str, Any]\n\nclass FileSearchRequest(BaseModel):\n    file: str\n    query: str\n\nasync def make_deepsearch_request(\n    payload: Dict[str, Any], \n    stream: bool = False\n) -> Dict[str, Any]:\n    \"\"\"\n    Make a request to the DeepSearch API.\n    \n    Args:\n        payload: The request payload\n        stream: Whether to stream the response\n    \n    Returns:\n        The response from the API\n    \n    Raises:\n        httpx.HTTPStatusError: If the request fails\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {os.getenv('DEEPSEARCH_API_KEY')}\",\n        \"Content-Type\": \"application/json\"\n    }\n    \n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            \"https://deepsearch.jina.ai/v1/chat/completions\",\n            headers=headers,\n            json=payload,\n            timeout=60\n        )\n        \n        response.raise_for_status()\n        \n        if stream:\n            return {\"stream\": True, \"data\": response.text}\n        return response.json()\n\n@mcp.tool()\nasync def search(request: SearchRequest) -> ToolResponse:\n    \"\"\"\n    Perform a search query and return the result.\n    \n    Args:\n        request: The search request containing query and parameters\n    \n    Returns:\n        ToolResponse with the search results\n    \"\"\"\n    payload = {\n        \"model\": \"deepsearch-chat\",\n        \"messages\": [{\"role\": \"user\", \"content\": request.query}],\n        \"stream\": request.stream,\n        \"reasoning_effort\": request.reasoning_effort\n    }\n    \n    try:\n        result = await make_deepsearch_request(payload, request.stream)\n        return ToolResponse(\n            success=True,\n            data=result,\n            message=\"Search completed successfully\"\n        )\n    except httpx.HTTPStatusError as e:\n        return ToolResponse(\n            success=False,\n            data=None,\n            message=f\"Search failed: {str(e)}\"\n        )\n\n@mcp.tool()\nasync def stream_search(request: StreamSearchRequest) -> ToolResponse:\n    \"\"\"\n    Perform a search query with streaming enabled.\n    \n    Args:\n        request: The search request containing query and parameters\n    \n    Returns:\n        ToolResponse with the streaming results\n    \"\"\"\n    payload = {\n        \"model\": \"deepsearch-chat\",\n        \"messages\": [{\"role\": \"user\", \"content\": request.query}],\n        \"stream\": True,\n        \"reasoning_effort\": request.reasoning_effort\n    }\n    \n    try:\n        result = await make_deepsearch_request(payload, True)\n        return ToolResponse(\n            success=True,\n            data=result,\n            message=\"Streaming search initiated\"\n        )\n    except httpx.HTTPStatusError as e:\n        return ToolResponse(\n            success=False,\n            data=None,\n            message=f\"Stream search failed: {str(e)}\"\n        )\n\n@mcp.tool()\nasync def structured_output(request: StructuredOutputRequest) -> ToolResponse:\n    \"\"\"\n    Get the search result in a structured format.\n    \n    Args:\n        request: The request containing query and output schema\n    \n    Returns:\n        ToolResponse with the structured results\n    \"\"\"\n    payload = {\n        \"model\": \"deepsearch-chat\",\n        \"messages\": [{\"role\": \"user\", \"content\": request.query}],\n        \"response_format\": {\"type\": \"json_object\", \"schema\": request.schema}\n    }\n    \n    try:\n        result = await make_deepsearch_request(payload)\n        return ToolResponse(\n            success=True,\n            data=result,\n            message=\"Structured output generated\"\n        )\n    except httpx.HTTPStatusError as e:\n        return ToolResponse(\n            success=False,\n            data=None,\n            message=f\"Structured output failed: {str(e)}\"\n        )\n\n@mcp.tool()\nasync def file_search(request: FileSearchRequest) -> ToolResponse:\n    \"\"\"\n    Search within the content of an uploaded file.\n    \n    Args:\n        request: The request containing file path and query\n    \n    Returns:\n        ToolResponse with the search results from the file\n    \"\"\"\n    try:\n        with open(request.file, 'rb') as f:\n            file_content = f.read().decode('utf-8')\n            \n        payload = {\n            \"model\": \"deepsearch-chat\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": request.query},\n                {\"role\": \"user\", \"content\": file_content}\n            ]\n        }\n        \n        result = await make_deepsearch_request(payload)\n        return ToolResponse(\n            success=True,\n            data=result,\n            message=\"File search completed\"\n        )\n    except FileNotFoundError:\n        return ToolResponse(\n            success=False,\n            data=None,\n            message=f\"File not found: {request.file}\"\n        )\n    except httpx.HTTPStatusError as e:\n        return ToolResponse(\n            success=False,\n            data=None,\n            message=f\"File search failed: {str(e)}\"\n        )\n    except Exception as e:\n        return ToolResponse(\n            success=False,\n            data=None,\n            message=f\"An error occurred: {str(e)}\"\n        )\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)",
        "requirements.txt": "fastapi\nfastmcp\nhttpx\npython-dotenv\npydantic\nuvicorn",
        ".env.example": "# DeepSearch API Key\nDEEPSEARCH_API_KEY=your_api_key_here",
        "README.md": "# DeepSearch MCP Server\n\nThis is an MCP server implementation for the DeepSearch API, providing tools for complex search operations.\n\n## Installation\n\n1. Clone this repository\n2. Create and activate a virtual environment:\n   ```\n   python -m venv venv\n   source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n   ```\n3. Install dependencies:\n   ```\n   pip install -r requirements.txt\n   ```\n4. Create a `.env` file based on `.env.example` and add your DeepSearch API key\n\n## Usage\n\nStart the server:\n```\nuvicorn main:app --reload\n```\n\nThe server will be available at `http://localhost:8000`.\n\n## Tools\n\nThe server provides the following tools:\n\n1. **search**: Perform a search query\n   - Parameters: query, reasoning_effort (low/medium/high), stream (bool)\n\n2. **stream_search**: Perform a search with streaming\n   - Parameters: query, reasoning_effort\n\n3. **structured_output**: Get results in a specific JSON schema\n   - Parameters: query, schema (JSON schema)\n\n4. **file_search**: Search within a file\n   - Parameters: file (path), query\n\n## Authentication\n\nThe server requires a DeepSearch API key set in the `.env` file as `DEEPSEARCH_API_KEY`.\n\n## API Documentation\n\nOnce the server is running, you can access the interactive API documentation at `http://localhost:8000/docs`."
    }
}
```