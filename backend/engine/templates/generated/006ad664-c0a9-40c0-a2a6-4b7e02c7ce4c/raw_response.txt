{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Literal\n\n# --- Type Definitions from Plan ---\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant'] = Field(..., description=\"The role of the message author ('user' or 'assistant').\")\n    content: str = Field(..., description=\"The content of the message. Can be plain text or a data URI for images/files.\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"URL of the source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation (ISO format likely).\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation associated with the message content.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass ResponseMessage(BaseModel):\n    \"\"\"Represents the message content in the response.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None, description=\"Role of the message author (always 'assistant' in response).\") # Added based on OpenAI schema\n    content: Optional[str] = Field(None, description=\"The textual content of the response message.\") # Optional for streaming delta\n    type: Optional[str] = Field(None, description=\"Type of the content (e.g., 'text').\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"List of annotations, like URL citations.\")\n\nclass Choice(BaseModel):\n    \"\"\"Represents one possible completion choice.\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    delta: Optional[ResponseMessage] = Field(None, description=\"The message content delta (used in streaming).\")\n    message: Optional[ResponseMessage] = Field(None, description=\"The complete message content (used in non-streaming or final chunk).\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probability information (null in example).\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the generation finished (e.g., 'stop').\") # Optional as it might not be in every chunk\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Tokens in the input prompt.\")\n    completion_tokens: Optional[int] = Field(None, description=\"Tokens in the generated completion.\") # Added based on OpenAI schema\n    total_tokens: Optional[int] = Field(None, description=\"Total tokens used in the request.\") # Added based on OpenAI schema\n\n# --- Input Model Definition ---\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the DeepSearch chat_completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history between the user and the assistant. Can include text, images (webp, png, jpeg encoded as data URI), or files (txt, pdf encoded as data URI, up to 10MB).\")\n    model: str = Field(..., description=\"ID of the DeepSearch model to use (e.g., 'jina-deepsearch-v1').\")\n    stream: Optional[bool] = Field(True, description=\"Whether to stream back partial progress using server-sent events. Strongly recommended to be true to avoid timeouts. Defaults to true.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field('medium', description=\"Constrains effort on reasoning. Supported values: 'low', 'medium', 'high'. Defaults to 'medium'.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides reasoning_effort. Larger budgets may improve quality for complex queries.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Allows trying different reasoning approaches. Overrides reasoning_effort.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces the model to perform search/thinking steps even for seemingly trivial queries. Defaults to false.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer, sorted by relevance.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"A JSON schema object to ensure the final answer conforms to the specified structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"A list of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"A list of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"A list of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        # Define example for documentation generation if needed\n        schema_extra = {\n            \"example\": {\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n                ],\n                \"model\": \"jina-deepsearch-v1\",\n                \"stream\": False\n            }\n        }\n\n# --- Return Type Definition ---\n\nclass DeepSearchChatOutput(BaseModel):\n    \"\"\"The final aggregated response from the DeepSearch API, containing the generated answer, choices, usage statistics, and visited/read URLs. If streaming was enabled, this represents the final message combining all chunks.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(..., description=\"The object type, which is always 'chat.completion' or 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[Choice] = Field(..., description=\"A list of chat completion choices.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics for the completion request.\")\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process.\") # Specific to DeepSearch\n    read_urls: Optional[List[str]] = Field(None, description=\"List of URLs read during the search process.\") # Specific to DeepSearch\n\n    class Config:\n        # Define example for documentation generation if needed\n        schema_extra = {\n            \"example\": {\n                \"id\": \"chatcmpl-123\",\n                \"object\": \"chat.completion\",\n                \"created\": 1677652288,\n                \"model\": \"jina-deepsearch-v1\",\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"message\": {\n                            \"role\": \"assistant\",\n                            \"content\": \"The capital of France is Paris.\",\n                            \"annotations\": [\n                                {\n                                    \"type\": \"url_citation\",\n                                    \"url_citation\": {\n                                        \"url\": \"https://en.wikipedia.org/wiki/Paris\",\n                                        \"title\": \"Paris - Wikipedia\"\n                                    }\n                                }\n                            ]\n                        },\n                        \"finish_reason\": \"stop\"\n                    }\n                ],\n                \"usage\": {\n                    \"prompt_tokens\": 9,\n                    \"completion_tokens\": 12,\n                    \"total_tokens\": 21\n                },\n                \"visited_urls\": [\"https://en.wikipedia.org/wiki/Paris\"],\n                \"read_urls\": [\"https://en.wikipedia.org/wiki/Paris\"]\n            }\n        }\n"
    },
    {
      "name": "client.py",
      "content": "import httpx\nimport os\nimport logging\nfrom typing import Dict, Any, AsyncGenerator\nfrom .models import DeepSearchChatInput, DeepSearchChatOutput\nimport json\n\nlogger = logging.getLogger(__name__)\n\n# --- Custom Exceptions ---\n\nclass DeepSearchError(Exception):\n    \"\"\"Base exception for DeepSearch client errors.\"\"\"\n    def __init__(self, message: str, status_code: Optional[int] = None):\n        self.status_code = status_code\n        super().__init__(message)\n\nclass AuthenticationError(DeepSearchError):\n    \"\"\"Raised for 401/403 errors.\"\"\"\n    pass\n\nclass RateLimitError(DeepSearchError):\n    \"\"\"Raised for 429 errors.\"\"\"\n    pass\n\nclass BadRequestError(DeepSearchError):\n    \"\"\"Raised for 400 errors.\"\"\"\n    pass\n\nclass APIError(DeepSearchError):\n    \"\"\"Raised for 5xx server errors.\"\"\"\n    pass\n\nclass TimeoutError(DeepSearchError):\n    \"\"\"Raised for request timeouts.\"\"\"\n    pass\n\nclass ConnectionError(DeepSearchError):\n    \"\"\"Raised for network connection errors.\"\"\"\n    pass\n\n# --- API Client ---\n\nclass DeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = 120.0):\n        \"\"\"\n        Initializes the DeepSearchClient.\n\n        Args:\n            api_key: Jina API key. Defaults to JINA_API_KEY environment variable.\n            base_url: Base URL for the DeepSearch API. Defaults to DEEPSEARCH_BASE_URL environment variable or 'https://deepsearch.jina.ai'.\n            timeout: Request timeout in seconds. Defaults to 120.0 (especially important for non-streaming).\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API key is required. Set JINA_API_KEY environment variable or pass it during initialization.\")\n\n        self.base_url = base_url or os.getenv(\"DEEPSEARCH_BASE_URL\", \"https://deepsearch.jina.ai\")\n        self.endpoint = f\"{self.base_url.rstrip('/')}/v1/chat/completions\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Ensure we accept JSON responses\n        }\n        self.client = httpx.AsyncClient(\n            headers=self.headers,\n            timeout=timeout\n        )\n\n    async def _request(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Internal method to make non-streaming POST requests.\"\"\"\n        try:\n            response = await self.client.post(self.endpoint, json=payload)\n            response.raise_for_status() # Raises HTTPStatusError for 4xx/5xx\n            return response.json()\n        except httpx.TimeoutException as e:\n            logger.error(f\"DeepSearch request timed out: {e}\")\n            raise TimeoutError(f\"Request timed out after {self.client.timeout.read} seconds.\") from e\n        except httpx.HTTPStatusError as e:\n            status_code = e.response.status_code\n            error_detail = e.response.text\n            logger.error(f\"DeepSearch HTTP error: {status_code} - {error_detail}\")\n            if status_code in (401, 403):\n                raise AuthenticationError(f\"Authentication failed (Status {status_code}): {error_detail}\", status_code) from e\n            elif status_code == 429:\n                raise RateLimitError(f\"Rate limit exceeded (Status {status_code}): {error_detail}\", status_code) from e\n            elif status_code == 400:\n                raise BadRequestError(f\"Bad request (Status {status_code}): {error_detail}\", status_code) from e\n            elif 500 <= status_code < 600:\n                raise APIError(f\"DeepSearch server error (Status {status_code}): {error_detail}\", status_code) from e\n            else:\n                raise DeepSearchError(f\"HTTP error (Status {status_code}): {error_detail}\", status_code) from e\n        except httpx.RequestError as e:\n            logger.error(f\"DeepSearch connection error: {e}\")\n            raise ConnectionError(f\"Network error connecting to DeepSearch API: {e}\") from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during DeepSearch request: {e}\")\n            raise DeepSearchError(f\"An unexpected error occurred: {str(e)}\") from e\n\n    async def _stream_request(self, payload: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Internal method to make streaming POST requests.\"\"\"\n        try:\n            async with self.client.stream(\"POST\", self.endpoint, json=payload) as response:\n                # Check initial status before starting iteration\n                if response.status_code >= 400:\n                    error_detail = await response.aread()\n                    status_code = response.status_code\n                    logger.error(f\"DeepSearch HTTP error on stream start: {status_code} - {error_detail.decode()}\")\n                    if status_code in (401, 403):\n                        raise AuthenticationError(f\"Authentication failed (Status {status_code}): {error_detail.decode()}\", status_code)\n                    elif status_code == 429:\n                        raise RateLimitError(f\"Rate limit exceeded (Status {status_code}): {error_detail.decode()}\", status_code)\n                    elif status_code == 400:\n                        raise BadRequestError(f\"Bad request (Status {status_code}): {error_detail.decode()}\", status_code)\n                    elif 500 <= status_code < 600:\n                        raise APIError(f\"DeepSearch server error (Status {status_code}): {error_detail.decode()}\", status_code)\n                    else:\n                        raise DeepSearchError(f\"HTTP error (Status {status_code}): {error_detail.decode()}\", status_code)\n\n                # Process Server-Sent Events (SSE)\n                async for line in response.aiter_lines():\n                    if line.startswith(\"data:\"):\n                        data_str = line[len(\"data:\"):].strip()\n                        if data_str == \"[DONE]\":\n                            logger.info(\"DeepSearch stream finished.\")\n                            break\n                        try:\n                            chunk = json.loads(data_str)\n                            yield chunk\n                        except json.JSONDecodeError:\n                            logger.warning(f\"Failed to decode JSON chunk: {data_str}\")\n                    elif line:\n                        logger.debug(f\"Received non-data line: {line}\")\n\n        except httpx.TimeoutException as e:\n            logger.error(f\"DeepSearch stream request timed out: {e}\")\n            raise TimeoutError(f\"Stream request timed out after {self.client.timeout.read} seconds.\") from e\n        except httpx.RequestError as e:\n            logger.error(f\"DeepSearch stream connection error: {e}\")\n            raise ConnectionError(f\"Network error connecting to DeepSearch API during stream: {e}\") from e\n        except Exception as e:\n            # Catch potential errors during stream processing or initial connection\n            # Check if it's an HTTPStatusError potentially wrapped\n            if isinstance(e, httpx.HTTPStatusError):\n                 status_code = e.response.status_code\n                 error_detail = e.response.text\n                 logger.error(f\"DeepSearch HTTP error during stream: {status_code} - {error_detail}\")\n                 if status_code in (401, 403): raise AuthenticationError(f\"Authentication failed (Status {status_code}): {error_detail}\", status_code) from e # type: ignore\n                 if status_code == 429: raise RateLimitError(f\"Rate limit exceeded (Status {status_code}): {error_detail}\", status_code) from e # type: ignore\n                 if status_code == 400: raise BadRequestError(f\"Bad request (Status {status_code}): {error_detail}\", status_code) from e # type: ignore\n                 if 500 <= status_code < 600: raise APIError(f\"DeepSearch server error (Status {status_code}): {error_detail}\", status_code) from e # type: ignore\n                 raise DeepSearchError(f\"HTTP error (Status {status_code}): {error_detail}\", status_code) from e # type: ignore\n            else:\n                logger.error(f\"An unexpected error occurred during DeepSearch stream request: {e}\")\n                raise DeepSearchError(f\"An unexpected error occurred during stream: {str(e)}\") from e\n\n    async def chat_completion(self, params: DeepSearchChatInput) -> Dict[str, Any] | AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"\n        Performs iterative search, reading, and reasoning using the DeepSearch model.\n\n        Args:\n            params: Input parameters including messages, model, and other options.\n\n        Returns:\n            If stream=False, returns a dictionary representing the DeepSearchChatOutput.\n            If stream=True, returns an async generator yielding dictionaries for each chunk.\n\n        Raises:\n            AuthenticationError: Invalid or missing API key.\n            RateLimitError: Exceeded API request limits.\n            TimeoutError: Request timed out.\n            BadRequestError: Invalid input parameters or message format.\n            APIError: Server-side errors on the DeepSearch API.\n            ConnectionError: Network issues connecting to the API.\n            DeepSearchError: Other unexpected errors.\n        \"\"\"\n        payload = params.model_dump(exclude_none=True) # Use model_dump for Pydantic v2\n        logger.info(f\"Sending request to DeepSearch: model={params.model}, stream={params.stream}\")\n        # logger.debug(f\"Payload: {payload}\") # Be careful logging potentially large message content\n\n        if params.stream:\n            logger.info(\"Initiating streaming request.\")\n            return self._stream_request(payload)\n        else:\n            logger.info(\"Initiating non-streaming request.\")\n            result = await self._request(payload)\n            # Validate or parse the result into DeepSearchChatOutput if needed, but\n            # for simplicity, we return the raw dict for now.\n            # You could add: return DeepSearchChatOutput(**result).model_dump()\n            return result\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n        logger.info(\"DeepSearchClient closed.\")\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, ToolContext\nfrom typing import Dict, Any, AsyncGenerator, Union\nimport logging\nimport os\nfrom dotenv import load_dotenv\n\n# Import local modules\nfrom .models import DeepSearchChatInput, DeepSearchChatOutput, Message # Ensure models are imported\nfrom .client import DeepSearchClient, DeepSearchError, AuthenticationError, RateLimitError, BadRequestError, APIError, TimeoutError, ConnectionError\n\n# --- Initialization --- #\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP server for Jina AI's DeepSearch API, providing advanced search, reading, and reasoning capabilities.\"\n)\n\n# Initialize API Client\n# Consider adding error handling for client initialization if API key is missing\ntry:\n    api_client = DeepSearchClient()\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize DeepSearchClient: {e}\")\n    # Decide how to handle this - exit or run without a functional client?\n    # For now, we'll let it proceed but tools will fail.\n    api_client = None\n\n# --- MCP Tools --- #\n\n@mcp.tool()\nasync def chat_completion(params: DeepSearchChatInput, context: ToolContext) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:\n    \"\"\"\n    Performs iterative search, reading, and reasoning using the DeepSearch model.\n\n    Takes a conversation history and various parameters to control the search and reasoning process.\n    Returns a detailed response including the answer, citations, and usage statistics.\n    Supports both streaming (default) and non-streaming responses.\n\n    Args:\n        params (DeepSearchChatInput): Input parameters including messages, model, stream, etc.\n        context (ToolContext): The MCP tool context.\n\n    Returns:\n        Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:\n        - If stream=False, returns a dictionary conforming to DeepSearchChatOutput.\n        - If stream=True, returns an async generator yielding dictionary chunks.\n\n    Raises:\n        Will convert specific client errors into MCP-compatible error responses.\n    \"\"\"\n    if not api_client:\n        logger.error(\"DeepSearchClient is not initialized. Cannot perform chat completion.\")\n        # You might want to return a more structured error compatible with MCP/OpenAI schema\n        return {\"error\": \"DeepSearch client not initialized. Check API key configuration.\"}\n\n    logger.info(f\"Received chat_completion request for model {params.model}. Stream={params.stream}\")\n\n    try:\n        result = await api_client.chat_completion(params)\n\n        if params.stream:\n            # If streaming, return the async generator directly\n            logger.info(\"Streaming response started.\")\n            # The generator needs to be consumed by the caller (FastMCP handles this)\n            async def stream_wrapper():\n                try:\n                    async for chunk in result:\n                        yield chunk\n                    logger.info(\"Streaming response completed.\")\n                except DeepSearchError as e:\n                     logger.error(f\"Error during stream consumption: {type(e).__name__} - {e}\")\n                     # Yield an error chunk if possible, or handle appropriately\n                     # This part might be tricky depending on FastMCP's stream error handling\n                     yield {\"error\": f\"{type(e).__name__}: {str(e)}\", \"status_code\": getattr(e, 'status_code', None)}\n                except Exception as e:\n                    logger.error(f\"Unexpected error during stream consumption: {e}\")\n                    yield {\"error\": f\"Unexpected stream error: {str(e)}\"}\n\n            return stream_wrapper()\n        else:\n            # If not streaming, return the complete result dictionary\n            logger.info(\"Non-streaming response received.\")\n            # Optionally validate/parse with DeepSearchChatOutput here if needed\n            # parsed_output = DeepSearchChatOutput(**result)\n            # return parsed_output.model_dump()\n            return result\n\n    except AuthenticationError as e:\n        logger.error(f\"Authentication Error: {e}\")\n        # Return an error structure compatible with OpenAI API errors if possible\n        return {\"error\": {\"message\": str(e), \"type\": \"authentication_error\", \"code\": e.status_code}}\n    except RateLimitError as e:\n        logger.error(f\"Rate Limit Error: {e}\")\n        return {\"error\": {\"message\": str(e), \"type\": \"rate_limit_error\", \"code\": e.status_code}}\n    except BadRequestError as e:\n        logger.error(f\"Bad Request Error: {e}\")\n        return {\"error\": {\"message\": str(e), \"type\": \"invalid_request_error\", \"code\": e.status_code}}\n    except TimeoutError as e:\n        logger.error(f\"Timeout Error: {e}\")\n        return {\"error\": {\"message\": str(e), \"type\": \"timeout_error\", \"code\": 504}} # Use 504 for timeout\n    except APIError as e:\n        logger.error(f\"API Error: {e}\")\n        return {\"error\": {\"message\": str(e), \"type\": \"api_error\", \"code\": e.status_code}}\n    except ConnectionError as e:\n        logger.error(f\"Connection Error: {e}\")\n        return {\"error\": {\"message\": str(e), \"type\": \"connection_error\", \"code\": 503}} # Use 503 for connection issues\n    except DeepSearchError as e:\n        logger.error(f\"DeepSearch Error: {e}\")\n        return {\"error\": {\"message\": str(e), \"type\": \"deepsearch_error\", \"code\": getattr(e, 'status_code', 500)}}\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred in chat_completion tool.\") # Log full traceback\n        return {\"error\": {\"message\": f\"An unexpected server error occurred: {str(e)}\", \"type\": \"internal_server_error\", \"code\": 500}}\n\n# --- Lifecycle Hooks --- #\n\n@mcp.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Cleanly close the API client on server shutdown.\"\"\"\n    if api_client:\n        await api_client.close()\n    logger.info(\"DeepSearch MCP server shutting down.\")\n\n# --- Run Server --- #\n\nif __name__ == \"__main__\":\n    # Note: To run with Uvicorn for production:\n    # uvicorn main:mcp.app --host 0.0.0.0 --port 8000\n    # FastMCP's run() is mainly for development/simplicity\n    logger.info(\"Starting DeepSearch MCP server...\")\n    mcp.run()\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp\nhttpx>=0.25.0\npydantic>=2.0.0\npython-dotenv\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina API Key for DeepSearch Authentication\n# Get your key from https://jina.ai/cloud/\nJINA_API_KEY=\"YOUR_JINA_API_KEY_HERE\"\n\n# Base URL for the DeepSearch API (Optional)\n# Defaults to https://deepsearch.jina.ai if not set\n# DEEPSEARCH_BASE_URL=\"https://deepsearch.jina.ai\"\n"
    },
    {
      "name": "README.md",
      "content": "# DeepSearch MCP Server\n\nThis project provides a Model Context Protocol (MCP) server for Jina AI's DeepSearch API, implemented using FastMCP.\n\nDeepSearch offers advanced search, reading, and reasoning capabilities to answer complex questions by iteratively searching the web. This MCP server exposes the DeepSearch functionality through an OpenAI-compatible Chat Completions API schema.\n\n## Features\n\n*   Provides the `chat_completion` tool mirroring the DeepSearch `/v1/chat/completions` endpoint.\n*   Supports all DeepSearch parameters (streaming, reasoning effort, domain filtering, structured output, etc.).\n*   Handles authentication using Jina API keys.\n*   Includes robust error handling for API errors, timeouts, rate limits, and connection issues.\n*   Supports streaming responses using Server-Sent Events (SSE).\n*   Built with FastMCP for easy integration into MCP ecosystems.\n\n## Prerequisites\n\n*   Python 3.8+\n*   A Jina AI API Key (get one from [Jina AI Cloud](https://jina.ai/cloud/))\n\n## Setup\n\n1.  **Clone the repository (or create the files):**\n    ```bash\n    # If you have the code in a directory\n    cd /path/to/your/deepsearch-mcp\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    Create a `.env` file in the project root directory by copying the example:\n    ```bash\n    cp .env.example .env\n    ```\n    Edit the `.env` file and add your Jina API key:\n    ```env\n    JINA_API_KEY=\"YOUR_JINA_API_KEY_HERE\"\n    # DEEPSEARCH_BASE_URL=\"https://deepsearch.jina.ai\" # Optional: uncomment to override default\n    ```\n\n## Running the Server\n\nYou can run the server directly using Python for development:\n\n```bash\npython main.py\n```\n\nFor production deployments, it's recommended to use an ASGI server like Uvicorn:\n\n```bash\n# Make sure uvicorn is installed: pip install uvicorn\nuvicorn main:mcp.app --host 0.0.0.0 --port 8000 --reload # --reload is for development\n```\n\nThe server will be available at `http://localhost:8000` (or the host/port you specify).\n\n## Available Tools\n\nThis MCP server exposes one primary tool:\n\n### `chat_completion`\n\n*   **Description:** Performs iterative search, reading, and reasoning using the DeepSearch model to answer user queries. Takes a conversation history and various parameters to control the search and reasoning process. Returns a detailed response including the answer, citations, and usage statistics.\n*   **Input Model:** `DeepSearchChatInput` (see `models.py` for details)\n    *   `messages` (List[Message]): Conversation history (required).\n    *   `model` (str): DeepSearch model ID (required).\n    *   `stream` (Optional[bool]): Enable streaming (default: True).\n    *   `reasoning_effort` (Optional[str]): 'low', 'medium', 'high' (default: 'medium').\n    *   `budget_tokens` (Optional[int]): Max token budget.\n    *   `max_attempts` (Optional[int]): Max retry attempts.\n    *   `no_direct_answer` (Optional[bool]): Force search steps (default: False).\n    *   `max_returned_urls` (Optional[int]): Max URLs in the final answer.\n    *   `structured_output` (Optional[Dict]): JSON schema for output structure.\n    *   `good_domains` (Optional[List[str]]): Prioritized domains.\n    *   `bad_domains` (Optional[List[str]]): Excluded domains.\n    *   `only_domains` (Optional[List[str]]): Exclusively included domains.\n*   **Returns:** `DeepSearchChatOutput` (Dictionary or AsyncGenerator[Dict]) - OpenAI-compatible chat completion response, potentially including `visited_urls` and `read_urls`.\n\n## Example Usage (using `curl`)\n\n**Non-Streaming:**\n\n```bash\ncurl -X POST http://localhost:8000/tools/chat_completion \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"params\": {\n    \"model\": \"jina-deepsearch-v1\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What are the main benefits of using Jina AI?\"}\n    ],\n    \"stream\": false\n  }\n}'\n```\n\n**Streaming:**\n\n```bash\ncurl -X POST http://localhost:8000/tools/chat_completion \\\n-H \"Content-Type: application/json\" \\\n-H \"Accept: text/event-stream\" \\\n-d '{\n  \"params\": {\n    \"model\": \"jina-deepsearch-v1\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Explain the concept of Retrieval-Augmented Generation (RAG).\"}\n    ],\n    \"stream\": true\n  }\n}'\n```\n\n(Note: The streaming request will return Server-Sent Events.)\n\n## Error Handling\n\nThe server maps errors from the DeepSearch API and the client itself to structured JSON error responses, often mimicking the OpenAI error schema where applicable:\n\n*   `AuthenticationError` (401/403)\n*   `RateLimitError` (429)\n*   `BadRequestError` (400)\n*   `TimeoutError` (504)\n*   `APIError` (5xx)\n*   `ConnectionError` (503)\n*   Other `DeepSearchError` or unexpected errors (500)\n\n## Project Structure\n\n```\n.\n├── .env.example        # Example environment variables\n├── .env                # Actual environment variables (ignored by git)\n├── client.py           # DeepSearch API client implementation\n├── main.py             # FastMCP server application\n├── models.py           # Pydantic models for API requests/responses\n├── README.md           # This documentation file\n└── requirements.txt    # Python dependencies\n```\n"
    }
  ]
}