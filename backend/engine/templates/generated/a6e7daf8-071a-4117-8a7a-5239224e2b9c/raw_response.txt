{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Union, Literal\nimport time\n\n# --- Type Definitions from Plan ---\n\nclass MessageContentPart(BaseModel):\n    \"\"\"Represents a part of a multimodal message content.\"\"\"\n    type: str # e.g., 'text', 'image_url'\n    text: Optional[str] = None\n    image_url: Optional[Dict[str, str]] = None # e.g., {\"url\": \"data:image/jpeg;base64,...\"}\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant', 'system'] = Field(..., description=\"The role of the message author.\")\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"The content of the message. Can be plain text or a list for multimodal content.\")\n    name: Optional[str] = Field(None, description=\"An optional name for the participant.\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation within the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"The URL of the source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation within the response content.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\n# --- Input Model ---\n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the chat_completion tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history. The last message should be the user's query.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(True, description=\"Whether to stream back partial progress. Strongly recommended.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains reasoning effort ('low', 'medium', 'high').\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides reasoning_effort.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Overrides reasoning_effort.\")\n    no_direct_answer: bool = Field(False, description=\"Forces search/thinking steps even for seemingly trivial queries.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer/chunk.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"Enables structured output matching a supplied JSON schema.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        use_enum_values = True # Ensure Literal values are sent as strings\n\n# --- Output Models (Mirroring OpenAI Schema) ---\n\n# Streaming Response Models\nclass DeepSearchChatCompletionChoiceDelta(BaseModel):\n    \"\"\"Delta content for a streaming chat completion chunk.\"\"\"\n    content: Optional[str] = None\n    role: Optional[Literal['assistant']] = None\n    # DeepSearch might include other fields like annotations here\n    annotations: Optional[List[Annotation]] = None\n\nclass DeepSearchChatCompletionChunkChoice(BaseModel):\n    \"\"\"A choice within a streaming chat completion chunk.\"\"\"\n    delta: DeepSearchChatCompletionChoiceDelta\n    index: int\n    finish_reason: Optional[Literal['stop', 'length', 'content_filter', 'tool_calls', 'error']] = None\n    # DeepSearch might include logprobs here\n\nclass DeepSearchUsage(BaseModel):\n    \"\"\"Token usage statistics.\"\"\"\n    prompt_tokens: Optional[int] = 0\n    completion_tokens: Optional[int] = 0\n    total_tokens: Optional[int] = 0\n\nclass DeepSearchChatCompletionChunk(BaseModel):\n    \"\"\"Represents a chunk of the streaming chat completion response.\"\"\"\n    id: str\n    object: str = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[DeepSearchChatCompletionChunkChoice]\n    usage: Optional[DeepSearchUsage] = None # Usually present only in the last chunk\n    # DeepSearch might include system_fingerprint or other fields\n\n# Non-Streaming Response Models\nclass DeepSearchChatCompletionChoice(BaseModel):\n    \"\"\"A choice in a non-streaming chat completion response.\"\"\"\n    message: Message\n    index: int\n    finish_reason: Optional[Literal['stop', 'length', 'content_filter', 'tool_calls', 'error']] = None\n    # DeepSearch might include logprobs or annotations here\n\nclass DeepSearchChatCompletion(BaseModel):\n    \"\"\"Represents the full response for a non-streaming chat completion.\"\"\"\n    id: str\n    object: str = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[DeepSearchChatCompletionChoice]\n    usage: Optional[DeepSearchUsage] = None\n    # DeepSearch might include system_fingerprint or other fields\n"
    },
    {
      "name": "client.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nimport asyncio\nfrom typing import AsyncGenerator, Union, Optional\nfrom pydantic import ValidationError\n\nfrom models import (\n    DeepSearchChatInput,\n    DeepSearchChatCompletion,\n    DeepSearchChatCompletionChunk,\n    DeepSearchUsage # Import Usage if needed for final chunk parsing\n)\n\nlogger = logging.getLogger(__name__)\n\n# Custom Exceptions\nclass DeepSearchError(Exception):\n    \"\"\"Base exception for DeepSearch client errors.\"\"\"\n    pass\n\nclass AuthenticationError(DeepSearchError):\n    \"\"\"Error for invalid or missing API key (401/403).\"\"\"\n    pass\n\nclass RateLimitError(DeepSearchError):\n    \"\"\"Error for exceeding rate limits (429).\"\"\"\n    pass\n\nclass BadRequestError(DeepSearchError):\n    \"\"\"Error for invalid input parameters (400).\"\"\"\n    pass\n\nclass TimeoutError(DeepSearchError):\n    \"\"\"Error for request timeouts.\"\"\"\n    pass\n\nclass InternalServerError(DeepSearchError):\n    \"\"\"Error for server-side issues (5xx).\"\"\"\n    pass\n\nclass NetworkError(DeepSearchError):\n    \"\"\"Error for network connectivity issues.\"\"\"\n    pass\n\nclass DeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = 180.0):\n        \"\"\"\n        Initializes the DeepSearchClient.\n\n        Args:\n            api_key: Jina API Key. Defaults to JINA_API_KEY environment variable.\n            base_url: Base URL for the DeepSearch API. Defaults to DEEPSEARCH_BASE_URL or https://deepsearch.jina.ai.\n            timeout: Default request timeout in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API Key must be provided via argument or JINA_API_KEY environment variable.\")\n\n        self.base_url = base_url or os.getenv(\"DEEPSEARCH_BASE_URL\", \"https://deepsearch.jina.ai\")\n        self.endpoint = \"/v1/chat/completions\"\n        self.timeout = timeout\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Ensure we accept JSON for non-streaming\n        }\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=self.timeout\n        )\n\n    async def _request(\n        self,\n        method: str,\n        endpoint: str,\n        payload: Optional[dict] = None,\n        stream: bool = False\n    ) -> Union[httpx.Response, AsyncGenerator[str, None]]:\n        \"\"\"Makes an HTTP request to the DeepSearch API, handling potential errors.\"\"\"\n        try:\n            if stream:\n                # For streaming, we need to handle the response differently\n                req = self.client.build_request(method, endpoint, json=payload)\n                response_stream = await self.client.send(req, stream=True)\n                # Raise status errors early for non-2xx responses before streaming\n                response_stream.raise_for_status()\n                return response_stream # Return the stream object\n            else:\n                response = await self.client.request(method, endpoint, json=payload)\n                response.raise_for_status() # Raises HTTPStatusError for 4xx/5xx\n                return response\n\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out: {e}\")\n            raise TimeoutError(f\"Request timed out after {self.timeout} seconds.\") from e\n        except httpx.RequestError as e:\n            logger.error(f\"Network error occurred: {e}\")\n            # E.g., DNS resolution failure, connection refused\n            raise NetworkError(f\"Network error: {e}\") from e\n        except httpx.HTTPStatusError as e:\n            status_code = e.response.status_code\n            response_text = e.response.text\n            logger.error(f\"HTTP error {status_code}: {response_text}\")\n            if status_code in (401, 403):\n                raise AuthenticationError(f\"Authentication failed ({status_code}): {response_text}\") from e\n            elif status_code == 429:\n                raise RateLimitError(f\"Rate limit exceeded ({status_code}): {response_text}\") from e\n            elif status_code == 400:\n                raise BadRequestError(f\"Bad request ({status_code}): {response_text}\") from e\n            elif 500 <= status_code < 600:\n                # Handle 524 specifically if needed, though httpx might raise TimeoutException earlier\n                if status_code == 524:\n                     raise TimeoutError(f\"Server timeout ({status_code}): {response_text}\") from e\n                raise InternalServerError(f\"Server error ({status_code}): {response_text}\") from e\n            else:\n                raise DeepSearchError(f\"Unhandled HTTP error ({status_code}): {response_text}\") from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during the request: {e}\")\n            raise DeepSearchError(f\"An unexpected error occurred: {e}\") from e\n\n    async def _process_sse_stream(\n        self, response_stream: httpx.Response\n    ) -> AsyncGenerator[DeepSearchChatCompletionChunk, None]:\n        \"\"\"Processes a Server-Sent Events (SSE) stream and yields parsed chunks.\"\"\"\n        buffer = \"\"\n        async with response_stream:\n            async for line in response_stream.aiter_lines():\n                if not line.strip(): # Skip empty lines used as separators\n                    continue\n                if line.startswith(\":\"): # Skip comments\n                    continue\n\n                buffer += line\n                # Check if the line indicates the end of a data block (often starts with 'data:')\n                # and process complete messages (might span multiple lines)\n                if line.startswith(\"data:\"): # Process data lines\n                    data_content = line[len(\"data:\"):].strip()\n                    if data_content == \"[DONE]\":\n                        logger.info(\"Stream finished with [DONE] signal.\")\n                        break\n                    try:\n                        chunk_data = json.loads(data_content)\n                        yield DeepSearchChatCompletionChunk.parse_obj(chunk_data)\n                    except json.JSONDecodeError:\n                        logger.warning(f\"Failed to decode JSON from SSE data: {data_content}\")\n                        # Continue, maybe part of a larger message, or log and skip\n                    except ValidationError as e:\n                        logger.warning(f\"Failed to validate SSE chunk: {e}. Data: {data_content}\")\n                        # Decide whether to yield anyway or skip\n                    except Exception as e:\n                        logger.error(f\"Error processing SSE chunk: {e}. Data: {data_content}\")\n                        # Decide how to handle unexpected errors during chunk processing\n\n                # Reset buffer or handle multi-line data if necessary based on API specifics\n                # Assuming here each 'data:' line is a complete JSON object for simplicity\n\n    async def chat_completion(\n        self, params: DeepSearchChatInput\n    ) -> Union[AsyncGenerator[DeepSearchChatCompletionChunk, None], DeepSearchChatCompletion]:\n        \"\"\"\n        Performs a deep search chat completion.\n\n        Args:\n            params: Input parameters for the chat completion.\n\n        Returns:\n            If stream=True, an async generator yielding DeepSearchChatCompletionChunk objects.\n            If stream=False, a DeepSearchChatCompletion object.\n\n        Raises:\n            AuthenticationError, RateLimitError, BadRequestError,\n            TimeoutError, InternalServerError, NetworkError, DeepSearchError\n        \"\"\"\n        payload = params.dict(exclude_none=True) # Exclude optional fields not provided\n        stream = params.stream\n\n        logger.info(f\"Sending chat completion request (stream={stream}) to {self.endpoint}\")\n        # Avoid logging sensitive message content in production if necessary\n        # logger.debug(f\"Request payload: {payload}\")\n\n        response_or_stream = await self._request(\"POST\", self.endpoint, payload=payload, stream=stream)\n\n        if stream:\n            if not isinstance(response_or_stream, httpx.Response):\n                 # Should not happen based on _request logic, but type checking helps\n                 raise DeepSearchError(\"Expected httpx.Response for streaming, got something else.\")\n            logger.info(\"Processing SSE stream...\")\n            # Return the async generator directly\n            return self._process_sse_stream(response_or_stream)\n        else:\n            if not isinstance(response_or_stream, httpx.Response):\n                 raise DeepSearchError(\"Expected httpx.Response for non-streaming, got something else.\")\n            try:\n                response_data = response_or_stream.json()\n                logger.info(\"Received non-streaming response.\")\n                # logger.debug(f\"Response data: {response_data}\")\n                return DeepSearchChatCompletion.parse_obj(response_data)\n            except json.JSONDecodeError as e:\n                logger.error(f\"Failed to decode JSON response: {e}. Response text: {response_or_stream.text}\")\n                raise DeepSearchError(f\"Failed to decode JSON response: {e}\") from e\n            except ValidationError as e:\n                logger.error(f\"Failed to validate response data: {e}. Response data: {response_data}\")\n                raise DeepSearchError(f\"Response validation failed: {e}\") from e\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, StreamingToolResponse\nfrom typing import Dict, Any, Optional, List, Union, AsyncGenerator\nimport logging\nimport os\nfrom dotenv import load_dotenv\n\n# Import models and client\nfrom models import (\n    DeepSearchChatInput,\n    DeepSearchChatCompletion,\n    DeepSearchChatCompletionChunk\n)\nfrom client import (\n    DeepSearchClient,\n    DeepSearchError,\n    AuthenticationError,\n    RateLimitError,\n    BadRequestError,\n    TimeoutError,\n    InternalServerError,\n    NetworkError\n)\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Initialize MCP Server\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP service for Jina AI's DeepSearch API. Provides advanced search capabilities by combining web searching, reading, and reasoning to answer complex questions. It is designed to be compatible with the OpenAI Chat API schema.\"\n)\n\n# Initialize DeepSearch API Client\ntry:\n    deepsearch_client = DeepSearchClient()\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize DeepSearchClient: {e}\")\n    # Optionally exit or prevent server start if client init fails\n    # exit(1)\n    deepsearch_client = None # Allow server to start but tools will fail\n\n@mcp.tool()\nasync def chat_completion(params: DeepSearchChatInput) -> Union[DeepSearchChatCompletion, StreamingToolResponse]:\n    \"\"\"\n    Performs a deep search and reasoning process based on a series of messages.\n    It iteratively searches the web, reads content, and reasons to find the best answer\n    to the user's query. Supports streaming responses.\n\n    Args:\n        params: Input parameters matching the DeepSearchChatInput model.\n\n    Returns:\n        If stream=True, a StreamingToolResponse yielding DeepSearchChatCompletionChunk objects.\n        If stream=False, a DeepSearchChatCompletion object.\n\n    Raises:\n        MCP specific errors mapping from client exceptions.\n    \"\"\"\n    if not deepsearch_client:\n        logger.error(\"DeepSearchClient is not initialized. Check API key configuration.\")\n        # You might want to raise a specific MCP error here\n        # For now, returning an error dictionary\n        return {\"error\": \"DeepSearch client not initialized. Check JINA_API_KEY.\"} # Or raise an appropriate MCP exception\n\n    try:\n        logger.info(f\"Received chat_completion request. Streaming: {params.stream}\")\n        result = await deepsearch_client.chat_completion(params)\n\n        if params.stream:\n            if isinstance(result, AsyncGenerator):\n                logger.info(\"Returning streaming response.\")\n                # Wrap the async generator in StreamingToolResponse\n                async def generator_wrapper():\n                    try:\n                        async for chunk in result:\n                            yield chunk.dict() # Yield dictionary representation for MCP\n                    except DeepSearchError as e:\n                        logger.error(f\"Error during stream processing: {e}\")\n                        # Yield an error chunk or handle differently\n                        yield {\"error\": str(e), \"type\": type(e).__name__}\n                    except Exception as e:\n                        logger.error(f\"Unexpected error during stream processing: {e}\", exc_info=True)\n                        yield {\"error\": f\"Unexpected stream error: {str(e)}\", \"type\": \"UnexpectedStreamError\"}\n\n                return StreamingToolResponse(content=generator_wrapper())\n            else:\n                # This case should ideally not happen if the client logic is correct\n                logger.error(\"Expected an async generator for streaming, but got a different type.\")\n                return {\"error\": \"Internal server error: Unexpected response type for streaming.\"} # Or raise\n        else:\n            if isinstance(result, DeepSearchChatCompletion):\n                logger.info(\"Returning non-streaming response.\")\n                return result.dict() # Return dictionary representation for MCP\n            else:\n                # This case should ideally not happen\n                logger.error(\"Expected a DeepSearchChatCompletion object for non-streaming, but got a different type.\")\n                return {\"error\": \"Internal server error: Unexpected response type for non-streaming.\"} # Or raise\n\n    # Map specific client errors to potential MCP error responses or logged events\n    except AuthenticationError as e:\n        logger.error(f\"Authentication Error: {e}\")\n        return {\"error\": str(e), \"type\": \"AuthenticationError\"}\n    except RateLimitError as e:\n        logger.error(f\"Rate Limit Error: {e}\")\n        return {\"error\": str(e), \"type\": \"RateLimitError\"}\n    except BadRequestError as e:\n        logger.error(f\"Bad Request Error: {e}\")\n        return {\"error\": str(e), \"type\": \"BadRequestError\"}\n    except TimeoutError as e:\n        logger.error(f\"Timeout Error: {e}\")\n        return {\"error\": str(e), \"type\": \"TimeoutError\"}\n    except InternalServerError as e:\n        logger.error(f\"Internal Server Error: {e}\")\n        return {\"error\": str(e), \"type\": \"InternalServerError\"}\n    except NetworkError as e:\n        logger.error(f\"Network Error: {e}\")\n        return {\"error\": str(e), \"type\": \"NetworkError\"}\n    except DeepSearchError as e:\n        logger.error(f\"DeepSearch API Error: {e}\")\n        return {\"error\": str(e), \"type\": \"DeepSearchError\"}\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred in the chat_completion tool.\") # Log full traceback\n        return {\"error\": f\"An unexpected internal error occurred: {str(e)}\", \"type\": \"UnexpectedError\"}\n\n# Run the MCP server\nif __name__ == \"__main__\":\n    # You can configure host and port here or via environment variables\n    # Uvicorn is typically used for production runs, e.g.:\n    # uvicorn main:mcp.app --host 0.0.0.0 --port 8000 --reload\n    logger.info(\"Starting MCP server for DeepSearch...\")\n    # FastMCP's run() is simple, for more control use uvicorn directly\n    # mcp.run() # This might block in ways not ideal for async; direct uvicorn is better\n\n    # To run programmatically (though CLI is standard):\n    import uvicorn\n    uvicorn.run(mcp.app, host=\"0.0.0.0\", port=int(os.getenv(\"PORT\", 8000)))\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.2.0 # Use latest or a specific compatible version\npydantic>=1.8,<2.0 # Ensure compatibility with FastMCP version\nhttpx>=0.23.0 # Use a recent version\npython-dotenv>=0.19.0\nuvicorn>=0.15.0 # For running the ASGI app\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI DeepSearch API Configuration\n\n# Obtain your API key from Jina AI\nJINA_API_KEY=your_jina_api_key_here\n\n# Optional: Override the default DeepSearch API base URL\n# DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai\n\n# Optional: Port for the MCP server\n# PORT=8000\n"
    },
    {
      "name": "README.md",
      "content": "# Jina AI DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for interacting with the Jina AI DeepSearch API using FastMCP.\n\n## Description\n\nThe Jina AI DeepSearch API provides advanced search capabilities by combining web searching, reading, and reasoning to answer complex questions. This MCP server exposes the DeepSearch functionality through a standardized tool interface, making it easy to integrate into MCP-compatible applications and agents.\n\nThe server is designed to be compatible with the OpenAI Chat API schema for inputs and outputs.\n\n## Features\n\n*   **`chat_completion` Tool:**\n    *   Performs a deep search and reasoning process based on a conversation history.\n    *   Supports streaming responses (Server-Sent Events) for real-time updates (recommended).\n    *   Supports non-streaming responses (single JSON object).\n    *   Configurable parameters like `model`, `reasoning_effort`, `max_attempts`, domain filtering, and structured output.\n\n## Setup\n\n1.  **Clone the Repository:**\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2.  **Create a Virtual Environment:**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install Dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure Environment Variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina AI API key:\n        ```dotenv\n        JINA_API_KEY=your_jina_api_key_here\n        ```\n    *   You can also optionally set `DEEPSEARCH_BASE_URL` and `PORT` in the `.env` file.\n\n## Running the Server\n\nYou can run the MCP server using Uvicorn:\n\n```bash\n# Basic run\nuvicorn main:mcp.app\n\n# Run with auto-reload (for development)\nuvicorn main:mcp.app --reload\n\n# Specify host and port\nuvicorn main:mcp.app --host 0.0.0.0 --port 8000\n```\n\nThe server will start, and you should see log output indicating it's running.\n\n## Usage\n\nOnce the server is running, you can interact with it using any MCP client or standard HTTP requests.\n\n**Example Request (using `curl`):**\n\nThis example calls the `chat_completion` tool with streaming enabled.\n\n```bash\ncurl -X POST http://localhost:8000/tools/chat_completion/invoke \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"params\": {\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What were the main causes of the French Revolution?\"}\n    ],\n    \"model\": \"jina-deepsearch-v1\",\n    \"stream\": true,\n    \"reasoning_effort\": \"medium\"\n  }\n}' --no-buffer\n```\n\n**Expected Response (Streaming):**\n\nYou will receive a stream of Server-Sent Events (SSE). Each event's `data` field will contain a JSON object representing a `DeepSearchChatCompletionChunk`.\n\n```text\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1700000000,\"model\":\"jina-deepsearch-v1\",\"choices\":[{\"delta\":{\"role\":\"assistant\"},\"index\":0,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1700000000,\"model\":\"jina-deepsearch-v1\",\"choices\":[{\"delta\":{\"content\":\"The main causes...\"},\"index\":0,\"finish_reason\":null}]}\n\n...\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1700000000,\"model\":\"jina-deepsearch-v1\",\"choices\":[{\"delta\":{},\"index\":0,\"finish_reason\":\"stop\"}], \"usage\": {\"prompt_tokens\": 15, \"completion_tokens\": 250, \"total_tokens\": 265}}\n\ndata: [DONE]\n```\n\n**Example Request (Non-Streaming):**\n\n```bash\ncurl -X POST http://localhost:8000/tools/chat_completion/invoke \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"params\": {\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ],\n    \"stream\": false\n  }\n}'\n```\n\n**Expected Response (Non-Streaming):**\n\nA single JSON object representing the complete `DeepSearchChatCompletion`.\n\n```json\n{\n  \"result\": {\n    \"id\": \"chatcmpl-yyy\",\n    \"object\": \"chat.completion\",\n    \"created\": 1700000010,\n    \"model\": \"jina-deepsearch-v1\",\n    \"choices\": [\n      {\n        \"message\": {\n          \"role\": \"assistant\",\n          \"content\": \"The capital of France is Paris.\"\n        },\n        \"index\": 0,\n        \"finish_reason\": \"stop\"\n      }\n    ],\n    \"usage\": {\n      \"prompt_tokens\": 10,\n      \"completion_tokens\": 6,\n      \"total_tokens\": 16\n    }\n  }\n}\n```\n\n## Authentication\n\nThis server requires a Jina AI API key for authentication. Ensure the `JINA_API_KEY` environment variable is set correctly in your `.env` file.\n\n## Error Handling\n\nThe server includes error handling for common issues:\n\n*   **Authentication Errors (401/403):** Invalid or missing API key.\n*   **Rate Limit Errors (429):** Exceeded API request limits.\n*   **Bad Request Errors (400):** Invalid input parameters or message format.\n*   **Timeout Errors:** Request took too long to complete.\n*   **Server Errors (5xx):** Issues on the Jina AI DeepSearch API side.\n*   **Network Errors:** Problems connecting to the API.\n\nErrors will be logged, and the API tool will typically return a JSON object containing an `\"error\"` key and a descriptive message.\n"
    }
  ]
}