{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Union, Literal\n\n# --- Type Definitions from Plan ---\n\nclass MessageContentPartText(BaseModel):\n    \"\"\"Represents a text part of a message content.\"\"\"\n    type: Literal['text'] = Field(..., description=\"The type of the content part.\")\n    text: str = Field(..., description=\"The text content.\")\n\nclass MessageContentPartImageURLData(BaseModel):\n    \"\"\"Structure for the image URL data.\"\"\"\n    url: str = Field(..., description=\"The URL of the image, typically a data URI.\")\n\nclass MessageContentPartImageURL(BaseModel):\n    \"\"\"Represents an image URL part of a message content.\"\"\"\n    type: Literal['image_url'] = Field(..., description=\"The type of the content part.\")\n    image_url: MessageContentPartImageURLData = Field(..., description=\"A dictionary containing the URL.\")\n\nclass MessageContentPartDocumentURLData(BaseModel):\n    \"\"\"Structure for the document URL data.\"\"\"\n    url: str = Field(..., description=\"The URL of the document, typically a data URI.\")\n\nclass MessageContentPartDocumentURL(BaseModel):\n    \"\"\"Represents a document URL part of a message content.\"\"\"\n    type: Literal['document_url'] = Field(..., description=\"The type of the content part.\")\n    document_url: MessageContentPartDocumentURLData = Field(..., description=\"A dictionary containing the URL.\")\n\nMessageContentPart = Union[MessageContentPartText, MessageContentPartImageURL, MessageContentPartDocumentURL]\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant', 'system'] = Field(..., description=\"The role of the message author.\")\n    content: Union[str, List[MessageContentPart]] = Field(..., description=\"The content of the message.\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details about a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source.\")\n    url: str = Field(..., description=\"The URL of the source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation within the response content.\"\"\"\n    type: str = Field(..., description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass ChoiceDelta(BaseModel):\n    \"\"\"The delta update for a choice in a streaming response.\"\"\"\n    content: Optional[str] = Field(None, description=\"The content delta.\")\n    role: Optional[Literal['assistant']] = Field(None, description=\"Role of the author of the message delta.\") # Added based on typical OpenAI schema\n    type: Optional[str] = Field(None, description=\"Type of content (e.g., 'text').\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations associated with this delta.\")\n\nclass ChatCompletionChunkChoice(BaseModel):\n    \"\"\"A choice within a streaming chat completion chunk.\"\"\"\n    index: int = Field(..., description=\"The index of the choice.\")\n    delta: ChoiceDelta = Field(..., description=\"The delta change for this choice.\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the model stopped generating tokens.\") # Added based on typical OpenAI schema\n    logprobs: Optional[Any] = Field(None, description=\"Log probability information, if requested.\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Number of tokens in the prompt.\")\n    completion_tokens: Optional[int] = Field(None, description=\"Number of tokens in the generated completion.\")\n    total_tokens: Optional[int] = Field(None, description=\"Total number of tokens used in the request.\")\n    search_requests: Optional[int] = Field(None, description=\"Number of search requests performed.\")\n    search_requests_used: Optional[int] = Field(None, description=\"Number of search requests actually used.\")\n\n# --- Input Model --- \n\nclass DeepSearchChatInput(BaseModel):\n    \"\"\"Input model for the Jina DeepSearch chat completions tool.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history.\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(True, description=\"Whether to stream back partial progress and the final answer.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains reasoning effort ('low', 'medium', 'high').\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum tokens allowed for the DeepSearch process.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces further thinking/search steps.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema to ensure the final answer matches the structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n# --- Output Models (Based on OpenAI Schema & DeepSearch additions) ---\n\nclass ChatCompletionChoice(BaseModel):\n    \"\"\"A choice in a non-streaming chat completion response.\"\"\"\n    index: int = Field(..., description=\"The index of the choice.\")\n    message: Message = Field(..., description=\"The message generated by the model.\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the model stopped generating tokens.\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probability information, if requested.\")\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations associated with the final message.\")\n\nclass ChatCompletion(BaseModel):\n    \"\"\"Represents a non-streaming chat completion response.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(\"chat.completion\", description=\"The object type, always 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[ChatCompletionChoice] = Field(..., description=\"A list of chat completion choices.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics for the completion request.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint for the model.\")\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process.\") # DeepSearch specific\n\nclass ChatCompletionChunk(BaseModel):\n    \"\"\"Represents a streaming chat completion chunk.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion chunk.\")\n    object: str = Field(\"chat.completion.chunk\", description=\"The object type, always 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chunk was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[ChatCompletionChunkChoice] = Field(..., description=\"A list of chat completion choices.\")\n    usage: Optional[Usage] = Field(None, description=\"Usage statistics, present in the final chunk.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"System fingerprint for the model.\")\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited, present in the final chunk.\") # DeepSearch specific\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import AsyncGenerator, Dict, Any, Union\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatInput, ChatCompletion, ChatCompletionChunk\n\nlogger = logging.getLogger(__name__) \n\nclass JinaDeepSearchError(Exception):\n    \"\"\"Custom exception for Jina DeepSearch API errors.\"\"\"\n    def __init__(self, status_code: int, detail: Any):\n        self.status_code = status_code\n        self.detail = detail\n        super().__init__(f\"HTTP {status_code}: {detail}\")\n\nclass JinaDeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina DeepSearch API.\"\"\"\n\n    def __init__(self, api_key: Optional[str] = None, base_url: str = \"https://deepsearch.jina.ai/v1\"):\n        \"\"\"\n        Initializes the Jina DeepSearch API client.\n\n        Args:\n            api_key: The Jina API key. Reads from JINA_API_KEY environment variable if not provided.\n            base_url: The base URL for the Jina DeepSearch API.\n        \n        Raises:\n            ValueError: If the API key is not provided and not found in environment variables.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API key not provided or found in JINA_API_KEY environment variable.\")\n        \n        self.base_url = base_url\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\"\n        }\n        # Increased timeout for potentially long searches, especially non-streaming\n        self.client = httpx.AsyncClient(base_url=self.base_url, headers=self.headers, timeout=300.0)\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n\n    async def chat_completions(self, params: DeepSearchChatInput) -> Union[ChatCompletion, AsyncGenerator[ChatCompletionChunk, None]]:\n        \"\"\"\n        Initiates a DeepSearch chat completion process.\n\n        Args:\n            params: The input parameters conforming to DeepSearchChatInput model.\n\n        Returns:\n            If stream=False, returns a ChatCompletion object.\n            If stream=True, returns an async generator yielding ChatCompletionChunk objects.\n        \n        Raises:\n            JinaDeepSearchError: For API-specific errors (4xx, 5xx).\n            httpx.RequestError: For network-related issues.\n            ValidationError: If the API response doesn't match the expected Pydantic model.\n            Exception: For other unexpected errors.\n        \"\"\"\n        endpoint = \"/chat/completions\"\n        # Use model_dump to serialize Pydantic models correctly, exclude None values\n        payload = params.model_dump(exclude_none=True)\n\n        try:\n            if params.stream:\n                logger.info(f\"Initiating streaming request to {self.base_url}{endpoint}\")\n                return self._stream_chat_completions(endpoint, payload)\n            else:\n                logger.info(f\"Initiating non-streaming request to {self.base_url}{endpoint}\")\n                response = await self.client.post(endpoint, json=payload)\n                response.raise_for_status() # Raises HTTPStatusError for 4xx/5xx\n                try:\n                    return ChatCompletion.model_validate(response.json())\n                except ValidationError as e:\n                    logger.error(f\"Failed to validate non-streaming response: {e}\")\n                    logger.debug(f\"Invalid response data: {response.text}\")\n                    raise\n\n        except httpx.HTTPStatusError as e:\n            status_code = e.response.status_code\n            try:\n                detail = e.response.json()\n            except json.JSONDecodeError:\n                detail = e.response.text\n            \n            log_message = f\"HTTP error {status_code} from Jina DeepSearch API: {detail}\"\n            if 400 <= status_code < 500:\n                logger.warning(log_message) # Client-side errors\n            else:\n                logger.error(log_message) # Server-side errors\n            raise JinaDeepSearchError(status_code=status_code, detail=detail) from e\n        \n        except httpx.RequestError as e:\n            logger.error(f\"Network error connecting to Jina DeepSearch API: {e}\")\n            raise # Re-raise network errors\n        \n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during chat completion: {e}\")\n            raise # Re-raise other unexpected errors\n\n    async def _stream_chat_completions(self, endpoint: str, payload: Dict[str, Any]) -> AsyncGenerator[ChatCompletionChunk, None]:\n        \"\"\"\n        Handles the streaming response from the chat completions endpoint.\n        Parses Server-Sent Events (SSE).\n        \"\"\"\n        try:\n            async with self.client.stream(\"POST\", endpoint, json=payload) as response:\n                # Check for initial errors before starting to stream\n                if response.status_code >= 400:\n                     error_content = await response.aread()\n                     try:\n                         detail = json.loads(error_content)\n                     except json.JSONDecodeError:\n                         detail = error_content.decode()\n                     raise JinaDeepSearchError(status_code=response.status_code, detail=detail)\n                \n                response.raise_for_status() # Should be redundant if check above works, but good practice\n                \n                async for line in response.aiter_lines():\n                    if line.startswith(\"data:\"):\n                        data_str = line[len(\"data:\"):].strip()\n                        if data_str == \"[DONE]\":\n                            logger.info(\"Stream finished with [DONE] marker.\")\n                            break\n                        if data_str:\n                            try:\n                                chunk_data = json.loads(data_str)\n                                yield ChatCompletionChunk.model_validate(chunk_data)\n                            except json.JSONDecodeError:\n                                logger.warning(f\"Failed to decode JSON chunk: {data_str}\")\n                            except ValidationError as e:\n                                logger.error(f\"Failed to validate streaming chunk: {e}\")\n                                logger.debug(f\"Invalid chunk data: {data_str}\")\n                                # Decide whether to raise or just log and continue\n                                # raise # Option: Stop processing on validation error\n                                continue # Option: Log and try to continue\n                    elif line.strip(): # Log other non-empty lines if needed\n                        logger.debug(f\"Received non-data line: {line}\")\n\n        except httpx.HTTPStatusError as e:\n            # This might catch errors that occur *during* the stream if the server sends an error status later\n            status_code = e.response.status_code\n            try:\n                detail = e.response.json()\n            except json.JSONDecodeError:\n                detail = e.response.text # Or await e.response.aread() if needed\n            log_message = f\"HTTP error {status_code} during Jina DeepSearch stream: {detail}\"\n            logger.error(log_message)\n            raise JinaDeepSearchError(status_code=status_code, detail=detail) from e\n        \n        except httpx.RequestError as e:\n            logger.error(f\"Network error during Jina DeepSearch stream: {e}\")\n            raise\n        \n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during stream processing: {e}\")\n            raise\n"
    },
    {
      "name": "main.py",
      "content": "import logging\nimport os\nimport asyncio\nfrom typing import Union, AsyncGenerator, Dict, Any\n\nfrom dotenv import load_dotenv\nfrom mcp.server.fastmcp import FastMCP\nfrom pydantic import ValidationError\nimport httpx\n\nfrom models import DeepSearchChatInput, ChatCompletion, ChatCompletionChunk\nfrom api import JinaDeepSearchClient, JinaDeepSearchError\n\n# --- Configuration ---\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# --- Initialization ---\n\nSERVICE_NAME = \"jina_deepsearch\"\nSERVICE_DESCRIPTION = \"MCP server for Jina DeepSearch API. DeepSearch combines web searching, reading, and reasoning for comprehensive investigation.\"\n\n# Initialize MCP Server\nmcp = FastMCP(SERVICE_NAME, description=SERVICE_DESCRIPTION)\n\n# Initialize API Client\ntry:\n    api_client = JinaDeepSearchClient()\nexcept ValueError as e:\n    logger.critical(f\"Failed to initialize JinaDeepSearchClient: {e}\")\n    # Decide if the application should exit or continue without a functional client\n    # For this service, the client is essential, so we might exit or raise\n    raise SystemExit(f\"Configuration error: {e}\")\n\n# --- Tool Definition ---\n\n@mcp.tool()\nasync def chat_completions(params: DeepSearchChatInput) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:\n    \"\"\"\n    Initiates a Jina DeepSearch process based on a conversation history.\n    \n    Performs iterative search, reading, and reasoning. Supports streaming \n    responses (recommended) to receive intermediate thinking steps and the \n    final answer.\n\n    Args:\n        params: Input parameters including messages, model, stream flag, and other options.\n\n    Returns:\n        If stream=False, returns a dictionary representing the ChatCompletion object.\n        If stream=True, returns an async generator yielding dictionaries representing ChatCompletionChunk objects.\n        Returns an error dictionary if an API or validation error occurs.\n    \"\"\"\n    logger.info(f\"Received chat_completions request. Streaming: {params.stream}\")\n    try:\n        result = await api_client.chat_completions(params)\n        \n        if isinstance(result, AsyncGenerator):\n            # Handle streaming response\n            async def stream_wrapper():\n                try:\n                    async for chunk in result:\n                        yield chunk.model_dump(exclude_none=True)\n                except (JinaDeepSearchError, httpx.RequestError, ValidationError) as e:\n                    logger.error(f\"Error during stream processing in tool: {e}\")\n                    # Yield an error dictionary as the last item in the stream\n                    yield {\"error\": str(e), \"status_code\": getattr(e, 'status_code', 500)}\n                except Exception as e:\n                    logger.exception(f\"Unexpected error during stream processing in tool: {e}\")\n                    yield {\"error\": \"An unexpected error occurred during streaming.\", \"status_code\": 500}\n            return stream_wrapper() # Return the async generator\n        else:\n            # Handle non-streaming response\n            return result.model_dump(exclude_none=True)\n            \n    except (JinaDeepSearchError, httpx.RequestError, ValidationError) as e:\n        logger.error(f\"Error calling Jina DeepSearch API: {e}\")\n        # Return a dictionary indicating the error for non-streaming cases\n        return {\"error\": str(e), \"status_code\": getattr(e, 'status_code', 500)}\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred in chat_completions tool: {e}\")\n        return {\"error\": \"An unexpected server error occurred.\", \"status_code\": 500}\n\n# --- Server Lifecycle ---\n\n@mcp.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Cleanly close the API client on server shutdown.\"\"\"\n    logger.info(\"Shutting down Jina DeepSearch client...\")\n    await api_client.close()\n    logger.info(\"Jina DeepSearch client closed.\")\n\n# --- Run Server ---\n\nif __name__ == \"__main__\":\n    logger.info(f\"Starting {SERVICE_NAME} MCP server...\")\n    # Note: FastMCP's run() method starts the Uvicorn server.\n    # Configuration like host and port can be passed to mcp.run() or set via environment variables.\n    mcp.run()\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\nhttpx>=0.25.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\ntyping_extensions # For Literal in older Python versions if needed, included for safety\nuvicorn>=0.15.0 # Required by FastMCP\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina DeepSearch API Configuration\n# Get your API key from https://jina.ai/cloud/\nJINA_API_KEY=your_jina_api_key_here\n\n# Optional: Override the default Jina DeepSearch API base URL\n# JINA_DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai/v1\n\n# Optional: Uvicorn server settings (used by FastMCP)\n# HOST=0.0.0.0\n# PORT=8000\n"
    },
    {
      "name": "README.md",
      "content": "# Jina DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server for interacting with the [Jina DeepSearch API](https://jina.ai/deepsearch/).\n\nDeepSearch combines web searching, reading, and reasoning to act as an agent performing iterative research. It aims to find the best answer to complex questions requiring world-knowledge or up-to-date information. The API is designed to be compatible with OpenAI's Chat API schema.\n\nThis server is built using [FastMCP](https://github.com/datascienceai/fastmcp). \n\n## Features\n\n*   Provides an MCP interface to Jina DeepSearch's `chat/completions` endpoint.\n*   Supports both streaming and non-streaming responses.\n*   Handles multimodal inputs (text, images via data URI, documents via data URI).\n*   Configurable reasoning effort, token budget, domain filtering, and more.\n*   Proper error handling and logging.\n*   Authentication via Jina API Key.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository-url>\n    cd <repository-directory>\n    ```\n\n2.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n3.  **Configure Environment Variables:**\n    Copy the example environment file:\n    ```bash\n    cp .env.example .env\n    ```\n    Edit the `.env` file and add your Jina API Key:\n    ```ini\n    JINA_API_KEY=your_jina_api_key_here\n    ```\n    You can obtain a Jina API key from the [Jina AI Cloud](https://jina.ai/cloud/).\n\n## Running the Server\n\nStart the MCP server using:\n\n```bash\npython main.py\n```\n\nBy default, the server will run on `http://127.0.0.1:8000`. You can configure the host and port using environment variables (`HOST`, `PORT`) or by modifying the `mcp.run()` call in `main.py`.\n\n## Available Tools\n\n### `chat_completions`\n\nInitiates a DeepSearch process based on a conversation history.\n\n**Description:** Performs iterative search, reading, and reasoning. Supports streaming responses (recommended) to receive intermediate thinking steps and the final answer.\n\n**Input Model:** `DeepSearchChatInput`\n\n*   `messages` (List[Message], **required**): Conversation history. Can include text, images (webp, png, jpeg via data URI), or documents (txt, pdf via data URI up to 10MB).\n*   `model` (str, optional, default: \"jina-deepsearch-v1\"): ID of the model to use.\n*   `stream` (bool, optional, default: `True`): Whether to stream responses. **Strongly recommended** to avoid timeouts.\n*   `reasoning_effort` (str, optional, default: \"medium\"): Constrains reasoning ('low', 'medium', 'high').\n*   `budget_tokens` (int, optional): Maximum tokens allowed for the process.\n*   `max_attempts` (int, optional): Maximum retries with different approaches.\n*   `no_direct_answer` (bool, optional, default: `False`): Forces further thinking/search steps.\n*   `max_returned_urls` (int, optional): Maximum URLs in the final answer.\n*   `structured_output` (Dict, optional): JSON schema for the final answer structure.\n*   `good_domains` (List[str], optional): Domains to prioritize.\n*   `bad_domains` (List[str], optional): Domains to exclude.\n*   `only_domains` (List[str], optional): Domains to exclusively include.\n\n**Returns:**\n\n*   If `stream=False`: A dictionary representing the `ChatCompletion` object containing the final answer, usage stats, and visited URLs.\n*   If `stream=True`: An asynchronous generator yielding dictionaries representing `ChatCompletionChunk` objects. The last chunk contains final details and usage stats.\n*   On error: A dictionary containing an `error` message and `status_code`.\n\n## Authentication\n\nThe server uses Bearer Token authentication. The Jina API Key provided in the `.env` file (`JINA_API_KEY`) is automatically included in the `Authorization` header for requests to the Jina DeepSearch API.\n\n## Error Handling\n\nThe server handles:\n\n*   HTTP errors (4xx, 5xx) from the Jina API.\n*   Network connection errors.\n*   Request/response validation errors.\n*   Timeouts (handled by `httpx` client timeout setting).\n\nErrors are logged, and an error dictionary is returned by the tool in case of failure.\n\n## Rate Limits\n\nThe Jina DeepSearch API has rate limits (e.g., 10 requests per minute as per the plan). This MCP server itself does not implement additional rate limiting beyond what the API enforces. Ensure your usage complies with Jina's terms of service.\n"
    }
  ]
}