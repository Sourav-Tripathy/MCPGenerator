{
  "files": [
    {
      "name": "models.py",
      "content": """
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Literal, Union

class ChatMessage(BaseModel):
    \"\"\"Represents a single message in the chat history, following OpenAI schema.\"\"\"
    role: Literal['system', 'user', 'assistant'] = Field(..., description="The role of the message author.")
    content: Union[str, List[Dict[str, Any]]] = Field(..., description="The content of the message. Can be text or a list for multimodal input (e.g., text and image URLs/data URIs).")
    # Example multimodal content:
    # [
    #   {"type": "text", "text": "Describe this image"},
    #   {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
    # ]

class DeepSearchChatInput(BaseModel):
    \"\"\"Input model for the Jina DeepSearch chat completions endpoint.\"\"\"
    messages: List[ChatMessage] = Field(..., description="A list of messages comprising the conversation history. Supports text and data URIs for images (webp, png, jpeg) or documents (txt, pdf) up to 10MB.")
    model: str = Field("jina-deepsearch-v1", description="ID of the model to use.")
    stream: bool = Field(True, description="Whether to stream back partial progress. Strongly recommended.")
    reasoning_effort: Literal['low', 'medium', 'high'] = Field("medium", description="Constrains the reasoning effort.")
    budget_tokens: Optional[int] = Field(None, description="Maximum number of tokens allowed for the entire DeepSearch process. Overrides reasoning_effort.")
    max_attempts: Optional[int] = Field(None, description="Maximum number of retries for solving the problem. Overrides reasoning_effort.")
    no_direct_answer: Optional[bool] = Field(False, description="Forces the model to perform search/thinking steps.")
    max_returned_urls: Optional[int] = Field(None, description="Maximum number of URLs to include in the final answer.")
    structured_output: Optional[Dict[str, Any]] = Field(None, description="A JSON schema to ensure the final answer conforms to the specified structure.")
    good_domains: Optional[List[str]] = Field(None, description="List of domains to prioritize during content retrieval.")
    bad_domains: Optional[List[str]] = Field(None, description="List of domains to strictly exclude from content retrieval.")
    only_domains: Optional[List[str]] = Field(None, description="List of domains to exclusively use for content retrieval.")

    class Config:
        # Ensure default values are correctly handled and None values are excluded if not set
        exclude_none = True
"""
    },
    {
      "name": "api.py",
      "content": """
import httpx
import os
import logging
import json
from typing import Dict, Any, AsyncGenerator, Union
from models import DeepSearchChatInput

logger = logging.getLogger(__name__)

class JinaDeepSearchClient:
    \"\"\"
    Asynchronous client for interacting with the Jina AI DeepSearch API.
    \"\"\"
    DEFAULT_BASE_URL = "https://deepsearch.jina.ai/v1"
    DEFAULT_TIMEOUT = 180.0  # Seconds, generous for potentially long searches

    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = DEFAULT_TIMEOUT):
        \"\"\"
        Initializes the Jina DeepSearch client.

        Args:
            api_key: The Jina API key. Reads from JINA_API_KEY environment variable if not provided.
            base_url: The base URL for the Jina API. Defaults to https://deepsearch.jina.ai/v1.
            timeout: Default request timeout in seconds.
        \"\"\"
        self.api_key = api_key or os.getenv("JINA_API_KEY")
        if not self.api_key:
            logger.warning("JINA_API_KEY not found in environment variables. Requests may fail or be severely rate-limited.")
            self.headers = {"Content-Type": "application/json"}
        else:
             self.headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }

        self.base_url = base_url or self.DEFAULT_BASE_URL
        self.timeout = timeout
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            headers=self.headers,
            timeout=self.timeout,
            http2=True # Enable HTTP/2 for potentially better performance
        )

    async def close(self):
        \"\"\"Closes the underlying HTTPX client.\"\"\"
        await self.client.aclose()

    async def chat_completions(
        self,
        params: DeepSearchChatInput
    ) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:
        \"\"\"
        Sends a request to the /chat/completions endpoint.

        Handles both streaming and non-streaming responses based on params.stream.

        Args:
            params: A DeepSearchChatInput object containing the request parameters.

        Returns:
            If stream=False, a dictionary representing the complete API response.
            If stream=True, an async generator yielding dictionary chunks from the API.

        Raises:
            httpx.HTTPStatusError: If the API returns an error status code (4xx or 5xx).
            httpx.RequestError: For network-related errors (connection, timeout, etc.).
            Exception: For unexpected errors during processing.
        \"\"\"
        endpoint = "/chat/completions"
        # Use exclude_none=True to avoid sending parameters with None values
        payload = params.model_dump(exclude_none=True)
        # Ensure 'stream' is explicitly in the payload for the API call
        payload['stream'] = params.stream

        logger.info(f"Sending request to {self.base_url}{endpoint} with stream={params.stream}")
        # logger.debug(f"Payload: {payload}") # Be cautious logging payload if it contains sensitive data

        try:
            if params.stream:
                # Use client.stream for server-sent events (SSE)
                async with self.client.stream("POST", endpoint, json=payload) as response:
                    # Raise exceptions for 4xx/5xx responses immediately
                    response.raise_for_status()
                    logger.info(f"Stream response status: {response.status_code}")
                    # Process the stream line by line
                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data_content = line[len("data: "):].strip()
                            if data_content == "[DONE]":
                                logger.info("Stream finished with [DONE] message.")
                                break
                            try:
                                chunk = json.loads(data_content)
                                yield chunk
                            except json.JSONDecodeError:
                                logger.warning(f"Failed to decode JSON chunk: {data_content}")
                                continue # Skip malformed lines
                        elif line: # Log non-empty, non-data lines if any
                            logger.debug(f"Received non-data line: {line}")

            else:
                # Use client.post for a single response
                response = await self.client.post(endpoint, json=payload)
                # Raise exceptions for 4xx/5xx responses
                response.raise_for_status()
                logger.info(f"Non-stream response status: {response.status_code}")
                return response.json()

        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error occurred: {e.response.status_code} - {e.response.text}")
            # Re-raise the error to be handled by the caller (MCP tool)
            raise
        except httpx.RequestError as e:
            logger.error(f"Network or request error occurred: {e}")
            # Re-raise the error
            raise
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode non-streaming JSON response: {e}")
            # Raise a custom exception or re-raise
            raise ValueError(f"Invalid JSON received from API: {e}") from e
        except Exception as e:
            logger.error(f"An unexpected error occurred in chat_completions: {type(e).__name__} - {e}")
            # Re-raise the error
            raise
"""
    },
    {
      "name": "main.py",
      "content": """
import logging
import os
import asyncio
from typing import Dict, Any, Optional, List, Union, AsyncGenerator, Literal

from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP, ToolException

from models import DeepSearchChatInput, ChatMessage
from api import JinaDeepSearchClient

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize MCP server
mcp = FastMCP(
    service_name="jina_deepsearch",
    description="Provides access to Jina AI's DeepSearch capability, an AI agent that combines web searching, reading, and iterative reasoning to answer complex questions requiring up-to-date information or deep investigation. It is fully compatible with the OpenAI Chat API schema."
)

# Initialize API Client
# Consider adding retry logic or more sophisticated error handling within the client if needed
api_client = JinaDeepSearchClient()

@mcp.tool()
async def chat_with_deepsearch(
    messages: List[Dict[str, Any]],
    model: str = "jina-deepsearch-v1",
    stream: bool = True,
    reasoning_effort: Literal['low', 'medium', 'high'] = "medium",
    budget_tokens: Optional[int] = None,
    max_attempts: Optional[int] = None,
    no_direct_answer: Optional[bool] = False,
    max_returned_urls: Optional[int] = None,
    structured_output: Optional[Dict[str, Any]] = None,
    good_domains: Optional[List[str]] = None,
    bad_domains: Optional[List[str]] = None,
    only_domains: Optional[List[str]] = None,
) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:
    \"\"\"
    Sends a chat message or conversation history to the Jina DeepSearch model.

    The model iteratively searches the web, reads content, and reasons to generate a
    comprehensive answer. Ideal for complex questions needing current information or
    multi-step analysis. Supports streaming responses.

    Args:
        messages: A list of messages comprising the conversation history. Each message
                  should be a dict with 'role' ('system', 'user', or 'assistant') and
                  'content' (string or list for multimodal).
        model: ID of the model to use (default: 'jina-deepsearch-v1').
        stream: Whether to stream back partial progress (default: True). Strongly recommended.
        reasoning_effort: Constrains reasoning effort ('low', 'medium', 'high', default: 'medium').
        budget_tokens: Maximum tokens for the process (overrides reasoning_effort).
        max_attempts: Maximum retries for solving (overrides reasoning_effort).
        no_direct_answer: Forces search/thinking steps even for trivial queries (default: False).
        max_returned_urls: Max URLs in the final answer.
        structured_output: JSON schema for structured output.
        good_domains: List of domains to prioritize.
        bad_domains: List of domains to strictly exclude.
        only_domains: List of domains to exclusively use.

    Returns:
        If stream=False, a single JSON object representing the complete chat completion.
        If stream=True (default), an async generator yielding JSON chunks.

    Raises:
        ToolException: If the API call fails due to HTTP errors, network issues, or invalid input.
    \"\"\"
    logger.info(f"Received request for chat_with_deepsearch (stream={stream})")

    try:
        # Validate and construct ChatMessage objects
        validated_messages = [ChatMessage(**msg) for msg in messages]

        # Construct the input parameters object
        params = DeepSearchChatInput(
            messages=validated_messages,
            model=model,
            stream=stream,
            reasoning_effort=reasoning_effort,
            budget_tokens=budget_tokens,
            max_attempts=max_attempts,
            no_direct_answer=no_direct_answer,
            max_returned_urls=max_returned_urls,
            structured_output=structured_output,
            good_domains=good_domains,
            bad_domains=bad_domains,
            only_domains=only_domains,
        )
    except Exception as e:
        logger.error(f"Input validation error: {e}", exc_info=True)
        raise ToolException(f"Invalid input parameters: {e}")

    try:
        # Call the API client method
        result = await api_client.chat_completions(params)

        # If streaming, return the generator directly
        if stream:
            logger.info("Streaming response started.")
            # Need an inner async generator to handle potential exceptions during iteration
            async def stream_wrapper():
                try:
                    async for chunk in result:
                        yield chunk
                    logger.info("Streaming response completed.")
                except Exception as e:
                    logger.error(f"Error during stream processing: {e}", exc_info=True)
                    # Depending on MCP's stream error handling, you might yield an error dict
                    # or just let the exception propagate. Raising here is safer.
                    raise ToolException(f"Error during streaming response: {e}")
            return stream_wrapper()
        else:
            # If not streaming, return the complete dictionary
            logger.info("Non-streaming response received.")
            return result

    except Exception as e:
        # Catch errors from the API client (HTTPError, RequestError, etc.)
        logger.error(f"Error calling Jina DeepSearch API: {type(e).__name__} - {e}", exc_info=True)
        # Wrap the exception in ToolException for MCP
        raise ToolException(f"API call failed: {e}")


# Graceful shutdown
@mcp.on_event("shutdown")
async def shutdown_event():
    logger.info("Shutting down API client...")
    await api_client.close()
    logger.info("API client closed.")

if __name__ == "__main__":
    # Use uvicorn to run the server
    # Example: uvicorn main:mcp.app --host 0.0.0.0 --port 8000 --reload
    # The FastMCP.run() method is primarily for simple development scenarios.
    # For production, use a proper ASGI server like uvicorn or hypercorn.
    logger.info("Starting MCP server for Jina DeepSearch.")
    logger.info("Run with: uvicorn main:mcp.app --host <host> --port <port>")
    # mcp.run() # This is blocking and less flexible than uvicorn
"""
    },
    {
      "name": "requirements.txt",
      "content": """mcp>=0.1.0
httpx[http2]>=0.25.0
pydantic>=2.0.0
python-dotenv>=1.0.0
uvicorn>=0.20.0
# Add any other specific dependencies if needed
"""
    },
    {
      "name": ".env.example",
      "content": """# Jina AI API Key
# Obtain your key from https://jina.ai/cloud/
# If you don't provide a key, requests will be severely rate-limited (2 RPM).
# Standard keys have 10 RPM, Premium keys have 100 RPM.
JINA_API_KEY=your_jina_api_key_here

# Optional: Override the default Jina API base URL
# JINA_API_BASE_URL=https://deepsearch.jina.ai/v1
"""
    },
    {
      "name": "README.md",
      "content": """# Jina DeepSearch MCP Server

This repository contains a Model Context Protocol (MCP) server implementation using FastMCP for interacting with the [Jina AI DeepSearch API](https://jina.ai/deepsearch/).

Jina DeepSearch is an AI agent that combines web searching, reading, and iterative reasoning to answer complex questions requiring up-to-date information or deep investigation. It is fully compatible with the OpenAI Chat API schema.

## Features

*   Provides an MCP interface to Jina DeepSearch's chat completions endpoint.
*   Supports both streaming and non-streaming responses.
*   Handles API authentication via Jina API keys.
*   Configurable reasoning effort, token budgets, domain filtering, and more.
*   Built with FastMCP, Pydantic, and HTTPX.

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure Environment Variables:**
    *   Copy the example environment file:
        ```bash
        cp .env.example .env
        ```
    *   Edit the `.env` file and add your Jina API key:
        ```
        JINA_API_KEY=your_jina_api_key_here
        ```
        *   You can obtain an API key from the [Jina AI Cloud dashboard](https://jina.ai/cloud/).
        *   If no API key is provided, requests will be subject to a very low public rate limit (e.g., 2 requests per minute). Standard keys typically allow 10 RPM, and premium keys 100 RPM.

## Running the Server

Use an ASGI server like Uvicorn to run the FastMCP application:
