{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Union, Literal\n\n# --- Type Definitions from Plan ---\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant', 'system'] = Field(..., description=\"The role of the message author ('user', 'assistant', or 'system').\")\n    content: Union[str, List[Dict[str, Any]]] = Field(..., description=\"The content of the message. Can be plain text or a list for multimodal inputs (e.g., text and image data URIs).\")\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation provided in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"The title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source used in the response.\")\n    url: str = Field(..., description=\"The URL of the citation source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\n# --- Input Model for chat_completion Tool ---\n\nclass DeepSearchChatParams(BaseModel):\n    \"\"\"Input parameters for the DeepSearch chat completion request.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history. Can include text, images (webp, png, jpeg as data URI), or files (txt, pdf as data URI up to 10MB).\")\n    model: str = Field(\"jina-deepsearch-v1\", description=\"ID of the model to use.\")\n    stream: bool = Field(True, description=\"Whether to deliver events as Server-Sent Events. Strongly recommended to be true to avoid timeouts, as DeepSearch requests can be long.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(\"medium\", description=\"Constrains reasoning effort. Supported values: 'low', 'medium', 'high'. Affects response time and token usage.\")\n    budget_tokens: Optional[int] = Field(None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides reasoning_effort.\")\n    max_attempts: Optional[int] = Field(None, description=\"Maximum number of retries for solving the problem. Allows different reasoning approaches. Overrides reasoning_effort.\")\n    no_direct_answer: Optional[bool] = Field(False, description=\"Forces further thinking/search steps even if the query seems trivial.\")\n    max_returned_urls: Optional[int] = Field(None, description=\"Maximum number of URLs to include in the final answer, sorted by relevance.\")\n    structured_output: Optional[Dict[str, Any]] = Field(None, description=\"JSON schema to ensure the final answer matches the structure.\")\n    good_domains: Optional[List[str]] = Field(None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        # Ensure that default values are included when exporting\n        # and None values are excluded if not explicitly set.\n        # Use model_dump(exclude_none=True) for API payload.\n        pass\n\n# --- Output Models for chat_completion Tool (Based on OpenAI Schema & Plan) ---\n\nclass DeepSearchUsage(BaseModel):\n    \"\"\"Token usage statistics for the DeepSearch request.\"\"\"\n    prompt_tokens: Optional[int] = Field(None, description=\"Number of tokens in the prompt.\")\n    completion_tokens: Optional[int] = Field(None, description=\"Number of tokens in the generated completion.\")\n    total_tokens: Optional[int] = Field(None, description=\"Total number of tokens used in the request.\")\n\nclass DeepSearchChatResponseChoiceMessage(BaseModel):\n    \"\"\"The message generated by the model in a non-streaming response.\"\"\"\n    role: Literal['assistant'] = Field(..., description=\"The role of the author of this message.\")\n    content: Optional[str] = Field(None, description=\"The contents of the message.\")\n\nclass DeepSearchChatResponseChoice(BaseModel):\n    \"\"\"A single choice provided in the chat completion response.\"\"\"\n    index: int = Field(..., description=\"The index of the choice in the list of choices.\")\n    message: DeepSearchChatResponseChoiceMessage = Field(..., description=\"A chat completion message generated by the model.\")\n    finish_reason: Optional[str] = Field(None, description=\"The reason the model stopped generating tokens.\")\n\nclass DeepSearchChatResponse(BaseModel):\n    \"\"\"The full response object for a non-streaming chat completion request.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: Literal['chat.completion'] = Field(..., description=\"The object type, which is always 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[DeepSearchChatResponseChoice] = Field(..., description=\"A list of chat completion choices. Can be more than one if n > 1 was requested.\")\n    usage: Optional[DeepSearchUsage] = Field(None, description=\"Usage statistics for the completion request.\")\n    url_citations: Optional[List[UrlCitation]] = Field(None, description=\"List of URL citations considered or used during the generation.\") # Added based on plan\n\n# --- Output Models for Streaming Response ---\n\nclass DeepSearchChatStreamChoiceDelta(BaseModel):\n    \"\"\"The delta content for a streaming response chunk.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None, description=\"The role of the author of this message.\")\n    content: Optional[str] = Field(None, description=\"The contents of the chunk message.\")\n\nclass DeepSearchChatStreamChoice(BaseModel):\n    \"\"\"A single choice within a streaming response chunk.\"\"\"\n    index: int = Field(..., description=\"The index of the choice in the list of choices.\")\n    delta: DeepSearchChatStreamChoiceDelta = Field(..., description=\"A chat completion delta generated by the model.\")\n    finish_reason: Optional[str] = Field(None, description=\"The reason the model stopped generating tokens. Sent in the final chunk.\")\n\nclass DeepSearchChatStreamResponse(BaseModel):\n    \"\"\"A single chunk received during a streaming chat completion request.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion. Same across chunks.\")\n    object: Literal['chat.completion.chunk'] = Field(..., description=\"The object type, which is always 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chunk was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[DeepSearchChatStreamChoice] = Field(..., description=\"A list of chat completion choices for this chunk.\")\n    usage: Optional[DeepSearchUsage] = Field(None, description=\"Usage statistics. May be sent in the final chunk.\")\n    url_citations: Optional[List[UrlCitation]] = Field(None, description=\"List of URL citations. May be sent in intermediate or final chunks.\") # Added based on plan\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import AsyncGenerator, Dict, Any, Union\nfrom pydantic import ValidationError\n\nfrom models import DeepSearchChatParams, DeepSearchChatResponse, DeepSearchChatStreamResponse\n\nlogger = logging.getLogger(__name__)\n\nclass DeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://deepsearch.jina.ai\"\n    API_ENDPOINT = \"/v1/chat/completions\"\n    # DeepSearch can take a while, especially for complex queries or non-streaming.\n    DEFAULT_TIMEOUT = 300.0 # 5 minutes\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = DEFAULT_TIMEOUT):\n        \"\"\"\n        Initializes the DeepSearchClient.\n\n        Args:\n            api_key: The Jina AI API key. Reads from JINA_API_KEY env var if not provided.\n            base_url: The base URL for the DeepSearch API. Reads from DEEPSEARCH_BASE_URL or uses default if not provided.\n            timeout: Default timeout for HTTP requests in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API key not provided and JINA_API_KEY environment variable not set.\")\n\n        self.base_url = base_url or os.getenv(\"DEEPSEARCH_BASE_URL\", self.DEFAULT_BASE_URL)\n        self.timeout = timeout\n\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\" # Ensure we get JSON back for non-streaming\n        }\n\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            headers=self.headers,\n            timeout=self.timeout\n        )\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n\n    async def chat_completion(self, params: DeepSearchChatParams) -> Union[DeepSearchChatResponse, AsyncGenerator[DeepSearchChatStreamResponse, None]]:\n        \"\"\"\n        Performs a chat completion request using the DeepSearch engine.\n\n        Args:\n            params: The parameters for the chat completion request.\n\n        Returns:\n            If stream=False, returns a DeepSearchChatResponse object.\n            If stream=True, returns an async generator yielding DeepSearchChatStreamResponse objects.\n\n        Raises:\n            httpx.HTTPStatusError: If the API returns an error status code (4xx or 5xx).\n            httpx.TimeoutException: If the request times out.\n            httpx.RequestError: For other network-related errors.\n            ValidationError: If the API response doesn't match the expected Pydantic model.\n            ValueError: If parameters are invalid (though Pydantic should catch most).\n            json.JSONDecodeError: If the response body is not valid JSON (for non-streaming).\n        \"\"\"\n        payload = params.model_dump(exclude_none=True)\n        request_kwargs = {\n            \"method\": \"POST\",\n            \"url\": self.API_ENDPOINT,\n            \"json\": payload\n        }\n\n        try:\n            if params.stream:\n                logger.info(f\"Initiating streaming chat completion request to {self.base_url}{self.API_ENDPOINT}\")\n                return self._stream_chat_completion(**request_kwargs)\n            else:\n                logger.info(f\"Initiating non-streaming chat completion request to {self.base_url}{self.API_ENDPOINT}\")\n                response = await self.client.request(**request_kwargs)\n                response.raise_for_status() # Raise HTTPStatusError for 4xx/5xx\n                response_data = response.json()\n                # Validate and return the full response\n                validated_response = DeepSearchChatResponse.model_validate(response_data)\n                logger.info(f\"Received non-streaming response (ID: {validated_response.id})\")\n                return validated_response\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP error occurred: {e.response.status_code} - {e.response.text}\")\n            # You might want to raise a more specific exception or return an error structure\n            raise\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out: {e}\")\n            raise\n        except httpx.RequestError as e:\n            logger.error(f\"Network request error: {e}\")\n            raise\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON response: {e}\")\n            raise\n        except ValidationError as e:\n            logger.error(f\"Response validation error: {e}\")\n            # Log the problematic data if possible and privacy allows\n            # logger.error(f\"Invalid data: {response_data}\")\n            raise\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during chat completion: {e}\")\n            raise\n\n    async def _stream_chat_completion(self, **request_kwargs) -> AsyncGenerator[DeepSearchChatStreamResponse, None]:\n        \"\"\"Handles the streaming logic for chat completions using SSE.\"\"\"\n        buffer = \"\"\n        try:\n            async with self.client.stream(**request_kwargs) as response:\n                response.raise_for_status() # Check status before starting iteration\n                async for line in response.aiter_lines():\n                    if not line:\n                        # Empty lines separate events in SSE\n                        if buffer.startswith(\"data:\"): # Process completed buffer\n                            data_str = buffer[len(\"data:\"):].strip()\n                            if data_str == \"[DONE]\":\n                                logger.info(\"Stream finished with [DONE] marker.\")\n                                break # End of stream\n                            try:\n                                chunk_data = json.loads(data_str)\n                                validated_chunk = DeepSearchChatStreamResponse.model_validate(chunk_data)\n                                yield validated_chunk\n                            except json.JSONDecodeError:\n                                logger.warning(f\"Skipping non-JSON data line in stream: {data_str}\")\n                            except ValidationError as e:\n                                logger.warning(f\"Stream chunk validation error: {e}. Data: {data_str}\")\n                            except Exception as e:\n                                logger.exception(f\"Error processing stream chunk: {e}. Data: {data_str}\")\n                        buffer = \"\" # Reset buffer after processing or if line wasn't data\n                    else:\n                        buffer += line + \"\\n\" # Append line to buffer\n\n                # Process any remaining buffer content after loop finishes (e.g., if stream ends without newline)\n                if buffer.startswith(\"data:\"):\n                    data_str = buffer[len(\"data:\"):].strip()\n                    if data_str and data_str != \"[DONE]\":\n                        try:\n                            chunk_data = json.loads(data_str)\n                            validated_chunk = DeepSearchChatStreamResponse.model_validate(chunk_data)\n                            yield validated_chunk\n                        except (json.JSONDecodeError, ValidationError) as e:\
                            logger.warning(f\"Error processing final buffer content: {e}. Data: {data_str}\")\n\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP error during streaming: {e.response.status_code} - {await e.response.aread()}\")\n            # Reraise or handle appropriately. Cannot easily yield an error through generator.\n            # Consider logging and stopping iteration.\n            raise\n        except httpx.TimeoutException as e:\n            logger.error(f\"Stream request timed out: {e}\")\n            raise\n        except httpx.RequestError as e:\n            logger.error(f\"Stream network request error: {e}\")\n            raise\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during streaming: {e}\")\n            raise\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, ToolContext\nfrom typing import Dict, Any, AsyncGenerator\nimport logging\nimport os\nfrom dotenv import load_dotenv\nimport httpx\nfrom pydantic import ValidationError\nimport json\n\nfrom models import DeepSearchChatParams, DeepSearchChatResponse, DeepSearchChatStreamResponse\nfrom api import DeepSearchClient\n\n# --- Configuration ---\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlog_level = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\nlogging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- MCP Server Initialization ---\n\nmcp = FastMCP(\n    service_name=\"deepsearch\",\n    description=\"MCP server for Jina AI's DeepSearch API. DeepSearch combines web searching, reading, and reasoning for comprehensive investigation, providing answers to complex questions requiring iterative reasoning, world-knowledge, or up-to-date information. It is compatible with the OpenAI Chat API schema.\"\n)\n\n# --- API Client Initialization ---\n\n# Initialize the DeepSearch API client\n# It reads the API key and base URL from environment variables (JINA_API_KEY, DEEPSEARCH_BASE_URL)\ntry:\n    api_client = DeepSearchClient()\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize DeepSearchClient: {e}\")\n    # Optionally, exit or prevent server startup if client initialization fails\n    # raise SystemExit(f\"Error: {e}\") from e\n    api_client = None # Set to None to handle gracefully in tool\n\n# --- MCP Tools ---\n\n@mcp.tool(\n    name=\"chat_completion\",\n    description=\"Performs a chat completion request using the DeepSearch engine. This involves iterative search, reading, and reasoning to find the best answer to the user's query, especially for complex questions requiring up-to-date information or deep research.\",\n    input_model=DeepSearchChatParams,\n    # Define output model based on whether it's streaming or not - FastMCP might need refinement here\n    # For now, let's specify the non-streaming response model and handle streaming via generator\n    output_model=DeepSearchChatResponse\n)\nasync def chat_completion(params: DeepSearchChatParams, context: ToolContext) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:\n    \"\"\"\n    MCP tool to call the DeepSearch chat completions endpoint.\n\n    Handles both streaming and non-streaming requests based on the `stream` parameter.\n\n    Args:\n        params: The validated input parameters matching DeepSearchChatParams.\n        context: The MCP ToolContext (unused here but available).\n\n    Returns:\n        If stream=False, a dictionary representing the DeepSearchChatResponse.\n        If stream=True, an async generator yielding dictionaries representing DeepSearchChatStreamResponse chunks.\n        In case of error, a dictionary with an 'error' key.\n    \"\"\"\n    if not api_client:\n        logger.error(\"DeepSearchClient is not initialized. Cannot process request.\")\n        return {\"error\": \"DeepSearch client not available. Check API key configuration.\"}\n\n    logger.info(f\"Received chat_completion request. Streaming: {params.stream}\")\n\n    try:\n        result = await api_client.chat_completion(params)\n\n        if isinstance(result, AsyncGenerator):\n            # Handle streaming response\n            logger.info(\"Streaming response back to MCP client.\")\n            async def stream_generator():\n                try:\n                    async for chunk in result:\n                        # Yield each chunk as a dictionary\n                        yield chunk.model_dump(exclude_none=True)\n                except (httpx.HTTPError, httpx.TimeoutException, httpx.RequestError, ValidationError) as e:\n                    logger.error(f\"Error during stream processing in MCP tool: {e}\")\n                    # Yield an error message if possible, though standard MCP streaming might not support it well\n                    yield {\"error\": f\"Stream processing error: {str(e)}\"}\n                except Exception as e:\n                    logger.exception(\"Unexpected error during stream processing in MCP tool\")\n                    yield {\"error\": f\"Unexpected stream processing error: {str(e)}\"}\n            return stream_generator() # Return the async generator\n        else:\n            # Handle non-streaming response\n            logger.info(f\"Received non-streaming response (ID: {result.id}). Returning to MCP client.\")\n            # Return the response as a dictionary\n            return result.model_dump(exclude_none=True)\n\n    except (httpx.HTTPError, httpx.TimeoutException, httpx.RequestError, ValidationError, json.JSONDecodeError) as e:\n        error_message = f\"API request failed: {str(e)}\"\n        if isinstance(e, httpx.HTTPStatusError):\n            try:\n                # Try to include API error details if available\n                error_detail = e.response.json()\n                error_message += f\" - Detail: {error_detail}\"\n            except Exception:\n                error_message += f\" - Body: {e.response.text}\"\n        logger.error(error_message)\n        return {\"error\": error_message}\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred in the chat_completion tool\")\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\n# --- Server Shutdown Hook ---\n\n@mcp.app.on_event(\"shutdown\")\nasync def shutdown_event():\n    logger.info(\"Shutting down MCP server...\")\n    if api_client:\n        await api_client.close()\n        logger.info(\"DeepSearchClient closed.\")\n\n# --- Run Server ---\n\nif __name__ == \"__main__\":\n    import uvicorn\n    logger.info(\"Starting DeepSearch MCP server...\")\n    # Run using Uvicorn. Adjust host, port, and workers as needed.\n    uvicorn.run(\"main:mcp.app\", host=\"0.0.0.0\", port=8000, reload=False) # Use reload=True for development\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\nhttpx>=0.25.0\npydantic>=2.0.0\npython-dotenv>=1.0.0\nuvicorn>=0.15.0\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI DeepSearch API Credentials and Configuration\n\n# Your Jina AI API Key (Required)\n# Obtain from: https://jina.ai/cloud/\nJINA_API_KEY=your_jina_api_key_here\n\n# Base URL for the DeepSearch API (Optional - Defaults to https://deepsearch.jina.ai)\n# DEEPSEARCH_BASE_URL=https://deepsearch.jina.ai\n\n# Log level for the application (Optional - Defaults to INFO)\n# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL\nLOG_LEVEL=INFO\n"
    },
    {
      "name": "README.md",
      "content": "# DeepSearch MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server implementation for Jina AI's DeepSearch API, built using FastMCP.\n\nDeepSearch combines web searching, reading, and reasoning for comprehensive investigation. It provides answers to complex questions requiring iterative reasoning, world-knowledge, or up-to-date information. The API is compatible with the OpenAI Chat API schema.\n\nThis MCP server exposes the DeepSearch chat completion functionality as a standardized tool.\n\n## Features\n\n*   **Chat Completion:** Provides access to the core DeepSearch `/v1/chat/completions` endpoint.\n*   **Streaming Support:** Handles Server-Sent Events (SSE) for real-time responses when `stream=True`.\n*   **Configurable Parameters:** Supports various DeepSearch parameters like `reasoning_effort`, `budget_tokens`, domain filtering, structured output, etc.\n*   **OpenAI Schema Compatible:** Uses input and output models largely compatible with the OpenAI API schema.\n*   **Error Handling:** Includes handling for API errors, network issues, timeouts, and validation errors.\n*   **Async Implementation:** Built with `asyncio`, `httpx`, and `FastMCP` for efficient asynchronous operations.\n\n## Setup\n\n1.  **Clone the repository:**\n    ```bash\n    git clone <repository_url>\n    cd <repository_directory>\n    ```\n\n2.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure environment variables:**\n    *   Copy the example `.env.example` file to `.env`:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina AI API key:\n        ```env\n        JINA_API_KEY=your_jina_api_key_here\n        # LOG_LEVEL=DEBUG # Optional: Set to DEBUG for more verbose logs\n        ```\n        You can obtain a Jina AI API key from [Jina AI Cloud](https://jina.ai/cloud/).\n\n## Running the Server\n\nUse Uvicorn to run the FastMCP application:\n\n```bash\n# For development with auto-reload\nuvicorn main:mcp.app --reload --host 0.0.0.0 --port 8000\n\n# For production (adjust workers as needed)\nuvicorn main:mcp.app --host 0.0.0.0 --port 8000 --workers 4\n```\n\nThe MCP server will be available at `http://localhost:8000`.\n\n*   **MCP Schema:** `http://localhost:8000/mcp/schema`\n*   **Tool Execution:** `http://localhost:8000/mcp/run/{tool_name}` (POST request)\n*   **OpenAPI Docs (FastAPI):** `http://localhost:8000/docs`\n\n## Environment Variables\n\n*   `JINA_API_KEY` (Required): Your Jina AI API key.\n*   `DEEPSEARCH_BASE_URL` (Optional): Overrides the default DeepSearch API base URL (`https://deepsearch.jina.ai`).\n*   `LOG_LEVEL` (Optional): Sets the application's logging level (e.g., `INFO`, `DEBUG`). Defaults to `INFO`.\n\n## Tools\n\n### `chat_completion`\n\nPerforms a chat completion request using the DeepSearch engine.\n\n*   **Description:** This involves iterative search, reading, and reasoning to find the best answer to the user's query, especially for complex questions requiring up-to-date information or deep research.\n*   **Input:** Accepts parameters defined by the `DeepSearchChatParams` model (see `models.py`). Key parameters include:\n    *   `messages`: List of conversation messages (user/assistant roles).\n    *   `model`: Model ID (defaults to `jina-deepsearch-v1`).\n    *   `stream`: Boolean, enables SSE streaming (defaults to `True`).\n    *   `reasoning_effort`, `budget_tokens`, `max_attempts`: Control reasoning complexity.\n    *   `max_returned_urls`, `good_domains`, `bad_domains`, `only_domains`: Control source retrieval.\n    *   `structured_output`: Enforce a JSON schema on the output.\n*   **Output:**\n    *   If `stream=False`: Returns a JSON object matching the `DeepSearchChatResponse` model (see `models.py`), containing the final answer, usage stats, and citations.\n    *   If `stream=True`: Returns an asynchronous stream of JSON objects, each matching the `DeepSearchChatStreamResponse` model, representing chunks of the response.\n\n## Authentication\n\nThe server uses an API Key (Bearer Token) for authenticating with the Jina AI DeepSearch API. The key is read from the `JINA_API_KEY` environment variable and included in the `Authorization` header of outgoing requests.\n\n## Error Handling\n\nThe server attempts to handle various errors:\n\n*   **HTTP Errors:** Catches 4xx and 5xx responses from the DeepSearch API.\n*   **Timeouts:** Handles request timeouts.\n*   **Network Errors:** Catches connection issues.\n*   **Validation Errors:** Validates input parameters and API responses against Pydantic models.\n*   **Configuration Errors:** Checks for the presence of the API key on startup.\n\nError details are logged, and an error message is returned to the MCP client, typically in a `{\"error\": \"...\"}` format.\n\n## Rate Limits\n\nThe DeepSearch API has rate limits (e.g., 10 requests per minute on free tiers, check Jina AI documentation for details). This MCP server implementation **does not** currently enforce these rate limits on incoming requests. Clients calling this MCP server should implement their own rate-limiting logic if necessary to avoid hitting the upstream API limits.\n\n## Dependencies\n\n*   `fastmcp`: The MCP server framework.\n*   `httpx`: Asynchronous HTTP client.\n*   `pydantic`: Data validation and settings management.\n*   `python-dotenv`: Loading environment variables from `.env` files.\n*   `uvicorn`: ASGI server to run the application.\n"
    }
  ]
}