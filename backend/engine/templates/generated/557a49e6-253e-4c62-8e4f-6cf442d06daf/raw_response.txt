{
  "files": [
    {
      "name": "models.py",
      "content": "from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any, Literal\n\n# --- Input Models ---\n\nclass Message(BaseModel):\n    \"\"\"Represents a single message in the conversation.\"\"\"\n    role: Literal['user', 'assistant', 'system'] = Field(..., description=\"The role of the message author ('user', 'assistant', or 'system').\")\n    content: str = Field(..., description=\"The content of the message. Can be plain text or a data URI for images/documents (e.g., 'data:image/png;base64,...', 'data:application/pdf;base64,...').\")\n\nclass DeepSearchInput(BaseModel):\n    \"\"\"Input model for the DeepSearch chat completion request.\"\"\"\n    messages: List[Message] = Field(..., description=\"A list of messages comprising the conversation history, including the latest user query. Supports text and multimodal content (images/documents as data URIs).\")\n    model: str = Field(default=\"jina-deepsearch-v1\", description=\"ID of the model to use. Currently only 'jina-deepsearch-v1' is supported.\")\n    stream: bool = Field(default=True, description=\"Whether to stream back partial progress and the final answer using server-sent events. Recommended to be true to avoid timeouts for long-running queries.\")\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = Field(default=\"medium\", description=\"Constrains the reasoning effort. Supported values: 'low', 'medium', 'high'. Affects response time and token usage.\")\n    budget_tokens: Optional[int] = Field(default=None, description=\"Maximum number of tokens allowed for the DeepSearch process. Overrides 'reasoning_effort'.\")\n    max_attempts: Optional[int] = Field(default=None, description=\"Maximum number of retries for solving the problem using different reasoning approaches. Overrides 'reasoning_effort'.\")\n    no_direct_answer: bool = Field(default=False, description=\"Forces the model to perform search/thinking steps even for seemingly trivial queries.\")\n    max_returned_urls: Optional[int] = Field(default=None, description=\"Maximum number of URLs to include in the final answer/chunk, sorted by relevance.\")\n    structured_output: Optional[Dict[str, Any]] = Field(default=None, description=\"JSON schema to ensure the final answer conforms to the specified structure.\")\n    good_domains: Optional[List[str]] = Field(default=None, description=\"List of domains to prioritize for content retrieval.\")\n    bad_domains: Optional[List[str]] = Field(default=None, description=\"List of domains to strictly exclude from content retrieval.\")\n    only_domains: Optional[List[str]] = Field(default=None, description=\"List of domains to exclusively include in content retrieval.\")\n\n    class Config:\n        # Ensure None values are not included in the output dict if not set\n        exclude_none = True\n\n# --- Output Models ---\n\nclass UrlCitation(BaseModel):\n    \"\"\"Details of a URL citation used in the response.\"\"\"\n    title: Optional[str] = Field(None, description=\"Title of the cited web page.\")\n    exactQuote: Optional[str] = Field(None, description=\"The exact quote from the source that supports the answer.\")\n    url: Optional[str] = Field(None, description=\"URL of the source.\")\n    dateTime: Optional[str] = Field(None, description=\"Timestamp associated with the citation.\")\n\nclass Annotation(BaseModel):\n    \"\"\"Annotation associated with the response content, like citations.\"\"\"\n    type: Optional[str] = Field(None, description=\"Type of annotation (e.g., 'url_citation').\")\n    url_citation: Optional[UrlCitation] = Field(None, description=\"Details if the annotation is a URL citation.\")\n\nclass Delta(BaseModel):\n    \"\"\"The content delta in a streamed chunk.\"\"\"\n    role: Optional[Literal['assistant']] = Field(None, description=\"The role of the message author, typically 'assistant'.\")\n    content: Optional[str] = Field(None, description=\"The text content of the chunk.\")\n    type: Optional[str] = Field(None, description=\"Type of content (e.g., 'text').\") # Note: Jina API might not use 'type' here\n    annotations: Optional[List[Annotation]] = Field(None, description=\"Annotations related to the content.\")\n\nclass Choice(BaseModel):\n    \"\"\"A single response choice (streaming or non-streaming).\"\"\"\n    index: int = Field(..., description=\"Index of the choice.\")\n    delta: Optional[Delta] = Field(None, description=\"The content delta for this choice (used in streaming).\")\n    message: Optional[Message] = Field(None, description=\"The full message object (used in non-streaming).\")\n    logprobs: Optional[Any] = Field(None, description=\"Log probabilities (currently null in Jina API).\")\n    finish_reason: Optional[str] = Field(None, description=\"Reason the model stopped generating tokens (e.g., 'stop', 'length', 'error').\")\n\nclass Usage(BaseModel):\n    \"\"\"Token usage statistics for the request.\"\"\"\n    prompt_tokens: int = Field(..., description=\"Tokens used in the prompt/reasoning process.\")\n    # Note: Jina DeepSearch API currently only provides prompt_tokens in usage\n    completion_tokens: Optional[int] = Field(None, description=\"Tokens generated for the completion (Not provided by Jina API).\")\n    total_tokens: Optional[int] = Field(None, description=\"Total tokens (Not provided by Jina API).\")\n\nclass DeepSearchChunk(BaseModel):\n    \"\"\"Represents a chunk received during a streaming response.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion chunk.\")\n    object: str = Field(..., description=\"The object type, which is always 'chat.completion.chunk'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion chunk was created.\")\n    model: str = Field(..., description=\"The model to generate the completion.\")\n    choices: List[Choice] = Field(..., description=\"A list of chat completion choices. Can contain more than one if n > 1, but typically 1 for Jina.\")\n    usage: Optional[Usage] = Field(None, description=\"An object describing the token usage statistics for the completion (usually present in the final chunk).\")\n    system_fingerprint: Optional[str] = Field(None, description=\"This fingerprint represents the backend configuration that the model runs with.\")\n    # Jina specific fields that might appear in the last chunk\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process (usually in the final chunk).\")\n\nclass DeepSearchResponse(BaseModel):\n    \"\"\"Represents the full response for a non-streaming request.\"\"\"\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\n    object: str = Field(..., description=\"The object type, which is always 'chat.completion'.\")\n    created: int = Field(..., description=\"The Unix timestamp (in seconds) of when the chat completion was created.\")\n    model: str = Field(..., description=\"The model used for the chat completion.\")\n    choices: List[Choice] = Field(..., description=\"A list of chat completion choices.\")\n    usage: Usage = Field(..., description=\"Usage statistics for the completion request.\")\n    system_fingerprint: Optional[str] = Field(None, description=\"This fingerprint represents the backend configuration that the model runs with.\")\n    # Jina specific fields\n    visited_urls: Optional[List[str]] = Field(None, description=\"List of URLs visited during the search process.\")\n"
    },
    {
      "name": "api.py",
      "content": "import httpx\nimport os\nimport logging\nimport json\nfrom typing import AsyncIterator, Union, Dict, Any\nfrom pydantic import ValidationError\nfrom async_sse_client import EventSource\n\nfrom models import DeepSearchInput, DeepSearchResponse, DeepSearchChunk\n\nlogger = logging.getLogger(__name__)\n\nclass JinaDeepSearchClient:\n    \"\"\"Asynchronous client for interacting with the Jina AI DeepSearch API.\"\"\"\n\n    DEFAULT_BASE_URL = \"https://deepsearch.jina.ai/v1\"\n    DEFAULT_TIMEOUT = 180.0  # Seconds, increased for potentially long searches\n\n    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None, timeout: float = DEFAULT_TIMEOUT):\n        \"\"\"\n        Initializes the JinaDeepSearchClient.\n\n        Args:\n            api_key: The Jina API key. Reads from JINA_API_KEY env var if None.\n            base_url: The base URL for the Jina API. Defaults to https://deepsearch.jina.ai/v1.\n            timeout: Default request timeout in seconds.\n        \"\"\"\n        self.api_key = api_key or os.getenv(\"JINA_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"Jina API key not provided or found in JINA_API_KEY environment variable.\")\n\n        self.base_url = base_url or self.DEFAULT_BASE_URL\n        self.timeout = timeout\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\", # For non-streaming\n            \"X-DeepSearch-Client\": \"mcp-server/0.1.0\" # Optional client identifier\n        }\n        self.client = httpx.AsyncClient(\n            base_url=self.base_url,\n            timeout=self.timeout\n        )\n\n    async def _request(\n        self,\n        method: str,\n        endpoint: str,\n        payload: Optional[Dict[str, Any]] = None,\n        stream: bool = False\n    ) -> Union[httpx.Response, EventSource]:\n        \"\"\"Internal method to make HTTP requests.\"\"\"\n        try:\n            headers = self.headers.copy()\n            if stream:\n                headers[\"Accept\"] = \"text/event-stream\"\n\n            if stream:\n                # For streaming, httpx doesn't handle SSE directly well with POST\n                # We use async_sse_client which needs the request object\n                req = self.client.build_request(\n                    method=method,\n                    url=endpoint,\n                    json=payload,\n                    headers=headers,\n                    timeout=self.timeout\n                )\n                # Note: async_sse_client doesn't automatically raise for status on connection\n                # Error handling needs to be done during iteration\n                return EventSource(req, client=self.client)\n            else:\n                response = await self.client.request(\n                    method=method,\n                    url=endpoint,\n                    json=payload,\n                    headers=headers\n                )\n                response.raise_for_status() # Raise HTTPStatusError for 4xx/5xx\n                return response\n\n        except httpx.TimeoutException as e:\n            logger.error(f\"Request timed out after {self.timeout}s: {e}\")\n            raise TimeoutError(f\"API request timed out: {e}\") from e\n        except httpx.RequestError as e:\n            logger.error(f\"Network or request error: {e}\")\n            raise ConnectionError(f\"API request failed: {e}\") from e\n        except httpx.HTTPStatusError as e:\n            logger.error(f\"HTTP error {e.response.status_code}: {e.response.text}\")\n            # You might want to raise a more specific custom exception here\n            raise\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during the API request: {e}\")\n            raise\n\n    async def chat_completion(\n        self, params: DeepSearchInput\n    ) -> Union[DeepSearchResponse, AsyncIterator[DeepSearchChunk]]:\n        \"\"\"\n        Performs a deep search chat completion.\n\n        Args:\n            params: Input parameters conforming to the DeepSearchInput model.\n\n        Returns:\n            If stream=False, returns a DeepSearchResponse object.\n            If stream=True, returns an async iterator yielding DeepSearchChunk objects.\n\n        Raises:\n            ValueError: If input validation fails.\n            ConnectionError: If there's a network issue.\n            TimeoutError: If the request times out.\n            httpx.HTTPStatusError: For API-level errors (4xx, 5xx).\n            Exception: For other unexpected errors.\n        \"\"\"\n        endpoint = \"/chat/completions\"\n        payload = params.model_dump(exclude_none=True) # Use exclude_none from model config\n\n        if params.stream:\n            return self._stream_chat_completion(endpoint, payload)\n        else:\n            return await self._non_stream_chat_completion(endpoint, payload)\n\n    async def _non_stream_chat_completion(self, endpoint: str, payload: Dict[str, Any]) -> DeepSearchResponse:\n        \"\"\"Handles non-streaming chat completion requests.\"\"\"\n        response = await self._request(\"POST\", endpoint, payload=payload, stream=False)\n        try:\n            response_data = response.json()\n            return DeepSearchResponse.model_validate(response_data)\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON response: {response.text}\")\n            raise ValueError(f\"Invalid JSON received from API: {e}\") from e\n        except ValidationError as e:\n            logger.error(f\"Failed to validate API response: {response_data}\")\n            raise ValueError(f\"API response validation failed: {e}\") from e\n\n    async def _stream_chat_completion(self, endpoint: str, payload: Dict[str, Any]) -> AsyncIterator[DeepSearchChunk]:\n        \"\"\"Handles streaming chat completion requests using SSE.\"\"\"\n        event_source = await self._request(\"POST\", endpoint, payload=payload, stream=True)\n        try:\n            async with event_source as source:\n                async for event in source:\n                    if event.event == 'error':\n                        logger.error(f\"SSE Error event received: {event.data}\")\n                        # Attempt to parse error details if JSON\n                        try:\n                            error_data = json.loads(event.data)\n                            raise httpx.HTTPStatusError(\n                                message=error_data.get('message', event.data),\n                                request=event_source.request,\n                                response=httpx.Response(status_code=error_data.get('status', 500), json=error_data)\n                            )\n                        except json.JSONDecodeError:\n                            raise ConnectionError(f\"Received non-JSON SSE error: {event.data}\")\n                    elif event.event == 'message' or event.type == 'message': # Check both event and type\n                        if event.data.strip() == '[DONE]':\n                            logger.info(\"SSE stream finished with [DONE] message.\")\n                            break\n                        try:\n                            chunk_data = json.loads(event.data)\n                            yield DeepSearchChunk.model_validate(chunk_data)\n                        except json.JSONDecodeError:\n                            logger.warning(f\"Received non-JSON SSE data: {event.data}\")\n                            continue # Skip malformed data\n                        except ValidationError as e:\n                            logger.warning(f\"Failed to validate SSE chunk: {event.data}, Error: {e}\")\n                            continue # Skip invalid chunks\n                    else:\n                        logger.debug(f\"Received unhandled SSE event type '{event.event}' or type '{event.type}': {event.data}\")\n\n        except httpx.HTTPStatusError as e:\n            # Errors during connection or initial response might raise this\n            logger.error(f\"HTTP error during SSE connection: {e.response.status_code} - {e.response.text}\")\n            raise\n        except httpx.TransportError as e:\n            # Handles connection errors, read timeouts during stream etc.\n            logger.error(f\"Transport error during SSE stream: {e}\")\n            raise ConnectionError(f\"SSE stream transport error: {e}\") from e\n        except Exception as e:\n            logger.exception(f\"An unexpected error occurred during SSE processing: {e}\")\n            raise\n        finally:\n            # Ensure the underlying client connection is closed if EventSource didn't close it\n            # This might be handled internally by async_sse_client's context manager\n            pass\n\n    async def close(self):\n        \"\"\"Closes the underlying HTTPX client.\"\"\"\n        await self.client.aclose()\n"
    },
    {
      "name": "main.py",
      "content": "from mcp.server.fastmcp import FastMCP, ToolContext\nfrom typing import AsyncIterator, Union, List, Optional, Dict, Any\nimport logging\nimport os\nimport asyncio\nfrom dotenv import load_dotenv\nimport httpx\n\n# Import models and API client\nfrom models import DeepSearchInput, DeepSearchResponse, DeepSearchChunk, Message\nfrom api import JinaDeepSearchClient\n\n# --- Configuration ---\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# --- MCP Server Initialization ---\n\nmcp = FastMCP(\n    service_name=\"jina_deepsearch\",\n    description=\"Integrates with Jina AI's DeepSearch API for advanced web search and reasoning.\"\n)\n\n# --- API Client Initialization ---\n\n# Initialize the Jina DeepSearch client\n# It will automatically pick up the API key from the JINA_API_KEY environment variable\ntry:\n    api_client = JinaDeepSearchClient()\nexcept ValueError as e:\n    logger.error(f\"Failed to initialize JinaDeepSearchClient: {e}\")\n    # Depending on deployment strategy, you might want to exit or handle this differently\n    api_client = None # Set to None to prevent tool usage if initialization fails\n\n# --- Tool Definition ---\n\n@mcp.tool()\nasync def chat_completion(\n    ctx: ToolContext,\n    messages: List[Message],\n    model: str = \"jina-deepsearch-v1\",\n    stream: bool = True,\n    reasoning_effort: Optional[Literal['low', 'medium', 'high']] = \"medium\",\n    budget_tokens: Optional[int] = None,\n    max_attempts: Optional[int] = None,\n    no_direct_answer: bool = False,\n    max_returned_urls: Optional[int] = None,\n    structured_output: Optional[Dict[str, Any]] = None,\n    good_domains: Optional[List[str]] = None,\n    bad_domains: Optional[List[str]] = None,\n    only_domains: Optional[List[str]] = None\n) -> Union[DeepSearchResponse, AsyncIterator[DeepSearchChunk]]:\n    \"\"\"\n    Performs a deep search and reasoning process based on the provided conversation history and query.\n\n    This tool iteratively searches the web, reads relevant content, and reasons to formulate the best possible answer.\n    Suitable for complex questions requiring up-to-date information or multi-hop reasoning.\n    Supports streaming responses.\n\n    Args:\n        ctx: The MCP ToolContext.\n        messages: A list of messages comprising the conversation history.\n        model: ID of the model to use (default: 'jina-deepsearch-v1').\n        stream: Whether to stream back partial progress (default: True).\n        reasoning_effort: Reasoning effort level ('low', 'medium', 'high', default: 'medium').\n        budget_tokens: Maximum tokens allowed for the process.\n        max_attempts: Maximum retry attempts for solving.\n        no_direct_answer: Force search/thinking steps (default: False).\n        max_returned_urls: Maximum URLs in the final answer.\n        structured_output: JSON schema for structured output.\n        good_domains: Prioritized domains.\n        bad_domains: Excluded domains.\n        only_domains: Exclusively included domains.\n\n    Returns:\n        If stream=False, a DeepSearchResponse object.\n        If stream=True, an async iterator yielding DeepSearchChunk objects.\n\n    Raises:\n        ConnectionError: If the API client cannot connect.\n        TimeoutError: If the API request times out.\n        Exception: For other API or processing errors.\n    \"\"\"\n    if not api_client:\n        logger.error(\"JinaDeepSearchClient is not initialized. Cannot perform chat completion.\")\n        # You might want to return a specific error structure or raise an exception\n        # depending on how the calling agent should handle this.\n        raise RuntimeError(\"Jina DeepSearch API client is not configured.\")\n\n    # Construct the input object\n    input_data = DeepSearchInput(\n        messages=messages,\n        model=model,\n        stream=stream,\n        reasoning_effort=reasoning_effort,\n        budget_tokens=budget_tokens,\n        max_attempts=max_attempts,\n        no_direct_answer=no_direct_answer,\n        max_returned_urls=max_returned_urls,\n        structured_output=structured_output,\n        good_domains=good_domains,\n        bad_domains=bad_domains,\n        only_domains=only_domains\n    )\n\n    logger.info(f\"Calling Jina DeepSearch API (stream={stream}) with model '{model}'.\")\n\n    try:\n        result = await api_client.chat_completion(params=input_data)\n\n        if stream:\n            # If streaming, return the async iterator directly\n            logger.info(\"Streaming response started.\")\n            # The iterator needs to be consumed by the caller\n            async def stream_wrapper():\n                try:\n                    async for chunk in result:\n                        yield chunk\n                    logger.info(\"Streaming response finished.\")\n                except Exception as e:\n                    logger.error(f\"Error during stream consumption: {e}\", exc_info=True)\n                    # Re-raise or handle as appropriate for the MCP framework\n                    raise\n            return stream_wrapper()\n        else:\n            # If not streaming, return the complete response object\n            logger.info(f\"Received non-streaming response. Usage: {result.usage}\")\n            return result\n\n    except httpx.HTTPStatusError as e:\n        logger.error(f\"API request failed with status {e.response.status_code}: {e.response.text}\")\n        # Re-raise or convert to a more user-friendly error\n        raise Exception(f\"Jina API Error {e.response.status_code}: {e.response.text}\") from e\n    except (ConnectionError, TimeoutError, httpx.RequestError) as e:\n        logger.error(f\"API connection/timeout error: {e}\")\n        raise ConnectionError(f\"Failed to connect to Jina DeepSearch API: {e}\") from e\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred in chat_completion tool: {e}\")\n        raise Exception(f\"An internal error occurred: {e}\") from e\n\n# --- Server Shutdown Hook ---\n\n@mcp.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Gracefully close the API client on server shutdown.\"\"\"\n    if api_client:\n        logger.info(\"Closing Jina DeepSearch API client...\")\n        await api_client.close()\n        logger.info(\"Jina DeepSearch API client closed.\")\n    else:\n        logger.info(\"No API client to close.\")\n\n# --- Run Server ---\n\nif __name__ == \"__main__\":\n    logger.info(\"Starting Jina DeepSearch MCP server...\")\n    mcp.run()\n"
    },
    {
      "name": "requirements.txt",
      "content": "fastmcp>=0.1.0\nhttpx>=0.25.0,<0.28.0\npydantic>=2.0.0,<3.0.0\npython-dotenv>=1.0.0\nasync-sse-client>=0.1.0\n# uvicorn is needed by fastmcp to run the server\nuvicorn>=0.20.0\n"
    },
    {
      "name": ".env.example",
      "content": "# Jina AI API Key\n# Obtain your key from https://jina.ai/cloud/\nJINA_API_KEY=your_jina_api_key_here\n\n# Optional: Override the default Jina DeepSearch API base URL\n# JINA_API_BASE_URL=https://deepsearch.jina.ai/v1\n\n# Optional: Override the default request timeout (in seconds)\n# JINA_API_TIMEOUT=180\n"
    },
    {
      "name": "README.md",
      "content": "# Jina DeepSearch MCP Server\n\nThis project provides a Model Context Protocol (MCP) server for interacting with the [Jina AI DeepSearch API](https://jina.ai/deepsearch/).\n\nJina DeepSearch combines web searching, reading, and reasoning to provide comprehensive answers to complex questions. It's designed for tasks requiring iterative research, access to real-time information, and deep reasoning capabilities. The API is compatible with OpenAI's Chat API schema.\n\nThis MCP server exposes the core functionality of the DeepSearch API as a tool that can be easily integrated into agentic workflows or other applications supporting MCP.\n\n## Features\n\n*   Provides a `chat_completion` tool to access Jina DeepSearch.\n*   Supports both streaming (Server-Sent Events) and non-streaming responses.\n*   Handles authentication using Jina API keys.\n*   Includes comprehensive Pydantic models for inputs and outputs.\n*   Configurable via environment variables.\n*   Built with [FastMCP](https://github.com/your-repo/fastmcp). <!-- Update link if available -->\n\n## Prerequisites\n\n*   Python 3.8+\n*   A Jina AI API Key (obtain from [jina.ai/cloud](https://jina.ai/cloud/))\n\n## Setup\n\n1.  **Clone the repository (or create the files):**\n    ```bash\n    # If cloned from a repo:\n    # git clone <repository-url>\n    # cd <repository-directory>\n\n    # If creating files manually, ensure you have main.py, models.py, api.py,\n    # requirements.txt, and .env.example in the same directory.\n    ```\n\n2.  **Create a virtual environment (recommended):**\n    ```bash\n    python -m venv venv\n    source venv/bin/activate  # On Windows use `venv\\Scripts\\activate`\n    ```\n\n3.  **Install dependencies:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n4.  **Configure environment variables:**\n    *   Copy the example environment file:\n        ```bash\n        cp .env.example .env\n        ```\n    *   Edit the `.env` file and add your Jina API key:\n        ```dotenv\n        JINA_API_KEY=your_jina_api_key_here\n        ```\n    *   You can optionally override the API base URL or timeout in the `.env` file if needed.\n\n## Running the Server\n\nStart the MCP server using:\n\n```bash\npython main.py\n```\n\nThe server will start, typically on `http://127.0.0.1:8000` (check console output for the exact address).\n\n## Available Tools\n\n### `chat_completion`\n\nPerforms a deep search and reasoning process based on the provided conversation history and query.\n\n**Parameters:**\n\n*   `messages` (List[Message], **required**): A list of message objects, where each object has `role` ('user', 'assistant', 'system') and `content` (string, can be text or data URI).\n*   `model` (str, optional): Model ID. Defaults to `jina-deepsearch-v1`.\n*   `stream` (bool, optional): Whether to stream the response. Defaults to `True`.\n*   `reasoning_effort` (str, optional): 'low', 'medium', or 'high'. Defaults to `medium`.\n*   `budget_tokens` (int, optional): Max tokens for the process.\n*   `max_attempts` (int, optional): Max retry attempts.\n*   `no_direct_answer` (bool, optional): Force search/thinking. Defaults to `False`.\n*   `max_returned_urls` (int, optional): Max URLs in the final answer.\n*   `structured_output` (Dict[str, Any], optional): JSON schema for structured output.\n*   `good_domains` (List[str], optional): Prioritized domains.\n*   `bad_domains` (List[str], optional): Excluded domains.\n*   `only_domains` (List[str], optional): Exclusively included domains.\n\n**Returns:**\n\n*   If `stream=True`: An asynchronous iterator yielding `DeepSearchChunk` objects.\n*   If `stream=False`: A `DeepSearchResponse` object containing the complete answer, usage stats, and visited URLs.\n\n**Example Usage (Conceptual MCP Client):**\n\n```python\nfrom mcp.client import MCPClient\n\nasync def main():\n    client = MCPClient(\"http://127.0.0.1:8000\") # Address where MCP server is running\n\n    messages = [\n        {\"role\": \"user\", \"content\": \"What are the latest advancements in quantum computing resistant cryptography?\"}\n    ]\n\n    # Non-streaming example\n    # response = await client.call(\n    #     tool_name=\"chat_completion\",\n    #     arguments={\n    #         \"messages\": messages,\n    #         \"stream\": False,\n    #         \"reasoning_effort\": \"high\"\n    #     }\n    # )\n    # print(\"Final Answer:\", response['choices'][0]['message']['content'])\n    # print(\"Visited URLs:\", response.get('visited_urls'))\n    # print(\"Usage:\", response['usage'])\n\n    # Streaming example\n    async for chunk in await client.call_stream(\n        tool_name=\"chat_completion\",\n        arguments={\n            \"messages\": messages,\n            \"stream\": True,\n            \"reasoning_effort\": \"medium\"\n        }\n    ):\n        if chunk['choices'] and chunk['choices'][0]['delta'] and chunk['choices'][0]['delta'].get('content'):\n            print(chunk['choices'][0]['delta']['content'], end=\"\", flush=True)\n        # Final chunk might contain usage and visited_urls\n        if chunk.get('usage'):\n            print(\"\\n--- Usage ---\")\n            print(chunk['usage'])\n        if chunk.get('visited_urls'):\n            print(\"\\n--- Visited URLs ---\")\n            print(chunk['visited_urls'])\n\n    await client.close()\n\n# Run the example\n# import asyncio\n# asyncio.run(main())\n```\n\n## Error Handling\n\nThe server includes error handling for:\n\n*   API authentication errors (missing or invalid API key).\n*   Network errors (connection issues, timeouts).\n*   API errors (4xx client errors, 5xx server errors from Jina API).\n*   Invalid input or output data validation errors.\n\nErrors originating from the API client or during processing within the tool will be logged and generally raised as exceptions, which the MCP client should handle.\n\n## Rate Limits\n\nThe Jina DeepSearch API has rate limits (e.g., 10 requests per minute on free tiers). This MCP server does *not* implement client-side rate limiting. Ensure your usage patterns comply with the API's limits to avoid `429 Too Many Requests` errors.\n\n## Development\n\n*   **Code Structure:**\n    *   `main.py`: MCP server setup and tool definitions.\n    *   `api.py`: Client for interacting with the Jina DeepSearch API.\n    *   `models.py`: Pydantic models for API requests and responses.\n    *   `requirements.txt`: Python dependencies.\n    *   `.env.example` / `.env`: Environment variable configuration.\n*   **Linting/Formatting:** Consider using tools like `black` and `ruff` for code quality.\n"
    }
  ]
}